[
["index.html", "R Programming for Research Online course book, ERHS 535", " R Programming for Research Colorado State University, ERHS 535 Brooke Anderson and Rachel Severson 2016-11-12 Online course book, ERHS 535 This is the online book for Colorado State University’s ERHS 535 R Programming for Research course. This book includes course information, course notes, links to download pdfs of lecture slides, in-course exercises, homework assignments, and vocabulary lists for quizzes for this course. Because this is my first semester teaching the course with this online book, it will be evolving throughout the semester, as we get to new material. "],
["course-information.html", "Course information 0.1 Course overview 0.2 Time and place 0.3 Detailed schedule 0.4 Grading 0.5 Course set-up 0.6 Helpful books for learning R", " Course information Download a pdf of the lecture slides covering this topic. 0.1 Course overview This document provides the course notes for Colorado State University’s ERHS 535 course for Fall 2016. The course offers in-depth instruction on data collection, data management, programming, and visualization, using data examples relevant to academic research. 0.2 Time and place This course meets in Room 120 of the Environmental Health Building on Mondays and Wednesdays, 10:00 am–12:00 pm. Exceptions to these meeting times are: There will be no meeting on Wednesday, Aug. 31. There will be no meeting on Labor Day (Monday, Sept. 5). To make up for missing class on Aug. 31, we will have a supplemental class on Friday, Sept. 9, 10:00 am–12:00 pm. You will not lose attendance points if you cannot attend this class, but will be responsible for the material covered. There are no course meetings the week of Thanksgiving. 0.3 Detailed schedule Here is a more detailed view of the schedule for this course for Fall 2016: Dates Level Lecture content Graded items Aug. 22, 24 Preliminary R Preliminaries Aug. 29 Basic Entering and cleaning data Sept. 7, Sept. 9* Basic Exploring data Quiz (W) Sept. 12, 14 Basic Reporting data results Quiz (M), HW #1 (W) Sept. 19, 21 Basic Reproducible Research Quiz (M) Sept. 26, 28 Intermediate Entering and cleaning data Quiz (M), HW #2 (W) Oct. 3, 5 Intermediate Exploring data Quiz (M) Oct. 10, 12 Intermediate Reporting data results Quiz (M), HW #3 (W) Oct. 17, 19 Intermediate Reproducible Research Quiz (M), Group choices (M) Oct. 24, 26 Advanced Entering and cleaning data Quiz (M), Project proposal (M), HW #4 (W) Oct. 31, Nov. 2 Advanced Exploring data Nov. 7, 9 Advanced Reporting data results HW #5 (W) Nov. 14, 16 Advanced Mapping in R Nov. 28, 30 Advanced Package development 1 HW #6 (W) Dec. 5, 7 Advanced Package development 2 Project draft (M) Week of Dec. 12 Group presentations Final project (M) 0.4 Grading Course grades will be determined by the following five components: Assessment component Percent of grade Final group project 30 Weekly in-class quizzes, weeks 3-10 25 Homework 25 Attendance and class participation 10 Weekly in-course group exercises 10 0.4.1 Attendance and class participation Because so much of the learning for this class is through interactive work in class, it is critical that you come to class. Out of a possible 10 points for class attendance, you will get: 10 points if you attend all classes 8 points if you miss one class 6 points if you miss two classes 4 points if you miss three classes 2 points if you miss four classes 0 points if you miss five or more classes You can get two extra credit attendance points (i.e., make up for a missed class) by attending either the seminar that Yihui Xie will give on Sept. 23 at 4:00 pm for the Statistics Department in Weber 237 to the short course he will give at 10:00-11:00 am in Weber 223H. (You are welcome to attend both, but can only get a maximum of two extra credit attendance points.) 0.4.2 Weekly in-course group exercises Part of each class will be spent doing in-course group exercises. Ten points of your final grade will be based on your participation in these exercises. As long as you are in class and participate in these exercises, you will get full credit for this component. If you miss a class, to get credit towards this component of your grade, you will need to turn in a one-page document describing what you learned from doing the in-course exercise on your own time. All in-class exercises are included in the online course book at the end of the chapter on the associated material. 0.4.3 In-class quizzes You will have eight total in-class quizzes. You will have one for each of the Week 2–10 class meetings. There will be at least 10 questions per quiz. You will get 1/3 point for each correct answer. If you do the math, you can get full credit for this if you get at least 75% of your answers right. You can not get more than the maximum of 25 points for this component– once you reach 25 points on quizzes, you will have achieved full credit for the quiz component of the course grade. All quiz questions will be multiple choice, matching, or some other form of “close-answered” question (i.e., no open-response-style questions). You can not make up a quiz for a class period you missed. You can still get full credit on your total possible quiz points if you miss a class, but it means you will have to work harder and get more questions right for days you are in class. Because grading format for these quizzes allows for you to miss some questions and still get the full quiz credit for the course, I will not ever re-consider the score you got on a previous quiz, give points back for a wrong answer on a poorly-worded question, etc. However, if a lot of people got a particular question wrong, I will be sure to cover it in the next class period. Also, especially if a question was poorly worded and caused confusion, I will work a similar question into a future quiz– in addition to the 10 guaranteed questions for that quiz– so every student will have the chance to get an extra 1/3 point of credit for the question. The “Vocabulary” appendix of our online book has the list of material for which you will be responsible for this quiz. Most of the functions and concepts will have been covered in class, but some may not. You are responsible for going through the list and, if there are things you don’t know or remember from class, learning them. To do this, you can use help functions in R, Google, StackOverflow, books on R, ask a friend, and any other resource you can find. In general, using R frequently in your research or other coursework will help you to prepare and do well on these quizzes. 0.4.4 Homework There will be six homework assignments, starting a few weeks into the course and then due approximately every two weeks (see the detailed schedule in the online course book for exact due dates). Homeworks should be done individually. You will get many chances to work with others during in-course exercises and your final group project, but these homeworks should be a chance to assess how well you understand and can use the course material on your own. Homeworks will be graded for correctness, but some partial credit will be given for questions you try but fail to answer correctly. If you can’t completely do a required task, be sure to show and explain what you tried to do to complete it. Homework is due by the start of class on the due date. Your grade will be reduced by 10 points for each day it is late, and will receive no credit if it is late by over a week. 0.4.5 Final group project You will do the final group project in groups of 2–3. The final product will be a statistical blog post-style article of 1,500 words or less and an accompanying Shiny web application. Come up with an interesting question you’d love to get the answer to that you think you can find data to help you answer. You will need to use the data you find, and R, to write your article. The final product will be a Word document created from an RMarkdown file and an accompanying Shiny web application. Here are some articles to give you an idea of the style and content for this project: Does Christmas come earlier each year? Hilary: the most poisoned baby name in US history Every Guest Jon Stewart Ever Had On “The Daily Show” Should Travelers Avoid Flying Airlines That Have Had Crashes in the Past? Billion-Dollar Billy Beane You will have in-class group work time during weeks 10–15 to work on this. This project will also require some work with your group outside of class. You will be able to get feedback from me through weekly informal written reports in these weeks. I will also provide feedback and help during the in-class group work time. The final group project will be graded with A through F, with the following point values (out of 30 possible): 30 points for an A 25 points for a B 20 points for a C 15 points for a D 10 points for an F If you turn nothing in, you will get 0 points. 0.4.5.1 Final presentation In total, the group’s presentation should last 25-30 minutes. There will then be 5 minutes for questions. Split the presentation up into three parts: (1) the main presentation (10-15 minutes), (2) a tutorial-style discussion of how you used R to do the project (10 minutes) and (3) an overview of your Shiny app (5 minutes). The main presentation part should include the following sections: Research question: In one sentence, what is the main thing you were trying to figure out? Introduction: Why did you decide to do this project? You must convince us that your project answers an interesting and important question. Methods: Where did you find data to answer the question? Why are they appropriate? Are there any limitations we should keep in mind? How did you investigate the data to try to answer your question? This should not include R code (save that for the tutorial part), but rather should use language like “To determine if … was associated with …, we measured the correlation …”. It’s fine for this project if the Methods are fairly simple (“We investigated the distribution of … using boxplots …”, “We took the mean and interquartile range of …”, “We mapped state-level averages of …”, etc.). Why do you choose to use the Methods you used? Why do you think they’re appropriate and useful for your project? Results: What did you find out? Most of these slides should be figures or tables. Discuss your interpretation of your results as you present them. Ideally, you should be able to show your main results in about 4 slides, with one figure or table per slide. Conclusions: So what? How do your results compare with what other people have found out about your research question? Based on what you found, are there now other things you want to check out? The tutorial part should include the following sections: Overview of your approach in R: Step us through a condensed version of how you did your project Interesting packages / techniques: Spend a bit more time on any parts that you found particularly interesting or exciting. Were there packages you used that were helpful that we haven’t talked about in class? Did you find out how to do anything that you think other students could use in the future? Did you end up writing a lot of functions to use? Did you have an interesting way of sharing code and data among your group members? Lessons learned: If you were to do this project again from scratch, what would you do differently? Were there any big wrong turns along the way? Did you find out how to do something late in the project that would have saved you time if you’d started using it earlier? 0.4.5.2 Final report The final report should not exceed 1,500 words. You should aim for no more than five figures and tables. In addition to the good examples linked above, you can find another example of the type of document we’re looking for here from FiveThirtyEight. This example would have received an excellent score if it had been turned in for this class because it is clearly and engagingly written, it presents figures and tables that directly help to answer its main question and that are clearly explained and attractively presented, and its author has convinced me that this is an interesting question worth reading an article about. Notice that it is not very long, only has three figures and tables, and uses fairly simple analysis. I will assess the final report on the following criteria: Is it written with correct spelling and grammar? Is it very clear what your over-arching research question is? Have you made a convincing case that this is an important or interesting problem? You could meet this criterion even by convincing me that this is a problem that just one of you is passionate about (as an example, see here). Are the data that you chose to use reasonable for answering the question? Have you explained any caveats or limitations to the data that I should keep in mind when interpreting your results? As an example of how to do this for an analysis with secondary (imperfect) data, see how this post handles describing the data it uses, particularly in footnotes 1 and 3 and the sentences in the main text that correspond to them. Have you explained the way you analyzed the data clearly enough that I think that I could reproduce your analysis if I had your data? Have you explained a bit why your method of analyzing the data is appropriate for your question? Have you let me know about major caveats or limitations related to the methods of analysis you’re using? Have you presented figures and / or tables with results that help answer your main research question? Is it clear what each is showing and how I should interpret it? (For a nice example of explaining how to interpret results, see footnote 4 here.) Have you explained and interpreted your main results in the text? Have you pointed out any particularly interesting observations (interesting outliers, for example)? When I’m finished with your article, do I have more insight into your research question than when I started? If you include a quote or a figure from an outside source, you must include a full reference for it. Otherwise, I am okay with you doing referencing more in a blog-post style. That is, if you are repeating another person’s ideas or findings, you must reference it, but you may use a web link rather than writing out full references. You do not need to include references of any type for standard analysis techniques (for example, you would not need to include a reference from a Stats book if you are fitting a regression model). 0.4.5.3 Shiny app Finally, you should create a Shiny app to provide an interactive visualization related to your research question. Expectations for that are: It should work. It should include text that is clearly written, without grammatical errors or typos. Any graphics are easily to interpret and follow some of the principles of good graphics covered in class. It includes at least two rendered outputs (e.g., one plot and one table; two plots). It is self-contained– in other words, a user shouldn’t need to read your report or hear your explain the app to understand it. It should include enough information on the app for a user to figure out how to use the app and interpret the output. It is publicly available online. The easiest way to do this is to create a free account at shinyapps.io and publish to that. Only one person in the group needs to publish the app. 0.5 Course set-up Please be sure you have the latest version of R and RStudio installed. Also, be sure to sign up for a free GitHub account. 0.6 Helpful books for learning R There are three publishers that are leaders in good books for learning R: O’Reilly No Starch Press Springer Some particular books you might want to check out: R for Data Science R for Dummies R in a Nutshell R Cookbook R Graphics Cookbook A Beginner’s Guide to R Roger Peng’s Leanpub books Books that other students have found useful include: Introductory R by Robert J. Knell "],
["r-preliminaries.html", "Chapter 1 R Preliminaries 1.1 R and R Studio 1.2 The “package” system 1.3 Basic code conventions of R 1.4 R’s most basic object types 1.5 Using R functions 1.6 In-course Exercise", " Chapter 1 R Preliminaries Download a pdf of the lecture slides covering this topic. 1.1 R and R Studio 1.1.1 What is R? R in an open-source programming language that evolved from the S language. The S language was developed at Bell Labs in the 1970s, which is the same place (and about the same time) that the C programming language was developed. R itself was developed in the 1990s–2000s at the University of Auckland. It is open-source software, freely and openly distributed under the GNU General License. The base version of R that you download when you install R on your computer includes the critical code for running R, but you can also install and run “packages” that people all over the world have developed to extend R. With new developments, R is becoming more and more useful for a variety of programming tasks. However, where it really shines is in working with data and doing statistical analysis. R is currently popular in a number of fields, including: Statistics Machine learning Data journalism / data analysis R has some of the same strengths (quick and easy to code, interfaces well with other languages, easy to work interactively) and weaknesses (slower than compiled languages) as Python. For data-related tasks, R and Python are fairly neck-and-neck. However, R is still the first choice of statisticians in most fields, so I would argue that R has a an advantage if you want to have access to cutting-edge statistical methods. “The best thing about R is that it was developed by statisticians. The worst thing about R is that… it was developed by statisticians.” -Bo Cowgill, Google, at the Bay Area R Users Group 1.1.2 Open-source software R is open-source software. Many other popular statistical programming languages, conversely, are proprietary. It’s useful to know what it means for software to be “open-source”, both conceptually and in terms of how you will be able to use and add to R in your own work. R is free, and it’s tempting to think of open-source software just as “free software”. Things, however, are a little more subtle than that. It helps to consider some different meanings of the word “free”. “Free” can mean: Gratis: Free as in beer Libre: Free as in speech Open-source software software is the libre type of free. This means that, with software that is open-source, you can: Access all of the code that makes up the software Change the code as you’d like for your own applications Build on the code with your own extensions Share the software and its code, as well as your extensions, with others In practice, this means that, once you are familiar with the software, you can dig deeply into the code to figure out exactly how it’s performing certain tasks. This can be useful for finding bugs and eliminating bugs, and also can help researchers figure out if there are any limitations in how the code works for their specific research. It also means that you can build your own software on top of existing R software and its extensions. I explain a bit more about R packages a bit later, but this open-source nature of R (and other languages, including Python) has created a large community of people worldwide who develop and share extensions to R. As a result, you can pull in packages that let you do all kinds of things in R, like visualizing Tweets, cleaning up accelerometer data, analyzing complex surveys, fitting maching learning models, and a wealth of other cool things. 1.1.3 What is RStudio? To get the R software, you’ll download R from the R Project for Statistical Computing. This is enough for you to use R on your own computer. However, I would suggest one additional, free piece of software to improve your experience while working with R, RStudio. RStudio is an integrated development environment (IDE) for R. This basically means that it provides you an interface for running R and coding in R, with a lot of nice extras that will make your life easier. You download RStudio separately from R– you’ll want to download and install R itself first, and then you can download RStudio. You want the Desktop version with the free license. The company that develops this IDE is a fantastic contributer to the global R community. RStudio currently: Develops and freely provides the RStudio IDE Provides excellent resources for learning and using R (cheatsheets, ) Is producing some of the most-used R packages Employs some of the top people in R development R has been advancing by leaps in bounds in terms of what it can do and the elegance with which it does it, in large part because of the enormous contributions of people involved with RStudio. 1.1.4 Setting up If do not already have them, you will need to download and install both R and RStudio. Go to CRAN and download the latest version of R for your system. Install. Go to the RStudio download page and download the latest version of RStudio for your system. Install. Defaults should be fine for everything when you install both R and RStudio. You will want the latest stable version, rather than the development version, for this course. 1.2 The “package” system 1.2.1 R packages Your original download of R is only a starting point. To me, this is a bit like the toy train set that my son was obsessed with for a while. You first buy a very basic set that looks something like Figure 1.1. Figure 1.1: The toy version of base R. To take full advantage of R, you’ll want to add on packages. In the case of the train set, at this point, a doting grandparent adds on extensively through birthday presents, so you end up with something that looks like Figure 1.2. Figure 1.2: The toy version of what your R set-up will look like once you find cool packages to use for your research. The main source for installing packages for R remains the Comprehensive R Archive Network, or CRAN. However, GitHub is growing in popularity, especially for packages that are still in development. You can also create and share packages among your collaborators or co-workers, without ever posting them publicly. 1.2.2 Installing from CRAN The most popular place from which to get packages is currently CRAN. You can install packages from CRAN using R code. For example, telephone keypads include letters for each number (Figure 1.3), which allow companies to have “named” phone numbers that are easier for people to remember, like 1-800-GO-FEDEX and 1-800-FLOWERS. Figure 1.3: Telephone keypad with letters corresponding to each number. The phonenumber package is a cool little package that will convert between numbers and letters based on the telephone keypad. Since this package is on CRAN, you can install the package to your computer using the install.packages function: install.packages(&quot;phonenumber&quot;) This downloads the package from CRAN and saves it in a special location on your computer where R can load it when you’re ready to use it. 1.2.3 Loading an installed package Once you have installed a package, it will be saved to your computer, but you won’t be able to access it’s functions until you load it in your R session. You can load a package in an R session using the library function, with the package name inside the parentheses. library(phonenumber) One thing that people often find confusing when they start using R is knowing when to use and not use quotation marks. The general rule is that you use quotation marks when you want to refer to a character string literally, but no quotation marks when you want to refer to the value in a previously-defined object. For example, if you saved the string “Anderson” as the object my_name (my_name &lt;- “Anderson”), then in later code, if you type my_name (no quotation marks), you’ll get “Anderson”, while if you type out “my_name” (with quotation marks), you’ll get “my_name” (what you typed, literally). One thing that makes this rule confusing is that there are a few cases in R where you really should (by this rule) use quotation marks, but the function is coded to let you be lazy and get away without them. One example is the library function. In the above code, you want to literally load the package “phonenumber”, rather than load whatever character string is saved in the object named phonenumber. However, library is one of the functions where you can be lazy and skip the quotation marks, and it will still load “phonenumber” for you (although, if you want, this function also works if you follow the rule and call library(“phonenumber”) instead). Once a package is loaded, you can use all its exported (i.e., public) functions by calling them directly. For example, the phonenumber has a function called letterToNumber that converts a character string to a number. Once you’ve loaded phonenumber using library, you can use this function in your R session: fedex_number &lt;- &quot;GoFedEx&quot; letterToNumber(fedex_number) ## [1] &quot;4633339&quot; R vectors can have several different classes. One common class is the character class, which is the class of the character string we’re using here (“GoFedEx”). You’ll always put character strings in quotation marks. Another key class is numeric (numbers). Later in the course, we’ll introduce other classes that vectors can have, including factors and dates. 1.3 Basic code conventions of R 1.3.1 R’s MVP: The gets arrow The gets arrow, &lt;-, is R’s assignment operator. It takes whatever you’ve created on the right hand side of the &lt;- and saves it as an object with the name you put on the left hand side of the &lt;-. The basic structure of a call with a gets arrow looks like this: ## Note: Generic code [name of object] &lt;- [thing I want to save] Sometimes, we’ll show “generic” code in a code block, that doesn’t actually work if you put it in R, but instead shows the generic structure of an R call. We’ll try to always include a comment with any generic code, so you’ll know not to try to run it in R. For example, if I just type &quot;GoFedEx&quot; at the R prompt, R will print that string back to me, but won’t save it anywhere for me to use later: &quot;GoFedEx&quot; ## [1] &quot;GoFedEx&quot; However, if I assign &quot;GoFedEx&quot; to an object using a gets arrow, I can print it out or use it later by typing (“referencing”) that object name: fedex_number &lt;- &quot;GoFedEx&quot; fedex_number ## [1] &quot;GoFedEx&quot; letterToNumber(fedex_number) ## [1] &quot;4633339&quot; 1.3.2 Assignment operator wars: &lt;- vs. = You can make assignments in R using either the gets arrow (&lt;-) or =. When you read other people’s code, you’ll see both. R gurus advise using &lt;- rather than = when coding in R, and as you move to doing more complex things, some subtle problems might crop up if you use =. I have heard from someone in the know that you can tell the age of a programmer by whether he or she uses the gets arrow or =, with = more common among the young and hip. For this course, however, I am asking you to code according to Hadley Wickham’s R style guide, which specifies using the gets arrow for assignment. While you will be coding with the gets arrow exclusively in this course, it will be helpful for you to know that the two assignment arrows do pretty much the same thing: one_to_ten &lt;- 1:10 one_to_ten ## [1] 1 2 3 4 5 6 7 8 9 10 one_to_ten = 1:10 one_to_ten ## [1] 1 2 3 4 5 6 7 8 9 10 1.3.3 Naming objects When you assign objects, you will need to choose names for them. This object name is what you will type later in your code to reference the object and use it in functions, figures, etc. For example, with the following code, I am assigning the character string “GoFedEx” to an object that I am naming fedex_number: fedex_number &lt;- &quot;GoFedEx&quot; There are only two fixed rules for naming objects in R: Use only letters, numbers, and underscores Don’t start with anything but a letter In addition to these fixed rules, there are also some guidelines for naming objects that you should adopt now, since they will make your life easier as you advance to writing more complex code in R. The following three guidelines for naming objects are from Hadley Wickham’s R style guide: Use lower case for variable names (fedex_number, not FedExNumber) Use an underscore as a separator (fedex_number, not fedex.number or fedexNumber) Avoid using names that are already defined in R (e.g., don’t name an object mean, because a function named mean already exists) Another good practice is to name objects after nouns (e.g., fedex_number) and later, when you start writing functions, name those after verbs (e.g., call_fedex). You’ll want your object names to be short enough that they don’t take forever to type as you’re coding, but not so short that you can’t remember what they stand for. Sometimes, you’ll want to create an object that you won’t want to keep for very long. For example, you might want to create a small object to test some code, but you plan to not need the object again once you’ve done that. You may want to come up with some short, generic object names that you use for these kinds of objects, so that you’ll know that you can delete them without problems when you want to clean up your R session. There are all kinds of traditions for these placeholder variable names in computer science. foo and bar are two popular choices, as are, evidently, xyzzy, spam, ham, and norf. There are different placeholder names in different languages: for example, toto, truc, and azerty (French); and pippo, pluto, paperino (Disney character names; Italian). See the Wikipedia page on metasyntactic variables to find out more. 1.3.4 Commenting code Sometimes, you’ll want to include notes in your code. You can do this in all programming languages by using a comment character to start the line with your comment. In R, the comment character is the hash symbol, #. R will skip any line that starts with # in a script. For example, if you run the following code: # Don&#39;t print this. &quot;But print this&quot; ## [1] &quot;But print this&quot; R will only print the second, uncommented line. You can also use a comment in the middle of a line, to add a note on what you’re doing in that line of the code. R will skip any part of the code from the hash symbol on. For example: &quot;Print this&quot; ## But not this, it&#39;s a comment. ## [1] &quot;Print this&quot; 1.4 R’s most basic object types An R object stores some type of data that you want to use later in your R code, without fully recreating it. The content of R objects can vary from very simple (the &quot;GoFedEx&quot; string in the example code above) to very complex objects with lots of elements (for example, a machine learning model). There are a variety of different object types in R, shaped to fit different types of objects ranging from the simple to complex. In this section, we’ll start by describing two object types that you will use most often in basic data analysis, vectors (1-dimensional objects) and dataframes (2-dimensional objects). 1.4.1 Vectors To get an initial grasp of the vector object type in R, think of it as a 1-dimensional object, or a string of values. All values in a vector must be of the same class (i.e., all numbers, all characters, all dates). If you try to create a vector with elements from different classes (like “FedEx”, which is a character, and 3, a number), R will coerce all of the elements to the most generic class of any of the elements (i.e., “FedEx” and “3” will both become characters, since “3” can be changed to a character, but “FedEx” can’t be changed to a number). To create a vector from different elements, you’ll use the concatenation function, c to join them together, with commas between the elements. For example, to create a vector with the first five elements of the Fibonacci sequence, you can run: fibonacci &lt;- c(1, 1, 2, 3, 5) fibonacci ## [1] 1 1 2 3 5 Here is an example of creating a vector using elements with the character class instead of numbers (note the quotation marks used around each element for character strings): one_to_five &lt;- c(&quot;one&quot;, &quot;two&quot;, &quot;three&quot;, &quot;four&quot;, &quot;five&quot;) one_to_five ## [1] &quot;one&quot; &quot;two&quot; &quot;three&quot; &quot;four&quot; &quot;five&quot; If you mix classes when you create the vector, R will coerce all the elements to most generic of the elements’ classes: mixed_classes &lt;- c(1, 3, &quot;five&quot;) mixed_classes ## [1] &quot;1&quot; &quot;3&quot; &quot;five&quot; A vector’s length is the number of elements in the vector. You can use the length function to determine a vector’s length: length(mixed_classes) ## [1] 3 Once you create an object, you will often want to reference the whole object in future code. However, there will be some times when you’ll want to reference just certain elements of the object (for example, the first three values). You can pull out certain values from a vector by using indexing with square brackets ([...]) to identify the locations of the elements you want to pull, with a numeric vector inside the brackets that lists the numbered positions of the elements you want to get: fibonacci[2] # Get the second value ## [1] 1 fibonacci[c(1, 5)] # Get first and fifth values ## [1] 1 5 fibonacci[1:3] # Get the first three values ## [1] 1 1 2 You can also use logic to pull out some values of a vector. For example, you might only want to pull out even values from the fibonacci vector. We’ll cover using logical statements to index vectors later in the book. 1.4.2 Dataframes A dataframe is a 2-dimensional object, and is made of one or more vectors of the same length stuck together side-by-side. It is the closest R has to an Excel spreadsheet-type structure. You can create dataframes using the data.frame function. However, most often you will create a dataframe by reading in data from a file, using something like read.csv. To create a dataframe using data.frame, in this case by sticking together vectors you already have saved as R objects, you can run: fibonacci_seq &lt;- data.frame(num_in_seq = one_to_five, fibonacci_num = fibonacci) fibonacci_seq ## num_in_seq fibonacci_num ## 1 one 1 ## 2 two 1 ## 3 three 2 ## 4 four 3 ## 5 five 5 Note that this call requires that the one_to_five and fibonacci vectors are the same length, although they don’t have to be (and in this case aren’t) the same class of objects (one_to_five is a character class, fibonacci is numeric). You can also create a dataframe using data.frame even if you don’t have the vectors for the columns saved as objects. Instead, in this case, you can put the vector assignment directly within the data.frame call: fibonacci_seq &lt;- data.frame(num_in_seq = c(&quot;one&quot;, &quot;two&quot;, &quot;three&quot;, &quot;four&quot;, &quot;five&quot;), fibonacci_num = c(1, 1, 2, 3, 5)) fibonacci_seq ## num_in_seq fibonacci_num ## 1 one 1 ## 2 two 1 ## 3 three 2 ## 4 four 3 ## 5 five 5 You can put more than one function call in a single line of R code, as in this example (the c creates a vector, while the data.frame creates a dataframe, using the vectors created by the calls to c). When you use multiple functions within a single R call, R will evaluate starting from the inner-most parentheses out, much like the order of operations in a math equation with parentheses. The general format for using data.frame is: ## Note: Generic code [name of object] &lt;- data.frame([1st column name] = [1st column content], [2nd column name] = [2nd column content]) with an equals sign between the column name and column content for each column, and commas between each of the columns. You can use square-bracket indexing ([..., ...]) for dataframes, too, but now they’ll have two dimensions (rows, then columns). Put the rows you want before the comma, the columns after. If you want all of something (e.g., all rows in the dataframe), leave the designated spot blank. Here are two examples of using square-bracket indexing to pull a subset of the fibonacci_seq dataframe we created above: fibonacci_seq[1:2, 2] # First two rows, second column ## [1] 1 1 fibonacci_seq[5, ] # Last row, all columns ## num_in_seq fibonacci_num ## 5 five 5 If you forget to put the comma in the indexing for a dataframe (e.g., fibonacci_seq[1:2]), you will index out the columns that fall at that position or positions. To avoid confusion, I suggest that you always use indexing with a comma when working with dataframes. So far, we’ve only shown how to create dataframes from scratch within an R session. Usually, however, you’ll create R dataframes instead by reading in data from an outside file using read.csv and related functions. For example, you might want to analyze data on all the guests that came on the Daily Show, circa Jon Stewart. If you have this data in a comma-separated (csv) file on your computer called “daily_show_guests.csv”, you can read it into your R session with the following code: daily_show &lt;- read.csv(&quot;daily_show_guests.csv&quot;, header = TRUE, skip = 4) In this code, the read.csv function is reading in the data from “daily_show_guests”, while the gets arrow (&lt;-) assigns that data to the object daily_show, which you can then reference in later code to explore and plot the data. Once you’ve read in the data and saved the resulting dataframe as an object, you can use square-bracket indexing to look at the first two rows in the data: daily_show[1:2, ] ## YEAR GoogleKnowlege_Occupation Show Group Raw_Guest_List ## 1 1999 actor 1/11/99 Acting Michael J. Fox ## 2 1999 Comedian 1/12/99 Comedy Sandra Bernhard You can use the functions dim, nrow, and ncol to figure out the dimensions (number of rows and columns) of a dataframe: dim(daily_show) ## [1] 2693 5 nrow(daily_show) ## [1] 2693 ncol(daily_show) ## [1] 5 1.5 Using R functions 1.5.1 Function structure In general, functions in R take the following structure: ## Generic code function.name(parameter 1 = argument 1, parameter 2 = argument 2, parameter 3 = argument 3) The result of the function will be output to your R session, unless you choose to save the output in an object: ## Generic code new_object &lt;- function.name(parameter 1 = argument 1, parameter 2 = argument 2, parameter 3 = argument 3) Here are some example function calls, to give you examples of this structure: head(daily_show) ## YEAR GoogleKnowlege_Occupation Show Group Raw_Guest_List ## 1 1999 actor 1/11/99 Acting Michael J. Fox ## 2 1999 Comedian 1/12/99 Comedy Sandra Bernhard ## 3 1999 television actress 1/13/99 Acting Tracey Ullman ## 4 1999 film actress 1/14/99 Acting Gillian Anderson ## 5 1999 actor 1/18/99 Acting David Alan Grier ## 6 1999 actor 1/19/99 Acting William Baldwin head(daily_show, n = 3) ## YEAR GoogleKnowlege_Occupation Show Group Raw_Guest_List ## 1 1999 actor 1/11/99 Acting Michael J. Fox ## 2 1999 Comedian 1/12/99 Comedy Sandra Bernhard ## 3 1999 television actress 1/13/99 Acting Tracey Ullman daily_show &lt;- read.csv(&quot;daily_show_guests.csv&quot;, skip = 4, header = TRUE) Within the function call, parameters allow you to customize the function to run in a certain way (e.g., use a certain dataframe as an input, give output in a certain format). Some function parameters will have default arguments, which means that you don’t have to put a value for that parameter for the function to run, but you can if you want the function to do something other than the default. 1.5.2 Function help files You can find out more about a function, include what parameters it has and what the default values, if any, are by using ? before the function name in the R console. For example, to find out more about the read.csv command, run: ?read.csv From the “Usage” section of the help file, you can figure out that the only required parameter is file, the pathname of the file that you want to read in, since this is the only argument in the “Usage” example without an argument value: read.csv(file, header = TRUE, sep = &quot;,&quot;, quote = &quot;\\&quot;&quot;, dec = &quot;.&quot;, fill = TRUE, comment.char = &quot;&quot;, ...) You can also see from this “Usage” section that the default value of header is TRUE, the default value of sep is a comma, etc. The “Arguments” section explains each of the parameters, and possible arguments that each can take. For example, here is the explanation of the nrows parameter in the read.csv function: integer: the maximum number of rows to read in. Negative and other invalid values are ignored. From this, you can determine that you should put in a whole number, 1 or higher, and the function will only read in that many lines of the dataframe when you run read.csv. 1.5.3 Function parameters Each function parameter has a name (e.g., nrows, header, file). The safest way to call a function in R is to use the structure parameter name = argument value for every parameter, like this: head(x = daily_show, n = 3) However, you can also give argument values by position. For example, in the head function, the first parameter is x, the object you want to look at, and the second is n, the number of elements you want to include when you look at the object. If you know this, you can call head using the shorter call: head(daily_show, 3) If you use position alone, you will have problems if you don’t include arguments in exactly the right order. However, if you use parameter names to set each argument, it doesn’t matter what order you include arguments when calling a function: # These two calls return the exact same object head(x = daily_show, n = 3) head(n = 3, x = daily_show) Because code tends to be more robust to errors when you use parameter names to set arguments, we recommend against using position, rather than name, to give arguments when calling functions, at least while you’re learning R. It’s too easy to forget the exact order and get errors in your code. However, there is one exception– the first argument to a function is almost always required (i.e., there’s not a default value), and you very quickly learn what the first parameter of most functions are as soon as you start using the function regularly. Therefore, it’s fine to use position alone to specify the first argument in a function, but for now always use the parameter name to set any later arguments: head(daily_show, n = 3) Using the full parameter names for arguments can take a bit more time, since it requires more typing. However, RStudio helps you out with that by offering code completion. Once you start typing the first few letters of a parameter name within a function call, try pressing the tab key. All possible arguments that start with those letters should show up, and you can scroll through to pick the right one, or keep typing until the argument you want is atnthe top of the list of choices, and then press the tab key again. 1.6 In-course Exercise 1.6.1 About the dataset For today’s class, you’ll be using a dataset of all the guests on Jon Stewart’s The Daily Show. This data was originally collected by Nate Silver’s website, FiveThirtyEight and is available on FiveThirtyEight’s GitHub page under the Creative Commons Attribution 4.0 International License. The only change made to the original file was to add (commented) attribution information at the start of the file. First, check out a bit more about this data and its source: Check out the Creative Commons license. What are we allowed to do with this data? What restrictions are there on using the data? It’s often helpful to use prior knowledge to help check out or validate your dataset. One thing we might want to know about this data is if it covers the whole time that Jon Stewart hosted The Daily Show. Find out the dates he started and finished as host. Briefly browse around FiveThirtyEight’s GitHub data page. What are some other datasets available that you find interesting? You can scroll to the bottom of the page to get to the compiled README.md content, which gives the full titles and links to relevant datasets. You can also click on any dataset to get more information. Look at the GitHub page for this Daily Show data. How many columns will be in this dataset? What kind of information does the data include? In this exercise, you’re using data posted by FiveThirtyEight on GitHub. We’ll be using a lot of data that’s on GitHub this semester, and GitHub is being used behind-the-scenes for both this book and the course note slides. We’ll talk more about GitHub later, but you might find it interesting to explore a bit now. It’s a place where people can post, work on, and share code in a number of programming languages– it’s been referred to as “Facebook for Nerds”. You can search GitHub repositories and code specifically by programming language, so it can be a good way to find example R code from which to learn. If you have extra time: Check out the related article on FiveThirtyEight. What are some specific questions they used this data to answer for this article? Who is Nate Silver? 1.6.2 Getting the data onto your computer First, download the data from GitHub onto your computer. Make a directory (folder) on your computer specifically for this course (I strongly recommend that you put it somewhere where the file path will not have any spaces in it– for example, putting it in your home directory, under the name “R_Prog_Course” would be great. Putting it in a directory called “R Prog Course” would not.). Put the Daily Show data in your directory for this course. Take the following steps to get the data onto your computer If you do not yet have a directory (folder) just for this course, make one someplace straightforward like in your home directory. Do not use any spaces in the directory name. Download the file from GitHub. Right click on Raw and then choose “Download linked file”. Put the file into the directory you created for this course. Find out what your home directory is in R. To do this, make sure R is set to your home directory using setwd(&quot;~&quot;), and then get R to print that home directory path using getwd(). Outside of R, open Finder (or your system’s equivalent). Go to your home directory (mine, for example, is /Users/brookeanderson). Figure out how to get from your home directory to the directory you created for this course. For example, from my home directory, I would go RProgrammingForResearch to data. Go back into R. Set R’s working directory to your directory for this class using the setwd command, now that you know the pathname for the directory. For example, I would use setwd(&quot;~/RProgrammingForResearch/data&quot;). Use the list.files command to make sure that the “daily_show_guests.csv” file is in your current working directory. The full R code for this task might look something like: setwd(&quot;~&quot;) getwd() setwd(&quot;~/RProgrammingForResearch/data&quot;) getwd() list.files() &quot;daily_show_guests.csv&quot; %in% list.files() If you have extra time: See if you can figure out the last line of code in the example R code. 1.6.3 Getting the data into R Now that you have the dataset in your working directory, you can read it into R. This dataset is in a .csv (comma separated values) format. (We will talk more about different file formats next week.) You can read csv files into R using the read.csv function. Read the data into your R session Use the read.csv function to read the data into R and save it as the object daily_show. Use the help file for the read.csv function to figure out how this function works. To pull that up, use ?read.csv. Why are we using the option header = TRUE? Can you figure out why we’re using the skip option, and why it’s set to 4? Note that you need to put the file name in quotation marks. What would have happened if you’d used read.csv but hadn’t saved the result as the object daily_show? (For example, you’d run the code read.csv(&quot;daily_show_guests.csv&quot;) rather than daily_show &lt;- read.csv(&quot;daily_show_guests.csv&quot;).) Example R code: daily_show &lt;- read.csv(&quot;daily_show_guests.csv&quot;, header = TRUE, skip = 4) If you have extra time: Say this was a really big dataset. You want to check out just the first 10 rows to make sure that you’ve got your code right before you take the time to pull in the whole dataset. Use the help file for read.csv to figure out how to only read in a few rows. Look through the help file for other options available for read.csv. Can you think of examples when some of these options would be useful? Example R code: daily_show_first10 &lt;- read.csv(&quot;daily_show_guests.csv&quot;, header = TRUE, skip = 4, nrows = 10) daily_show_first10 1.6.4 Checking out the data You now have the data available in the daily_show option. You’ll want to check it out to make sure it read in correctly, and also to get a feel for the data. Throughout, you can use the help pages to figure out more about any of the functions being used (for example, ?dim). Take the following steps to check out the dataset Use indexing to look at the first two rows of the dataset. Based on this, what does each row “measure”? What information do you get for each “measurement”? As a reminder, indexing uses square brackets immediately after the object name. If the object has two dimensions, like a dataframe (rows and columns), you put the rows you want, then a comma, then the columns you want. If you want all rows (or columns), you leave that space blank. For example, if you wanted to get the first two rows and the first three columns, you’d use daily_show[1:2, 1:3]. If you wanted to get the first five rows and all the columns, you’d use daily_show[1:5, ]. Use the dim function to find out how many rows and columns this dataframe has. Based on what you found out about the data from the GitHub page, does it have the number of columns you expected? Based on what you know about the data (all the guests who came on The Daily Show with Jon Stewart), do you think it has about the right number of rows? Use the head function to look at the first few rows of the dataframe. Does it look like the rows go in order by date? What was the date of Jon Stewart’s first show? Does it look like this dataset covers that first show? Use the tail function to look at the last few rows of the dataframe. What is the last show date covered by the dataframe? Who was the last guest? Example R code: daily_show[1:2, ] dim(daily_show) head(daily_show) tail(daily_show) If you have extra time: Say you wanted to look at the first ten rows of the dataframe, rather than the first six. How could you use an option with head to do this? Example R code: head(daily_show, n = 10) 1.6.5 Using the data to answer questions Nate Silver was a guest on The Daily Show. Let’s use this data to figure out how many times he was a guest and when he was on the show. Find out more about Nate Silver on The Daily Show Use the subset function to create a new dataframe that only has the rows of daily_show when Nate Silve was a guest. Put it in the object nate_silver. Print out the full nate_silver dataframe by typing nate_silver. (You could just use this to answer both questions, but still try the next steps. They would be important with a bigger dataset.) To count the number of times Nate Silver was a guest, you’ll need to count the number of rows in the new dataset. You can either use the dim function or the nrow function to do this. What additional information does the dim function give you? To get the dates when Nate Silver was a guest, you can print out just the Show column of the dataframe. There are a few ways you can do this using indexing: nate_silver[ , 3] (since Show is the third column), nate_silver[ , &quot;Show&quot;], or nate_silver$Show. Try these. Example R code: nate_silver &lt;- subset(daily_show, Raw_Guest_List == &quot;Nate Silver&quot;) nate_silver dim(nate_silver) nrow(nate_silver) nate_silver[ , 3] nate_silver[ , &quot;Show&quot;] If you have extra time: When you print out the Show column, why does it also print out something underneath about Levels? Hint: This has to do with the class that R has saved this column as. What class is it currently? What other classes might we want to consider converting it to as we continue working with the dataset? Check out the example code below to get some ideas. Was Nate Silver the only statistician to be a guest on the show? What were the occupations that were only represented by one guest visit? Since GoogleKnowlege_Occupation is a factor, you can use the table function to create a new vector with the number of times each value of GoogleKnowlege_Occupation shows up. You can put this information into a new vector and then pull out only the values that equal 1 (so, only had one guest). (Note that “Statistician” doesn’t show up– there was only one person who was a guest, but he had three visits.) Pick your favorite “one-off” example and find out who the guest was for that occupation. Example R code: class(nate_silver$Show) range(nate_silver$Show) nate_silver$Show &lt;- as.Date(nate_silver$Show, format = &quot;%m/%d/%y&quot;) range(nate_silver$Show) diff(range(nate_silver$Show)) # Time between Nate&#39;s first and last shows statisticians &lt;- subset(daily_show, GoogleKnowlege_Occupation == &quot;Statistician&quot;) statisticians num_visits &lt;- table(daily_show[ , 2]) head(num_visits) # Note: This is a vector rather than a dataframe names(num_visits[num_visits == 1]) subset(daily_show, GoogleKnowlege_Occupation == &quot;chess player&quot;) subset(daily_show, GoogleKnowlege_Occupation == &quot;mathematician&quot;) subset(daily_show, GoogleKnowlege_Occupation == &quot;orca trainer&quot;) subset(daily_show, GoogleKnowlege_Occupation == &quot;Puzzle Creator&quot;) subset(daily_show, GoogleKnowlege_Occupation == &quot;Scholar&quot;) "],
["entering-and-cleaning-data-1.html", "Chapter 2 Entering and cleaning data #1 2.1 R scripts 2.2 Directories and pathnames 2.3 Diversion: paste 2.4 Reading data into R 2.5 Data cleaning 2.6 Dates in R 2.7 In-course Exercise", " Chapter 2 Entering and cleaning data #1 Download a pdf of the lecture slides covering this topic. There are four basic steps you will often repeat as you prepare to analyze data in R: Identify where the data is (If it’s on your computer, which directory? If it’s online, what’s the url?) Read data into R (read.csv, read.table) using the file path you figured out in step 1 Check to make sure the data came in correctly (dim, head, tail, str) Clean the data up In this chapter, I’ll go basics for each of these steps, as well as dive a bit deeper into some related topics you should learn now to make your life easier as you get started using R for research. 2.1 R scripts This is a good point in learning R for you to start putting your work in R scripts, rather than entering commands at the console. An R script is a plain text file where you can save a series of R commands. You can save the script and open it up later to see (or re-do) what you did earlier, just like you could with something like a Word document when you’re writing a paper. To open a new R script in RStudio, go to the menu bar and select “File” -&gt; “New File” -&gt; “R Script”. Alternatively, you can use the keyboard shortcut Command-Shift-N. Figure 2.1 gives an example of an R script file opened in RStudio and points out some interesting elements. Figure 2.1: Example of an R script in RStudio. To save a script you’re working on, you can click on the “Save” button (which looks like a floppy disk) at the top of your R script window in RStudio or use the keyboard shortcut Command-S. You should save R scripts using a “.R” file extension. Within the R script, you’ll usually want to type your code so there’s one command per line. If your command runs long, you can write a single call over multiple lines. It’s unusual to put more than one command on a single line of a script file, but you can if you separate the commands with semicolons (;). These rules all correspond to how you can enter commands at the console. Running R code from a script file is very easy in RStudio. You can use either the “Run” button or Command-Return, and any code that is selected (i.e., that you’ve highlighted with your cursor) will run at the console. You can use this functionality to run a single line of code, multiple lines of code, or even just part of a specific line of code. If no code is highlighted, then R will instead run all the code on the line with the cursor and then move the cursor down to the next line in the script. You can also run all of the code in a script. To do this, use the “Source” button at the top of the script window. You can also run the entire script either from the console or from within another script by using the source() function, with the filename of the script you want to run as the argument. For example, to run all of the code in a file named “MyFile.R” that is saved in your current working directory, run: source(&quot;MyFile.R&quot;) You can add comments into an R script to let others know (and remind yourself) what you’re doing and why. To do this, use R’s comment character, #. Any line on a script line that starts with # will not be read by R. You can also take advantage of commenting to comment out certain parts of code that you don’t want to run at the moment. While it’s generally best to write your R code in a script and run it from there rather than entering it interactively at the R console, there are some exceptions. A main example is when you’re initially checking out a dataset, to make sure you’ve read it in correctly. It often makes more sense to run commands for this task, like str(), head(), tail(), and summary(), at the console. These are all examples of commands where you’re trying to look at something about your data right now, rather than code that builds toward your analysis, or helps you read in or clean up your data. 2.2 Directories and pathnames 2.2.1 Directory structure It seems a bit of a pain and a bit complex to have to think about computer directory structure in the “basics” part of this class, but this structure is not terribly complex once you get the idea of it. There are a couple of very good reasons why it’s worth learning now. First, many of the most frustrating errors you get when you start using R trace back to understanding directories and filepaths. For example, when you try to read a file into R using only the filename, and that file is not in your current working directory, you will get an error like: Error in file(file, &quot;rt&quot;) : cannot open the connection In addition: Warning message: In file(file, &quot;rt&quot;) : cannot open file &#39;Ex.csv&#39;: No such file or directory This error is especially frustrating when you’re new to R because it happens at the very beginning of your analysis – you can’t even get your data in. Also, if you don’t understand a bit about working directories and how R looks for the file you’re asking it to find, you’d have no idea where to start to fix this error. Second, once you understand how to use pathnames, especially relative pathnames, to tell R how to find a file that is in a directory other than your working directory, you will be able to organize all of your files for a project in a much cleaner way. For example, you can create a directory for your project, then create one subdirectory to store all of your R scripts, and another to store all of your data, and so on. This can help you keep very complex projects more structured and easier to navigate. We’ll talk about these ideas more in the course sections on Reproducible Research, but it’s good to start learning how directory structures and filepaths work early. Your computer organizes files through a collection of directories. Chances are, you are fairly used to working with these in your daily life already (although you may call them “folders” rather than “directories”). For example, you’ve probably created new directories to store data files and Word documents for a specific project. Figure 2.2 illustrates the file directory structure on my computer. (Note that I have omitted many, many additional files and directories – this just shows an example of a few directories and files and how they are structured together). Directories are shown in blue, and files in green. Figure 2.2: An example of file directory structure. You can notice a few interesting things from Figure 2.2. First, you might notice the structure includes a few of the directories that you use a lot on your own computer, like Desktop, Documents, and Downloads. Next, the directory at the very top is the computer’s root directory, /. For a PC, the root directory might something like C:; for Unix and Macs, it’s usually /. Finally, if you look closely, you’ll notice that it’s possible to have different files in different locations of the directory structure with the same file name. For example, in the figure, there are files names heat_mort.csv in both the CourseText directory and in the example_data directory. These are two different files with different contents, but they can have the same name as long as they’re in different directories. This fact – that you can have files with the same name in different places – should help you appreciate how useful it is that R requires you to give very clear directions to describe exactly which file you want R to read in, if you aren’t reading in something in your current working directory. You will have a home directory somewhere near the top of your structure, although it’s likely not your root directory. My home directory is /Users/brookeanderson. I’ll describe just a bit later how you can figure out what your own home directory is on your own computer. 2.2.2 Working directory When you run R, it’s always running from within some working directory, which will be one of the directories somewhere in your computer’s directory structure. At any time, you can figure out which directory R is working in by running the command getwd() (short for “get working directory”). For example, my R session is currently running in the following directory: getwd() ## [1] &quot;/home/travis/build/geanders/RProgrammingForResearch&quot; This means that, for my current R session, R is working in the RProgrammingForResearch subdirectory of my brookeanderson directory (which is my home directory). There are a few general rules for which working directory R will start in when you open an R session. These are not absolute rules, but they’re generally true. If you have R closed, and you open it by double-clicking on an R script, then R will generally open with, as its working directory, the directory in which that script is stored. This is often a very convenient convention, because often any of the data you’ll be reading in for that script is somewhere near where the script file is saved in the directory structure. If you open R by double-clicking on the R icon in “Applications” (or something similar on a PC), R will start in its default working directory. You can find out what this is, or change it, in RStudio’s “Preferences”. I have never had a compelling reason to change this on my own computer, as I find it very easy to just move around the directories and set a new working directory using pathnames and the setwd() function. Finally, later in the course, we’ll talk about using R Projects from within RStudio. If you open an R Project, R will start in that project’s working dirctory (the directory in which the .Rproj file for the project is stored). 2.2.3 File and directory pathnames Once you get a picture of how your directories and files are organized, you can use pathnames, either absolute or relative, to move around the directories, set a different working directory, and read in files from different directories than your current working directory. Pathnames are the directions for getting to a directory or file stored on your computer. When you want to reference a directory or file, you can use one of two types of pathnames: Relative pathname: How to get to the file or directory from your current working directory Absolute pathname: How to get to the file or directory from anywhere on the computer Absolute pathnames are a bit more straightforward conceptually, because they don’t depend on your current working directory. However, they’re also a lot longer to write, and they’re much less convenient if you’ll be sharing some of your code with other people who might run it on their own computers. I’ll explain this second point a bit more later in this section. Absolute pathnames give the full directions to a directory or file, starting all the way at the root directory. For example, the heat_mort.csv file in the CourseText directory has the absolute pathname: &quot;/Users/brookeanderson/Desktop/RCourseFall2015/CourseText/heat_mort.csv&quot; You can use this absolute pathname to read this file in using read.csv. This absolute pathname will always work, regardless of your current working directory, because it gives directions from the root – it will always be clear to R exactly what file you’re talking about. Here’s the code to use to read that file in using the read.csv function with the file’s absolute pathname: heat_mort &lt;- read.csv(&quot;/Users/brookeanderson/Desktop/RCourseFall2015/CourseText/heat_mort.csv&quot;) The relative pathname, on the other hand, gives R the directions for how to get to a directory or file from the current working directory. If the file or directory you’re looking for is pretty close to your current working directory in your directory structure, then a relative pathname can be a much shorter way to tell R how to get to the file than an absolute pathname. However, the relative pathname depends on your current working directory – the relative pathname that works perfectly when you’re working in one directory will not work at all once you move into a different working directory. As an example of a relative pathname, say you’re working in the directory RCourseFall2015 within the file structure shown in Figure 2.2, and you want to read in the heat_mort.csv file in the CourseText directory. To get from RCourseFall2015 to that file, you’d need to look in the subdirectory CourseText, where you could find heat_mort.csv. Therefore, the relative pathname from your working directory would be: &quot;CourseText/heat_mort.csv&quot; You can use this relative pathname to tell R where to find and read in the file: heat_mort &lt;- read.csv(&quot;CourseText/heat_mort.csv&quot;) While this pathname is much shorter than the absolute pathname, it is important to remember that if you changed your working directory (for example, if you used setwd(&quot;CourseText&quot;) to move into the CourseText directory), this relative pathname would no longer work. There are a few abbreviations that can be really useful for pathnames: Shorthand Meaning ~ Home directory . Current working directory .. One directory up from current working directory ../.. Two directories up from current working directory These can help you keep pathnames shorter and also help you move “up-and-over” to get to a file or directory that’s not on the direct path below your current working directory. For example, my home directory is /Users/brookeanderson. If I wanted to change my working directory to the Downloads directory, which is a direct sub-directory of my home directory, I could use: setwd(&quot;~/Downloads&quot;) As a second example, say I was working in the working directory CourseText, but I wanted to read in the heat_mort.csv file that’s in the example_data directory, rather than the one in the CourseText directory. I can use the .. abbreviation to tell R to look up one directory from the current working directory, and then down within a subdirectory of that. The relative pathname in this case is: &quot;../Week2_Aug31/example_data/heat_mort.csv&quot; This tells R to look one directory up from the working directory (..), which in this case is to RCourseFall2015, and then down within that directory to Week2_Aug31, then to example_data, and then to look there for the file heat_mort.csv. The relative pathname to read this file while R is working in the CourseTest directory would be: heat_mort &lt;- read.csv(&quot;../Week2_Aug31/example_data/heat_mort.csv&quot;) These relative pathnames would “break” as soon as you tried them from a different working directory – this fact might make it seem like you would never want to use relative pathnames, and would always want to use absolute ones instead, even if they’re longer. If that were the only consideration (length of the pathname), then perhaps that would be true. However, as you do more and more in R, there will likely be many occasions when you want to use relative pathnames instead. They are particularly useful if you ever want to share a whole directory, with R scripts and data, with a collaborator. In that case, if you’ve used relative pathnames, all the code should work fine for the person you share with, even though they’re running it on their own computer. Conversely, if you’d used absolute pathnames, none of them would work on another computer, because the “top” of the directory structure (i.e., for me, /Users/brookeanderson/Desktop) will almost definitely be different for your collaborator’s computer than it is for yours. You can use absolute or relative pathnames for a number of things: To set your working directory: setwd(&quot;../Week2_Aug31&quot;), for example To read in files from a different directory (as shown in the previous examples) To list files in a different directory: for example, list.files(&quot;..&quot;) will list all files in the directory directly about your current working directory (the parent directory of your working directory) If you’re getting errors reading in files, and you think it’s related to the relative pathname you’re using, it’s often helpful to use list.files() to make sure the file you’re trying to load is in the directory that the relative pathname you’re using is directing R to. 2.3 Diversion: paste This is a good opportunity to explain how to use some functions that can be very helpful when you’re using relative or absolute pathnames: paste() and paste0(). As a bit of important background information, it’s important that you understand that you can save a pathname (absolute or relative) as an R object, and then use that R object in calls to later functions like list.files() and read.csv(). For example, to use the absolute pathname to read the heat_mort.csv file in the CourseText directory, you could run: my_file &lt;- &quot;/Users/brookeanderson/Desktop/RCourseFall2015/CourseText/heat_mort.csv&quot; heat_mort &lt;- read.csv(my_file) You’ll notice from this code that the pathname to get to a directory or file can sometimes become ungainly and long. To keep your code cleaner, you can address this by using the paste or paste0 functions. These functions come in handy in a lot of other applications, too, but this is a good place to introduce them. The paste() function is very straightforward. It takes, as inputs, a series of different character strings you want to join together, and it pastes them together in a single character string. (As a note, this means that your result vector will only be one element long, for basic uses of paste(), while the inputs will be several different character stings.) You separate all the different things you want to paste together using with commas in the function call. For example: paste(&quot;Sunday&quot;, &quot;Monday&quot;, &quot;Tuesday&quot;) ## [1] &quot;Sunday Monday Tuesday&quot; length(c(&quot;Sunday&quot;, &quot;Monday&quot;, &quot;Tuesday&quot;)) ## [1] 3 length(paste(&quot;Sunday&quot;, &quot;Monday&quot;, &quot;Tuesday&quot;)) ## [1] 1 The paste() function has an option called sep =. This tells R what you want to use to separate the values you’re pasting together in the output. The default is for R to use a space, as shown in the example above. To change the separator, you can change this option, and you can put in just about anything you want. For example, if you wanted to paste all the values together without spaces, you could use sep = &quot;&quot;: paste(&quot;Sunday&quot;, &quot;Monday&quot;, &quot;Tuesday&quot;, sep = &quot;&quot;) ## [1] &quot;SundayMondayTuesday&quot; As a shortcut, instead of using the sep = &quot;&quot; option, you could achieve the same thing using the paste0 function. This function is almost exacly like paste, but it defaults to &quot;&quot; (i.e., no space) as the separator between values by default: paste0(&quot;Sunday&quot;, &quot;Monday&quot;, &quot;Tuesday&quot;) ## [1] &quot;SundayMondayTuesday&quot; With pathnames, you will usually not want spaces. Therefore, you could think about using paste0() to write an object with the pathname you want to ultimately use in commands like list.files() and setwd(). This will allow you to keep your code cleaner, since you can now divide long pathnames over multiple lines: my_file &lt;- paste0(&quot;/Users/brookeanderson/Desktop/&quot;, &quot;RCourseFall2015/CourseText/heat_mort.csv&quot;) heat_mort &lt;- read.csv(my_file) You will end up using paste() and paste0() for many other applications, but this is a good example of how you can start using these functions to start to get a feel for them. 2.4 Reading data into R Data comes in files of all shapes and sizes. R has the capability to read data in from many of these, even priorietary files for other software (e.g., Excel and SAS files). As a small sample, here are some of the types of data files that R can read and work with: Flat files (much more about these in just a minute) Files from other statistical packages (SAS, Excel, Stata, SPSS) Tables on webpages (e.g., the table on ebola outbreaks near the end of this Wikipedia page) Data in a database (e.g., MySQL, Oracle) Data in JSON and XML formats Really crazy data formats used in other disciplines (e.g., netCDF files from climate research, MRI data stored in Analyze, NIfTI, and DICOM formats) Geographic shapefiles Data through APIs Often, it is possible to read in and clean up even incredibly messy data, by using functions like scan and readLines to read the data in a line at a time, and then using regular expressions (which I’ll cover in the “Intermediate” section of the course) to clean up each line as it comes in. In over a decade of coding in R, I think the only time I’ve come across a data file I couldn’t get into R was for proprietary precision agriculture data collected at harvest by a combine. 2.4.1 Reading local flat files Much of the data that you will want to read in will be in flat files. Basically, these are files that you can open using a text editor; the most common type you’ll work with are probably comma-separated files (often with a .csv or .txt file extension). Most flat files come in two general categories: Fixed width files Delimited files “.csv”: Comma-separated values “.tab”, “.tsv”: Tab-separated values Other possible delimiters: colon, semicolon, pipe (“|”) Fixed width files are files where a column always has the same width, for all the rows in the column. These tend to look very neat and easy-to-read when you open them in a text editor. For example, the first few rows of a fixed-width file might look like this: Course Number Day Time Intro to Epi 501 M/W/F 9:00-9:50 Advanced Epi 521 T/Th 1:00-2:15 Delimited files use some delimiter (for example, a column or a tab) to separate each column value within a row. The first few rows of a delimited file might look like this: Course, Number, Day, Time &quot;Intro to Epi&quot;, 501, &quot;M/W/F&quot;, &quot;9:00-9:50&quot; &quot;Advanced Epi&quot;, 521, &quot;T/Th&quot;, &quot;1:00-2:15&quot; These flat files can have a number of different file extensions. The most generic is .txt, but they will also have ones more specific to their format, like .csv for a comma-delimited file or .fwf for a fixed with file. R can read in data from both fixed with and delimited flat files. The only catch is that you need to tell R a bit more about the format of the flat file, including whether it is fixed width or delimited. If the file is fixed width, you will usually have to tell R the width of each column. If the file is delimited, you’ll need to tell R which delimiter is being used. If the file is delimited, you can use the read.table family of functions to read it in. This family of functions includes several specialized functions. All members of the read.table family are doing the same basic thing. The only difference is what defaults each function has for the separator (sep) and the decimal point (dec). Members of the read.table family include: Function Separator Decimial point read.table comma period read.csv comma period read.csv2 semi-colon comma read.delim tab period read.delim2 tab period You can use read.table to read in any delimited file, regardless of the separator and the value used for the decimal point. However, you will need to specify these values using the sep and dec parameters if they differ from the defaults for read.table (a space for the delimiter and period for the decimal). If you remember the more specialized function call, therefore, you can save yourself some typing. There are a few other default values besides sep and dec that differ between different functions in this family: header, for example, specifies whether the first row should be used as column names. For example, to read in the Ebola data, which is comma-delimited, you could either use read.table with a sep argument specified or use read.csv, in which case you don’t have to specify sep: # These two calls do the same thing ebola &lt;- read.table(&quot;data/country_timeseries.csv&quot;, sep = &quot;,&quot;, header = TRUE) ebola &lt;- read.csv(&quot;data/country_timeseries.csv&quot;) These functions have a number of different parameters to help you tell R how to read in data. For example, if the first few lines of the file aren’t part of the tabular data, you can tell R how many rows of the file to skip before it starts reading in the data. If the data uses an unusual value for missing data (e.g., -999), you can specify that, as well. Some of the interesting parameters with the read.table family of functions are: Option Description sep What is the delimiter in the data? skip How many lines of the start of the file should you skip? header Does the first line you read give column names? as.is Should you bring in strings as characters, not factors? nrows How many rows do you want to read in? na.strings How are missing values coded? Remember that you can always find out more about a function by looking at its help file. For example, check out ?read.table and ?read.fwf. You can also use the help files to determine the default values of arguments for each function. 2.4.2 The read_* functions The read.table family of functions are part of base R. There is a newer package called readr that has a family of read_* functions. These functions are very similar, but have some more sensible defaults. Compared to the read.table family of functions, the read_* functions: Work better with large datasets: faster, includes progress bar Have more sensible defaults (e.g., characters default to characters, not factors) Functions in the read_* family include: read_csv, read_tsv (specific delimiters) read_delim, read_table (generic) read_fwf read_log read_lines These functions work very similarly to functions from the read.table family. For example, to read in the Daily Show guest data, you can call: library(readr) daily_show &lt;- read_csv(&quot;data/daily_show_guests.csv&quot;, skip = 4) ## Parsed with column specification: ## cols( ## YEAR = col_integer(), ## GoogleKnowlege_Occupation = col_character(), ## Show = col_character(), ## Group = col_character(), ## Raw_Guest_List = col_character() ## ) The message that R prints after this call (“Parsed with column specification:..”) lets you know what classes R used for each column (this function tries to guess the appropriate function and, unlike the readr functions, will assign characters to a character rather than factor class – this is usually what you want). The readr package is a member of the tidyverse of packages. The tidyverse describes an evolving collection of R packages with a common philosophy, and they are unquestionably changing the way people code in R. Most were developed in part or full by Hadley Wickham and others at RStudio. Many of these packages are only a few years old, but have been rapidly adapted by the R community. As a result, newer examples of R code will often look very different from the code in older R scripts, including examples in books that are more than a few years old. In this course, I’ll focus on “tidyverse” functions when possible, but I do put in details about base R equivalent functions or processes at some points – this will help you interpret older code. You can use the tidyverse package to download all tidyverse packages at one. 2.4.3 Reading online flat files So far, I’ve only shown you how to read in data from files that are saved to your computer. R can also read in data directly from the web. If a flat file is posted online, you can read it into R in almost exactly the same way that you would read in a local file. The only difference is that you will use the file’s url instead of a local file path for the file argument. With the read_* family of functions, you can do this both for flat files from a non-secure webpage (i.e., one that starts with http) and for files from a secure webpage (i.e., one that starts with https), including GitHub and Dropbox. With the read.table family of functions, you can read in online flat files from non-secure webpages, but not from secure ones. For example, to read in the tab-separated file saved at this web address, which is non-secure, you can run: url &lt;- paste0(&quot;http://www2.unil.ch/comparativegenometrics&quot;, &quot;/docs/NC_006368.txt&quot;) ld_genetics &lt;- read_tsv(url) ld_genetics[1:5, 1:4] ## # A tibble: 5 × 4 ## pos nA nC nG ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 500 307 153 192 ## 2 1500 310 169 207 ## 3 2500 319 167 177 ## 4 3500 373 164 168 ## 5 4500 330 175 224 Similarly , to read in data from this GitHub repository of Ebola data, which is a secure website, you can run: url &lt;- paste0(&quot;https://raw.githubusercontent.com/cmrivers/&quot;, &quot;ebola/master/country_timeseries.csv&quot;) ebola &lt;- read_csv(url) ebola[1:3, 1:3] ## # A tibble: 3 × 3 ## Date Day Cases_Guinea ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 1/5/2015 289 2776 ## 2 1/4/2015 288 2775 ## 3 1/3/2015 287 2769 2.4.4 Saving and loading R objects You can save an R object you’ve created as an .RData file. To save an R object in a .RData file, use the save function: save(ebola, file = &quot;Ebola.RData&quot;) list.files() ## [1] &quot;_book&quot; &quot;_bookdown.yml&quot; ## [3] &quot;_build.sh&quot; &quot;_deploy.sh&quot; ## [5] &quot;_output.yml&quot; &quot;01-course_info.Rmd&quot; ## [7] &quot;02-prelim.Rmd&quot; &quot;03-databasics.Rmd&quot; ## [9] &quot;book.bib&quot; &quot;data&quot; ## [11] &quot;DESCRIPTION&quot; &quot;Ebola.RData&quot; ## [13] &quot;figures&quot; &quot;homework.Rmd&quot; ## [15] &quot;index.Rmd&quot; &quot;LICENSE&quot; ## [17] &quot;packages.bib&quot; &quot;preamble.tex&quot; ## [19] &quot;README.md&quot; &quot;references.Rmd&quot; ## [21] &quot;RProgrammingForResearch.Rmd&quot; &quot;RProgrammingForResearch.Rproj&quot; ## [23] &quot;slides&quot; &quot;style.css&quot; ## [25] &quot;toc.css&quot; &quot;vocabulary.Rmd&quot; Notice that, once you save the object, a new file named “Ebola.RData” is listed in the files in your current working directory. The default is for R to save the R object in your current working directory; to save it elsewhere, use a full relative or absolute pathname for the file argument. Once you’ve saved an R object, you can re-load it later using the load function with the object’s file path. For example, since I’ve saved this R object, I can remove it from my current R workspace using the rm function, after which it will not show up when I run ls: rm(ebola) ls() ## [1] &quot;ld_genetics&quot; &quot;my_dir&quot; &quot;url&quot; Then I can use the load command to re-load the object, after which it will again show up as an object in my R workspace: load(&quot;Ebola.RData&quot;) ls() ## [1] &quot;ebola&quot; &quot;ld_genetics&quot; &quot;my_dir&quot; &quot;url&quot; There is one caveat for saving R objects: some people suggest you avoid this if possible, to make your research more reproducible. Imagine someone wants to look at your data and code in 30 years. R might not work the same way, so you might not be able to read an .RData file. Notice that, if you try to open an .RData file in a text edit, it won’t make any sense. However, you can open flat files (e.g., .csv, .txt) and R scripts (.R) in text editors – you should still be able to do this regardless of what happens to R. Some potential exceptions, when it might be useful to save an R object, include when: You have an object that you need to save that has a structure that won’t work well in a flat file (a list rather than a dataframe, for example); or Your starting dataset is very large, and it would take a long time for you to read in your data fresh every time. In this case it may make sense to do some data cleaning and then save the cleaned R object as a .RData file, but be sure to also save the script you used to clean the raw data. 2.4.5 Reading other file types Later in the course, we’ll talk about how to open a variety of other file types in R. However, you might find it immediately useful to be able to read in files from other statistical programs. There are two “tidyverse” packages – readxl and haven – that help with this. They allow you to read in files from the following formats: File type Function Package Excel read_excel readxl SAS read_sas haven SPSS read_spss haven Stata `read_stata haven 2.5 Data cleaning Once you have loaded data into R, you’ll likely need to clean it up a little before you’re ready to analyze it. Here, I’ll go over the first steps of how to do that with functions from dplyr, another package in the tidyverse. Here are some of the most common data-cleaning tasks, along with the corresponding dplyr function for each: Task dplyr function Renaming columns rename Filtering to certain rows filter Selecting certain columns select Adding or changing columns mutate In this section, I’ll describe how to do each of these four tasks; in later sections of the course, we’ll go much deeper into how to clean messier data. For the examples in this section, I’ll use example data listing guests to the Daily Show. To follow along with these examples, you’ll want to load that data, as well as load the dplyr package (install it using install.packages if you have not already): library(dplyr) daily_show &lt;- read_csv(&quot;data/daily_show_guests.csv&quot;, skip = 4) I’ve used this data in previous examples, but as a reminder, here’s what it looks like: head(daily_show) ## # A tibble: 6 × 5 ## YEAR GoogleKnowlege_Occupation Show Group Raw_Guest_List ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1999 actor 1/11/99 Acting Michael J. Fox ## 2 1999 Comedian 1/12/99 Comedy Sandra Bernhard ## 3 1999 television actress 1/13/99 Acting Tracey Ullman ## 4 1999 film actress 1/14/99 Acting Gillian Anderson ## 5 1999 actor 1/18/99 Acting David Alan Grier ## 6 1999 actor 1/19/99 Acting William Baldwin 2.5.1 Renaming columns A first step is often re-naming the columns of the dataframe. It can be hard to work with a column name that: is long includes spaces includes upper case You can check out the column names for a dataframe using the colnames function, with the dataframe object as the argument. Several of the column names in daily_show have some of these issues: colnames(daily_show) ## [1] &quot;YEAR&quot; &quot;GoogleKnowlege_Occupation&quot; ## [3] &quot;Show&quot; &quot;Group&quot; ## [5] &quot;Raw_Guest_List&quot; To rename these columns, use rename. The basic syntax is: ## Generic code rename(dataframe, new_column_name_1 = old_column_name_1, new_column_name_2 = old_column_name_2) The first argument is the dataframe for which you’d like to rename columns. Then you list each pair of new versus old column names (in that order) for each of the columns you want to rename. To rename columns in the daily_show data using rename, for example, you would run: daily_show &lt;- rename(daily_show, year = YEAR, job = GoogleKnowlege_Occupation, date = Show, category = Group, guest_name = Raw_Guest_List) head(daily_show, 3) ## # A tibble: 3 × 5 ## year job date category guest_name ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1999 actor 1/11/99 Acting Michael J. Fox ## 2 1999 Comedian 1/12/99 Comedy Sandra Bernhard ## 3 1999 television actress 1/13/99 Acting Tracey Ullman Many of the functions in tidyverse packages, including those in dplyr, provide exceptions to the general rule about when to use quotation marks versus when to leave them off. Unfortunately, this may make it a bit hard to learn when to use quotation marks versus when not to. One way to think about this, which is a bit of an oversimplification but can help as you’re learning, is to assume that anytime you’re using a dplyr function, every column in the dataframe you’re working with has been loaded to your R session as its own object. 2.5.2 Selecting columns Next, you may want to select only some columns of the dataframe. You can use the select function from dplyr to subset the dataframe to certain columns. The basic structure of this command is: ## Generic code select(dataframe, column_name_1, column_name_2, ...) In this call, you first specify the dataframe to use and then list all of the column names to include in the output dataframe, with commas between each column name. For example, to select all columns in daily_show except year (since that information is already included in date), run: select(daily_show, job, date, category, guest_name) ## # A tibble: 2,693 × 4 ## job date category guest_name ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 actor 1/11/99 Acting Michael J. Fox ## 2 Comedian 1/12/99 Comedy Sandra Bernhard ## 3 television actress 1/13/99 Acting Tracey Ullman ## 4 film actress 1/14/99 Acting Gillian Anderson ## 5 actor 1/18/99 Acting David Alan Grier ## 6 actor 1/19/99 Acting William Baldwin ## 7 Singer-lyricist 1/20/99 Musician Michael Stipe ## 8 model 1/21/99 Media Carmen Electra ## 9 actor 1/25/99 Acting Matthew Lillard ## 10 stand-up comedian 1/26/99 Comedy David Cross ## # ... with 2,683 more rows Don’t forget that, if you want to change column names in the saved object, you must reassign the object to be the output of rename. If you run one of these cleaning functions without reassigning the object, R will print out the result, but the object itself won’t change. You can take advantage of this, as I’ve done in this example, to look at the result of applying a function to a dataframe without changing the original dataframe. This can be helpful as you’re figuring out how to write your code. The select function also provides some time-saving tools. For example, in the last example, we wanted all the columns except one. Instead of writing out all the columns we want, we can use - with the columns we don’t want to save time: daily_show &lt;- select(daily_show, -year) head(daily_show, 3) ## # A tibble: 3 × 4 ## job date category guest_name ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 actor 1/11/99 Acting Michael J. Fox ## 2 Comedian 1/12/99 Comedy Sandra Bernhard ## 3 television actress 1/13/99 Acting Tracey Ullman 2.5.3 Filtering to certain rows Next, you might want to filter the dataset down so that it only includes certain rows. For example, you might want to get a dataset with only the guests from 2015, or only guests who are scientists. You can use the filter function from dplyr to filter a dataframe down to a subset of rows. The syntax is: ## Generic code filter(dataframe, logical statement) The logical statement in this call gives the condition that a row must meet to be included in the output data frame. For example, if you want to create a data frame that only includes guests who were scientists, you can run: scientists &lt;- filter(daily_show, category == &quot;Science&quot;) head(scientists) ## # A tibble: 6 × 4 ## job date category guest_name ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 neurosurgeon 4/28/03 Science Dr Sanjay Gupta ## 2 scientist 1/13/04 Science Catherine Weitz ## 3 physician 6/15/04 Science Hassan Ibrahim ## 4 doctor 9/6/05 Science Dr. Marc Siegel ## 5 astronaut 2/13/06 Science Astronaut Mike Mullane ## 6 Astrophysicist 1/30/07 Science Neil deGrasse Tyson To build a logical statment to use in filter, you’ll need to know some of R’s logical operators. Some of the most commonly used ones are: Operator Meaning Example == equals category == &quot;Acting&quot; != does not equal category != &quot;Comedy %in% is in category %in% c(&quot;Academic&quot;, &quot;Science&quot;) is.na() is NA is.na(job) !is.na() is not NA !is.na(job) &amp; and year == 2015 &amp; category == &quot;Academic&quot; | or year == 2015 | category == &quot;Academic&quot; We’ll use these logical operators a lot more as the course continues, so they’re worth learning by heart. Two common errors with logical operators are: (1) Using = instead of == to check if two values are equal; and (2) Using == NA instead of is.na to check for missing observations. 2.5.4 Add or change columns You can change a column or add a new column using the mutate function from the dplyr package. That function has the syntax: # Generic code mutate(dataframe, changed_column = function(changed_column), new_column = function(other arguments)) For example, the job column in daily_show sometimes uses upper case and sometimes does not (this call uses the unique function to list only unique values in this column): head(unique(daily_show$job), 10) ## [1] &quot;actor&quot; &quot;Comedian&quot; &quot;television actress&quot; ## [4] &quot;film actress&quot; &quot;Singer-lyricist&quot; &quot;model&quot; ## [7] &quot;stand-up comedian&quot; &quot;actress&quot; &quot;comedian&quot; ## [10] &quot;Singer-songwriter&quot; To make all the observations in the job column lowercase, use the tolower function within a mutate function: mutate(daily_show, job = tolower(job)) ## # A tibble: 2,693 × 4 ## job date category guest_name ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 actor 1/11/99 Acting Michael J. Fox ## 2 comedian 1/12/99 Comedy Sandra Bernhard ## 3 television actress 1/13/99 Acting Tracey Ullman ## 4 film actress 1/14/99 Acting Gillian Anderson ## 5 actor 1/18/99 Acting David Alan Grier ## 6 actor 1/19/99 Acting William Baldwin ## 7 singer-lyricist 1/20/99 Musician Michael Stipe ## 8 model 1/21/99 Media Carmen Electra ## 9 actor 1/25/99 Acting Matthew Lillard ## 10 stand-up comedian 1/26/99 Comedy David Cross ## # ... with 2,683 more rows 2.5.5 Piping So far, I’ve shown how to use these dplyr functions one at a time to clean up the data, reassigning the dataframe object at each step. However, there’s a trick called “piping” that will let you clean up your code a bit when you’re writing a script to clean data. If you look at the format of these dplyr functions, you’ll notice that they all take a dataframe as their first argument: # Generic code rename(dataframe, new_column_name_1 = old_column_name_1, new_column_name_2 = old_column_name_2) select(dataframe, column_name_1, column_name_2) filter(dataframe, logical statement) mutate(dataframe, changed_column = function(changed_column), new_column = function(other arguments)) Without piping, you have to reassign the dataframe object at each step of this cleaning if you want the changes saved in the object: daily_show &lt;-read_csv(&quot;data/daily_show_guests.csv&quot;, skip = 4) daily_show &lt;- rename(daily_show, job = GoogleKnowlege_Occupation, date = Show, category = Group, guest_name = Raw_Guest_List) daily_show &lt;- select(daily_show, -YEAR) daily_show &lt;- mutate(daily_show, job = tolower(job)) daily_show &lt;- filter(daily_show, category == &quot;Science&quot;) Piping lets you clean this code up a bit. It can be used with any function that inputs a dataframe as its first argument. It pipes the dataframe created right before the pipe (%&gt;%) into the function right after the pipe. With piping, therefore, the same data cleaning looks like: daily_show &lt;-read_csv(&quot;data/daily_show_guests.csv&quot;, skip = 4) %&gt;% rename(job = GoogleKnowlege_Occupation, date = Show, category = Group, guest_name = Raw_Guest_List) %&gt;% select(-YEAR) %&gt;% mutate(job = tolower(job)) %&gt;% filter(category == &quot;Science&quot;) Notice that, when piping, the first argument (the name of the dataframe) is excluded from all function calls that follow a pipe. This is because piping sends the dataframe from the last step into each of these functions as the dataframe argument. 2.5.6 Base R equivalents to dplyr functions Just so you know, all of these dplyr functions have alternatives, either functions or processes, in base R: dplyr Base R equivalent rename Reassign colnames select Square bracket indexing filter subset mutate Use $ to change / create columns You will see these alternatives used in older code examples. 2.6 Dates in R As part of the data cleaning process, you may want to change the class of some of the columns in the dataframe. For example, you may want to change a column from a character to a date. Here are some of the most common vector classes in R: Class Example character “Chemistry”, “Physics”, “Mathematics” numeric 10, 20, 30, 40 factor Male [underlying number: 1], Female [2] Date “2010-01-01” [underlying number: 14,610] logical TRUE, FALSE To find out the class of a vector (including a column in a dataframe – remember each column can be thought of as a vector), you can use class(): class(daily_show$date) ## [1] &quot;character&quot; It is especially common to need to convert dates during the data cleaning process, since date columns will usually be read into R as characters or factors – you can do some interesting things with vectors that are in a Date class that you cannot do with a vector in a character class. To convert a vector to the Date class, you can use the as.Date function. For example, to convert the date column in the daily_show data into a Date class, you can run: daily_show &lt;- mutate(daily_show, date = as.Date(date, format = &quot;%m/%d/%y&quot;)) head(daily_show, 3) ## # A tibble: 3 × 4 ## job date category guest_name ## &lt;chr&gt; &lt;date&gt; &lt;chr&gt; &lt;chr&gt; ## 1 neurosurgeon 2003-04-28 Science Dr Sanjay Gupta ## 2 scientist 2004-01-13 Science Catherine Weitz ## 3 physician 2004-06-15 Science Hassan Ibrahim class(daily_show$date) ## [1] &quot;Date&quot; Once you have an object in the Date class, you can do things like plot by date, calculate the range of dates, and calculate the total number of days the dataset covers: range(daily_show$date) diff(range(daily_show$date)) You can convert dates expressed in a number of different ways into a Date class in R, as long as you can explain to R how to parse the format that the date is in before you convert it. The only tricky thing in converting objects into a Date class is learning the abbreviations for the format option of the as.Date function. Here are some common ones: Abbreviation Meaning %m Month as a number (e.g., 1, 05) %B Full month name (e.g., August) %b Abbreviated month name (e.g., Aug) %y Two-digit year (e.g., 99) %Y Four-digit year (e.g., 1999) %A Full weekday (e.g., Monday) %a Abberviated weekday (e.g., Mon) Here are some examples of what you would specify for the format argument of as.Date for some different original formats of date columns: Your date format 10/23/2008 “%m/%d%Y” 08-10-23 “%y-%m-%d” Oct. 23 2008 “%b. %d %Y” October 23, 2008 “%B %d, %Y” Thurs, 23 October 2008 “%a, %d %B %Y” You must use the format argument to specify what your date column looks like before it’s converted to a Date class, not how you’d like it to look after its converted. Once an objects is in a date class, it will always be printed out using a common format, unless you change it back into a character class. (Confusingly, there is a format function that you can use to convert from a Date class to a character class and, in that case, the format argument does specify how the final date will look. This is mainly useful as a last step in data analysis, when you’re creating plot labels of table columns, for example.) There is also a function in the tidyverse, called lubridate, that helps in parsing dates. In many cases you can use functions from this package to parse dates much more easily, without having to specify specific starting formats. The ymd function from lubridate can be used to parse a column into a Date clase, regardless of the original format of the date, as long as the date elements are in the order: year, month, day. For example: library(lubridate) ymd(&quot;2008-10-13&quot;) ## [1] &quot;2008-10-13&quot; ymd(&quot;&#39;08 Oct 13&quot;) ## [1] &quot;2008-10-13&quot; ymd(&quot;&#39;08 Oct 13&quot;) ## [1] &quot;2008-10-13&quot; The lubridate package has similar functions for other date orders or for date-times, including: dmy mdy ymd_h ymd_hm We could have used these to transform the date in daily_show, using the following pipe chain: daily_show &lt;- read_csv(&quot;data/daily_show_guests.csv&quot;, skip = 4) %&gt;% rename(job = GoogleKnowlege_Occupation, date = Show, category = Group, guest_name = Raw_Guest_List) %&gt;% select(-YEAR) %&gt;% mutate(date = mdy(date)) %&gt;% filter(category == &quot;Science&quot;) head(daily_show, 2) ## # A tibble: 2 × 4 ## job date category guest_name ## &lt;chr&gt; &lt;date&gt; &lt;chr&gt; &lt;chr&gt; ## 1 neurosurgeon 2003-04-28 Science Dr Sanjay Gupta ## 2 scientist 2004-01-13 Science Catherine Weitz The lubridate package also includes functions to pull out certain elements of a date, including: wday mday yday month quarter year For example, we could use wday to create a new column with the weekday of each show: mutate(daily_show, show_day = wday(date, label = TRUE)) %&gt;% select(date, show_day, guest_name) %&gt;% slice(1:5) ## # A tibble: 5 × 3 ## date show_day guest_name ## &lt;date&gt; &lt;ord&gt; &lt;chr&gt; ## 1 2003-04-28 Mon Dr Sanjay Gupta ## 2 2004-01-13 Tues Catherine Weitz ## 3 2004-06-15 Tues Hassan Ibrahim ## 4 2005-09-06 Tues Dr. Marc Siegel ## 5 2006-02-13 Mon Astronaut Mike Mullane 2.7 In-course Exercise 2.7.1 Checking out directory structures Download the whole directory for this week from Github. Put the “data” directory as a subdirectory in your directory for this class. To do that, go the the GitHub page for the course and, in the top right, choose “Clone or Download” and then choose “Download ZIP”. Then move the “data” subdirectory into your course directory and throw away rest of what you downloaded. Unfortunately, while GitHub will let you download data files one at a time, it doesn’t offer a straightforward way of downloading a whole subdirectory from a repository. For this task, where you’re pulling all of the data in the “data” subdirectory of the course repository, the quickest method is to download the entire directory, find and move the subdirectory that you need, and then delete the rest. Look through the structure of the “data” directory. What files are in the directory? What subdirectories? Sketch out the structure of this directory. Create a new R script to put all the code you use for this exercise. Create a subdirectory in your course directory called “R” and save this script there using a .R extension (e.g., “week_2.R”). 2.7.2 Using relative and absolute file pathnames Once you have the data, I’d like you to try using setwd() to move around the directories on your computer. For this section of the exercise, you’ll try to open the same file from different working directories, to get practice using absolute and relative file pathnames. Start by changing your working directory to the “data” subdirectory you just downloaded. For me, the absolute path to that is /Users/brookeanderson/RProgrammingForResearch/data, so to change my working directory to this one using an absolute file pathname, I would run: setwd(&quot;/Users/brookeanderson/RProgrammingForResearch/data&quot;) getwd() The absolute pathname will be different for each person, based on the directories on his or her computer and where he or she saved this particular directory. Since “~” is shorthand for my home directory (“/Users/brookeanderson”), I could also use the absolute pathname “~/RProgrammingForResearch/data” when setting this directory as my working directory. Now, use the list.files function to make sure you have all the files that you just pulled in your current working directory: list.files() ## [1] &quot;accident.csv&quot; &quot;App-1&quot; ## [3] &quot;App-2&quot; &quot;country_timeseries.csv&quot; ## [5] &quot;daily_show_guests.csv&quot; &quot;deaths-weather.csv&quot; ## [7] &quot;example_choropleth.Rdata&quot; &quot;icd-10.xls&quot; ## [9] &quot;ICU_Data_Code_Sheet.pdf&quot; &quot;icu.sas7bdat&quot; ## [11] &quot;ld_genetics.txt&quot; &quot;measles_data&quot; ## [13] &quot;mexico_deaths.csv&quot; &quot;mexico_exposure.csv&quot; ## [15] &quot;serial_map_data.csv&quot; &quot;serial_phone_data.csv&quot; ## [17] &quot;titanic.csv&quot; Try the following tasks: Read in the ebola data in country_timeseries.csv. Save it as the R object ebola. How many rows and columns does it have? What are the names of the columns? Now try moving up one directory from your current working directory (which should be the “data” directory), into the directory you created for this course. What happens if you use the same code as before to read in the file? What do you need to change to be able to read the file in from here? Try to read in the data from here using: A relative pathname An absolute pathname Now move down into the subdirectory of “data” called “measles_data”. Try to read in the Ebola data from this working directory using: A relative pathname An absolute pathname Which method (absolute or relative pathnames) always used the same code, regardless of your current working directory? Which method used different code, depending on the starting working directory? Example R code: getwd() ## Make sure you&#39;re in the &quot;data&quot; directory to start ## [1] &quot;/Users/brookeanderson/RProgrammingForResearch/data&quot; ebola &lt;- read.csv(&quot;country_timeseries.csv&quot;, header = TRUE) ebola[1:5, 1:5] ## Date Day Cases_Guinea Cases_Liberia Cases_SierraLeone ## 1 1/5/2015 289 2776 NA 10030 ## 2 1/4/2015 288 2775 NA 9780 ## 3 1/3/2015 287 2769 8166 9722 ## 4 1/2/2015 286 NA 8157 NA ## 5 12/31/2014 284 2730 8115 9633 dim(ebola) # To figure out number of rows and columns ## [1] 122 18 colnames(ebola) # To figure out column names ## [1] &quot;Date&quot; &quot;Day&quot; &quot;Cases_Guinea&quot; ## [4] &quot;Cases_Liberia&quot; &quot;Cases_SierraLeone&quot; &quot;Cases_Nigeria&quot; ## [7] &quot;Cases_Senegal&quot; &quot;Cases_UnitedStates&quot; &quot;Cases_Spain&quot; ## [10] &quot;Cases_Mali&quot; &quot;Deaths_Guinea&quot; &quot;Deaths_Liberia&quot; ## [13] &quot;Deaths_SierraLeone&quot; &quot;Deaths_Nigeria&quot; &quot;Deaths_Senegal&quot; ## [16] &quot;Deaths_UnitedStates&quot; &quot;Deaths_Spain&quot; &quot;Deaths_Mali&quot; ## Move up one directory setwd(&quot;..&quot;) getwd() ## Get the file using the relative pathname ebola &lt;- read.csv(&quot;data/country_timeseries.csv&quot;, header = TRUE) ebola[1:5, 1:5] ## Date Day Cases_Guinea Cases_Liberia Cases_SierraLeone ## 1 1/5/2015 289 2776 NA 10030 ## 2 1/4/2015 288 2775 NA 9780 ## 3 1/3/2015 287 2769 8166 9722 ## 4 1/2/2015 286 NA 8157 NA ## 5 12/31/2014 284 2730 8115 9633 ## Get the file using the absolute pathname abs_path &lt;- paste0(&quot;/Users/brookeanderson/RProgrammingForResearch/&quot;, &quot;data/country_timeseries.csv&quot;) abs_path ## [1] &quot;/Users/brookeanderson/RProgrammingForResearch/data/country_timeseries.csv&quot; ebola &lt;- read.csv(abs_path, header = TRUE) ebola[1:5, 1:5] ## Date Day Cases_Guinea Cases_Liberia Cases_SierraLeone ## 1 1/5/2015 289 2776 NA 10030 ## 2 1/4/2015 288 2775 NA 9780 ## 3 1/3/2015 287 2769 8166 9722 ## 4 1/2/2015 286 NA 8157 NA ## 5 12/31/2014 284 2730 8115 9633 ## Reset your working directory as your directory for this course ## and then check that you&#39;re in the right place getwd() ## [1] &quot;/Users/brookeanderson/RProgrammingForResearch&quot; ## Move into the measles_data subdirectory setwd(&quot;data/measles_data&quot;) getwd() ## [1] &quot;/Users/brookeanderson/RProgrammingForResearch/data/measles_data&quot; ## Get the file using the relative pathname ebola &lt;- read.csv(&quot;../country_timeseries.csv&quot;, header = TRUE) ebola[1:5, 1:5] ## Date Day Cases_Guinea Cases_Liberia Cases_SierraLeone ## 1 1/5/2015 289 2776 NA 10030 ## 2 1/4/2015 288 2775 NA 9780 ## 3 1/3/2015 287 2769 8166 9722 ## 4 1/2/2015 286 NA 8157 NA ## 5 12/31/2014 284 2730 8115 9633 # Get the file using the absolute pathname (note: we&#39;ve already assigned # the absolute pathname in the `abs_path` object, so we can just use that # object in the `read.csv` call here, rather than typing out the full # absolute pathname again.) ebola &lt;- read.csv(abs_path, header = TRUE) ebola[1:5, 1:5] ## Date Day Cases_Guinea Cases_Liberia Cases_SierraLeone ## 1 1/5/2015 289 2776 NA 10030 ## 2 1/4/2015 288 2775 NA 9780 ## 3 1/3/2015 287 2769 8166 9722 ## 4 1/2/2015 286 NA 8157 NA ## 5 12/31/2014 284 2730 8115 9633 If you have extra time: Find out some more about this Ebola dataset by checking out Caitlin Rivers’ Ebola data GitHub repository. Who is Caitlin Rivers? How did she put this dataset together? Search for R code related to Ebola research on GitHub. Go to the GitHub home page and use the search bar to search for “ebola”. On the results page, scroll down and use the “Language” sidebar on the left to choose repositories with R code. Did you find any interesting projects? When you list.files() when your working directory is the “data” directory, almost everything listed has a file extension, like .csv, .xls, .sas7bdat. One thing does not. Which one? Why does this listing not have a file extension? 2.7.3 Reading in different types of files First, make sure your reset your working directory to your course directory: setwd(&quot;~/RProgrammingForResearch/&quot;) Now you’ll try reading in data from a variety of types of file formats. All of these files are stored in the “data” subdirectory of your current working directory, so you’ll use filenames throughout that start with “data/”. Try the following tasks: What type of flat file do you think the “ld_genetics.txt” file is? See if you can read it in and save it as the R object ld_genetics. Use the summary function to check out basic statistics on the data. Check out the file “measles_data/02-09-2015.txt”. What type of flat file do you think it is? Stay in the “data” directory and use a relative pathname to read the file in and save it as the R object ca_measles. Use the col.names option to name the columns “city” and “count”. What would the default column names be if you didn’t use this option? Read in the Excel file “icd-10.xls” and assign it to the object name idc10. Use the readxl package to do that (examples are at the bottom of the linked page). Read in the SAS file icu.sas7bdat. To do this, use the haven package. Read the file into the R object icu. Example R code: ld_genetics &lt;- read.delim(&quot;data/ld_genetics.txt&quot;, header = TRUE) summary(ld_genetics) ## pos nA nC nG ## Min. : 500 Min. :185 Min. :120.0 Min. : 85.0 ## 1st Qu.: 876000 1st Qu.:288 1st Qu.:173.0 1st Qu.:172.0 ## Median :1751500 Median :308 Median :190.0 Median :189.0 ## Mean :1751500 Mean :309 Mean :191.9 Mean :191.8 ## 3rd Qu.:2627000 3rd Qu.:329 3rd Qu.:209.0 3rd Qu.:208.0 ## Max. :3502500 Max. :463 Max. :321.0 Max. :326.0 ## nT GCsk TAsk cGCsk ## Min. :188.0 Min. :-189.0000 Min. :-254.000 Min. : -453 ## 1st Qu.:286.0 1st Qu.: -30.0000 1st Qu.: -36.000 1st Qu.:10796 ## Median :306.0 Median : 0.0000 Median : -2.000 Median :23543 ## Mean :307.2 Mean : -0.1293 Mean : -1.736 Mean :22889 ## 3rd Qu.:328.0 3rd Qu.: 29.0000 3rd Qu.: 32.500 3rd Qu.:34940 ## Max. :444.0 Max. : 134.0000 Max. : 205.000 Max. :46085 ## cTAsk ## Min. :-6247 ## 1st Qu.: 1817 ## Median : 7656 ## Mean : 7855 ## 3rd Qu.:15036 ## Max. :19049 ca_measles &lt;- read.delim(&quot;data/measles_data/02-09-2015.txt&quot;, header = FALSE, col.names = c(&quot;city&quot;, &quot;count&quot;)) head(ca_measles) ## city count ## 1 ALAMEDA 6 ## 2 LOS ANGELES 20 ## 3 City of Long Beach 2 ## 4 City of Pasadena 4 ## 5 MARIN 2 ## 6 ORANGE 34 library(readxl) icd10 &lt;- read_excel(&quot;data/icd-10.xls&quot;) ## DEFINEDNAME: 20 00 00 01 0b 00 00 00 01 00 00 00 00 00 00 06 3b 00 00 00 00 51 2b 00 00 0a 00 ## DEFINEDNAME: 20 00 00 01 0b 00 00 00 01 00 00 00 00 00 00 06 3b 00 00 00 00 51 2b 00 00 0a 00 ## DEFINEDNAME: 20 00 00 01 0b 00 00 00 01 00 00 00 00 00 00 06 3b 00 00 00 00 51 2b 00 00 0a 00 ## DEFINEDNAME: 20 00 00 01 0b 00 00 00 01 00 00 00 00 00 00 06 3b 00 00 00 00 51 2b 00 00 0a 00 head(icd10) ## # A tibble: 6 × 2 ## Code `ICD Title` ## &lt;chr&gt; &lt;chr&gt; ## 1 A00-B99 I. Certain infectious and parasitic diseases ## 2 A00-A09 Intestinal infectious diseases ## 3 A00 Cholera ## 4 A00.0 Cholera due to Vibrio cholerae 01, biovar cholerae ## 5 A00.1 Cholera due to Vibrio cholerae 01, biovar eltor ## 6 A00.9 Cholera, unspecified library(haven) icu &lt;- read_sas(&quot;data/icu.sas7bdat&quot;) icu[1:5, 1:5] ## # A tibble: 5 × 5 ## ID STA AGE GENDER RACE ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 4 1 87 1 1 ## 2 8 0 27 1 1 ## 3 12 0 59 0 1 ## 4 14 0 77 0 1 ## 5 27 1 76 1 1 If you have extra time: Is there a way to read the “ld_genetics.txt” file in using read.table() and specific options? If so, try to read the data in using that function. Why can you use both read.delim and read.table to read in this file? Example R code: ## Using the read.table function ld_genetics &lt;- read.table(&quot;data/ld_genetics.txt&quot;, header = TRUE, sep = &quot;\\t&quot;) ld_genetics[1:5, 1:5] ## pos nA nC nG nT ## 1 500 307 153 192 348 ## 2 1500 310 169 207 314 ## 3 2500 319 167 177 337 ## 4 3500 373 164 168 295 ## 5 4500 330 175 224 271 2.7.4 Cleaning up data Try out the following tasks: You now have an R object called ebola. Create an object called ebola_liberia that only has the columns with the Date and cases and deaths in Liberia. How many observations do you have? Change the column names to date, cases, and deaths. What class is the date column currently in? Convert it to a Date object. If it’s currently a factor, you will need to first convert it to a character (do you know why)? What are the starting and ending dates of this data? This data has earliest dates last and latest dates first. Often, we want our data in chronological order. Change the dataset so it’s in chronological order. Hint: You can use row indices to do this, if you use row indices to list the data from last row to first. For example, what do you get if you do 3:1? What about if you do ebola[3:1, ]? Think of how you can extend this, along with nrow() do this task. There are a number of different ways to do this step, and we’ll talk about more ways later in the course. Example R code: library(dplyr) ## Create a subset with just the Liberia columns and Date ebola_liberia &lt;- select(ebola, Date, Cases_Liberia, Deaths_Liberia) head(ebola_liberia) ## Date Cases_Liberia Deaths_Liberia ## 1 1/5/2015 NA NA ## 2 1/4/2015 NA NA ## 3 1/3/2015 8166 3496 ## 4 1/2/2015 8157 3496 ## 5 12/31/2014 8115 3471 ## 6 12/28/2014 8018 3423 ## How many rows does the whole dataset have? nrow(ebola_liberia) ## [1] 122 ## Rename the columns ebola_liberia &lt;- rename(ebola_liberia, date = Date, cases = Cases_Liberia, deaths = Deaths_Liberia) ebola_liberia[1:3, ] ## date cases deaths ## 1 1/5/2015 NA NA ## 2 1/4/2015 NA NA ## 3 1/3/2015 8166 3496 ## What class is the `date` column? class(ebola_liberia$date) ## [1] &quot;factor&quot; ## Use the `mdy` from `lubridate` to convert to Date class library(lubridate) ebola_liberia &lt;- mutate(ebola_liberia, date = mdy(date)) head(ebola_liberia$date) ## [1] &quot;2015-01-05&quot; &quot;2015-01-04&quot; &quot;2015-01-03&quot; &quot;2015-01-02&quot; &quot;2014-12-31&quot; ## [6] &quot;2014-12-28&quot; ## What are the starting and ending dates? range(ebola_liberia$date) ## [1] &quot;2014-03-22&quot; &quot;2015-01-05&quot; ## Re-order the dataset from last to first ebola_liberia &lt;- ebola_liberia[nrow(ebola_liberia):1, ] ebola_liberia[1:3, ] ## date cases deaths ## 122 2014-03-22 NA NA ## 121 2014-03-24 NA NA ## 120 2014-03-25 NA NA If you have extra time: Limit the ebola_liberia dataset just to the days with non-missing case data. How many observations do you have now? Write a pipe chain for the full data cleaning process for creating ebola_liberia up to this point. Hint: Try using the arrange function from dplyr to re-order the data by dates in this pipe. Try using the basic plotting function, plot(), to plot the number of cases over time. Do you think that the cases variable is measuring the count of cases for that day, or the cumulative number of cases up to that day? See if you can figure out more on Caitlin Rivers’ GitHub documentation. Do you notice any potential data quality issues in this data? Hint: The plot() function takes, as required arguments, the vector you want to plot on the x-axis and then the vector you want to plot on the y-axis, like plot([x vector], [y vector]). If you are pulling the vectors from a dataset, you will need to use indexing to pull out the column you want as a vector, like plot([dataframe name]$[column name for x], [dataframe]$[column name for y]). Example R code: ## Remove observations with missing cases ebola_liberia &lt;- filter(ebola_liberia, !is.na(cases)) nrow(ebola_liberia) ## [1] 83 ## Rewrite the data cleaning for `ebola_liberia` using a pipe chain ebola_liberia &lt;- ebola %&gt;% select(Date, Cases_Liberia, Deaths_Liberia) %&gt;% rename(date = Date, cases = Cases_Liberia, deaths = Deaths_Liberia) %&gt;% mutate(date = mdy(date)) %&gt;% arrange(date) %&gt;% filter(!is.na(cases)) ## Plot the data plot(ebola_liberia$date, ebola_liberia$cases) 2.7.5 A taste of what’s to come… Here’s an example to give you a feel for why it’s worth learning all these different things about directories, list.files, and pathnames. The measles_data subdirectory includes counts of measles made at different times in different cities in California. Say that we wanted to read them all in and make put them into one long dataframe with the variables city, count, and date. You can put together the things you’ve learned so far, along with a few new ideas (including doing a loop), to do this very easily. We’ll talk more later about using loops and functions to make your programming more efficient, but for right now just look through this code and see if you can get a feel for how it’s working (make sure your directory for this course is your working directory): ## Create a vector of all the file names in the `measles_data` subdirectory measles_files &lt;- list.files(&quot;data/measles_data&quot;) measles_files ## [1] &quot;02-09-2015.txt&quot; &quot;02-11-2015.txt&quot; &quot;02-13-2015.txt&quot; &quot;02-18-2015.txt&quot; ## [5] &quot;02-20-2015.txt&quot; &quot;02-23-2015.txt&quot; &quot;02-25-2015.txt&quot; &quot;02-27-2015.txt&quot; ## [9] &quot;03-02-2015.txt&quot; &quot;03-06-2015.txt&quot; &quot;03-13-2015.txt&quot; &quot;03-20-2015.txt&quot; ## [13] &quot;03-27-2015.txt&quot; &quot;04-03-2015.txt&quot; &quot;04-10-2015.txt&quot; &quot;04-17-2015.txt&quot; ## Create a vector of all the dates for files by taking the ## `.txt` off each of these file names and change it into ## a date measles_dates &lt;- sub(&quot;.txt&quot;, &quot;&quot;, measles_files) measles_dates ## [1] &quot;02-09-2015&quot; &quot;02-11-2015&quot; &quot;02-13-2015&quot; &quot;02-18-2015&quot; &quot;02-20-2015&quot; ## [6] &quot;02-23-2015&quot; &quot;02-25-2015&quot; &quot;02-27-2015&quot; &quot;03-02-2015&quot; &quot;03-06-2015&quot; ## [11] &quot;03-13-2015&quot; &quot;03-20-2015&quot; &quot;03-27-2015&quot; &quot;04-03-2015&quot; &quot;04-10-2015&quot; ## [16] &quot;04-17-2015&quot; class(measles_dates) ## [1] &quot;character&quot; measles_dates &lt;- mdy(measles_dates) measles_dates ## [1] &quot;2015-02-09&quot; &quot;2015-02-11&quot; &quot;2015-02-13&quot; &quot;2015-02-18&quot; &quot;2015-02-20&quot; ## [6] &quot;2015-02-23&quot; &quot;2015-02-25&quot; &quot;2015-02-27&quot; &quot;2015-03-02&quot; &quot;2015-03-06&quot; ## [11] &quot;2015-03-13&quot; &quot;2015-03-20&quot; &quot;2015-03-27&quot; &quot;2015-04-03&quot; &quot;2015-04-10&quot; ## [16] &quot;2015-04-17&quot; ## Before I show the loop, let me talk you through some ## of the parts of it: i &lt;- 1 # I&#39;m setting the index to 1 ## Now I&#39;ll use `paste0` to create the first file name ## I want to read. file_name &lt;- paste0(&quot;data/measles_data/&quot;, measles_files[i]) file_name ## [1] &quot;data/measles_data/02-09-2015.txt&quot; ## Now I&#39;ll read in that tab-delimited file df &lt;- read_tsv(file_name, col_names = c(&quot;city&quot;, &quot;count&quot;)) head(df) ## # A tibble: 6 × 2 ## city count ## &lt;chr&gt; &lt;int&gt; ## 1 ALAMEDA 6 ## 2 LOS ANGELES 20 ## 3 City of Long Beach 2 ## 4 City of Pasadena 4 ## 5 MARIN 2 ## 6 ORANGE 34 ## Now I&#39;ll add on a column with the date for all the ## values from that file. Notice that I&#39;m using `i` to ## index this, as well df &lt;- mutate(df, date = measles_dates[i]) head(df) ## # A tibble: 6 × 3 ## city count date ## &lt;chr&gt; &lt;int&gt; &lt;date&gt; ## 1 ALAMEDA 6 2015-02-09 ## 2 LOS ANGELES 20 2015-02-09 ## 3 City of Long Beach 2 2015-02-09 ## 4 City of Pasadena 4 2015-02-09 ## 5 MARIN 2 2015-02-09 ## 6 ORANGE 34 2015-02-09 ## Loop through and read in files. After the first file, ## add on the new information to the data that&#39;s already ## been read in. Note that you can use `rbind` to add on ## new rows to a dataframe as long as the new and old rows ## have the same number of columns and the same column names. for(i in 1:length(measles_files)){ file_name &lt;- paste0(&quot;data/measles_data/&quot;, measles_files[i]) df &lt;- read.delim(file_name, header = FALSE, col.names = c(&quot;city&quot;, &quot;count&quot;)) df &lt;- mutate(df, date = measles_dates[i]) if(i == 1){ ca_measles &lt;- df } else { ca_measles &lt;- rbind(ca_measles, df) } } dim(ca_measles) ## [1] 232 3 summary(ca_measles) ## city count date ## ALAMEDA : 16 Min. : 1.000 Min. :2015-02-09 ## City of Long Beach: 16 1st Qu.: 2.000 1st Qu.:2015-02-20 ## City of Pasadena : 16 Median : 4.000 Median :2015-03-02 ## LOS ANGELES : 16 Mean : 8.698 Mean :2015-03-07 ## MARIN : 16 3rd Qu.:12.000 3rd Qu.:2015-03-27 ## ORANGE : 16 Max. :35.000 Max. :2015-04-17 ## (Other) :136 library(ggplot2) ggplot(subset(ca_measles, city %in% c(&quot;LOS ANGELES&quot;, &quot;SAN DIEGO&quot;, &quot;ORANGE&quot;)), aes(x = date, y = count)) + geom_line() + facet_grid(. ~ city) "],
["exploring-data-1.html", "Chapter 3 Exploring data #1 3.1 Data from a package 3.2 Plots to explore data 3.3 Simple statistics functions 3.4 Logical vectors 3.5 Regression models 3.6 In-course exercise", " Chapter 3 Exploring data #1 Download a pdf of the lecture slides covering this topic. 3.1 Data from a package So far we’ve covered three ways to get data into R: From flat files (either on your computer or online) From files like SAS and Excel From R objects (i.e., using load()) Many R packages come with their own data, which is very easy to load and use. For example, the faraway package, which complements Julian Faraway’s book Linear Models with R (available as an ebook from the CSU library), has a dataset called worldcup that I’ll use for some examples and that you’ll use for part of this week’s in-course exercise. To load this dataset, first load the package with the data (faraway) and then use the data() function with the dataset name (“worldcup”) as the argument to the data function: library(faraway) data(&quot;worldcup&quot;) Unlike most data objects you’ll work with, datasets that are part of an R package will often have their own help files. You can access this help file for a dataset using the ? operator with the dataset’s name: ?worldcup This helpful will usually include information about the size of the dataset, as well as definitions for each of the columns. To get a list of all of the datasets that are available in the packages you currently have loaded, run data() without an option inside the parentheses: data() If you run the library function without any arguments (library()), it works in a similar way– R will open a list of all the R packages that you have installed on your computer and can open with a library call. 3.2 Plots to explore data Exploratory data analysis is a key step in data analysis, and plotting your data in different ways is an important part of this process. In this section, I will focus on the basics of ggplot2 plotting, to get you started creating some plots to explore your data. This section will focus on making useful, rather than attractive graphs, since at this stage we are focusing on exploring data for yourself rather than presenting results to others. Next week, I will explain more about how you can customize ggplot objects, to help you make plots to communicate with others. All of the plots we’ll make today will use the ggplot2 package (another member of the tidyverse!). If you don’t already have that installed, you’ll need to install it. You then need to load the package in your current session of R: # install.packages(&quot;ggplot2&quot;) ## Uncomment and run if you don&#39;t have `ggplot2` installed library(ggplot2) The process of creating a plot using ggplot2 follows conventions that are a bit different than most of the code you’ve seen so far in R (although it is somewhat similar to the idea of piping I introduced in the last chapter). The basic steps behind creating a plot with ggplot2 are: Create an object of the ggplot class, typically specifying the data and some or all of the aesthetics; Add on geoms and other elements to create and customize the plot, using +. You can add on one or many geoms and other elements to create plots that range from very simple to very customized. This week, we’ll focus on simple geoms and added elements, and then explore more detailed customization next week. If R gets to the end of a line and there is not some indication that the call is not over (e.g., %&gt;% for piping or + for ggplot2 plots), R interprets that as a message to run the call without reading in further code. A common error when writing ggplot2 code is to put the + to add a geom or element at the beginning of a line rather than the end of a previous line– in this case, R will try to execute the call too soon. To avoid errors, be sure to end lines with +, don’t start lines with it. 3.2.1 Initializing a ggplot object The first step in creating a plot using ggplot2 is to create a ggplot object. This object will not, by itself, create a plot with anything in it. Instead, it typically specifies the data frame you want to use and which aesthetics will be mapped to certain columns of that data frame (aesthetics are explained more in the next subsection). Use the following conventions to initialize a ggplot object: ## Generic code object &lt;- ggplot(dataframe, aes(x = column_1, y = column_2)) The data frame is the first parameter in a ggplot call and, if you like, you can use the parameter definition with that call (e.g., data = dataframe). Aesthetics are defined within an aes function call that typically is used within the ggplot call. While the ggplot call is the place where you will most often see an aes call, aes can also be used within the calls to add specific geoms. This can be particularly useful if you want to map aesthetics differently for different geoms in your plot. We’ll see some examples of this use of aes more in later sections, when we talk about customizing plots. The data parameter can also be used in geom calls, to use a different data frame from the one defined when creating the original ggplot object, although this tends to be less common. 3.2.2 Plot aesthetics Aesthetics are properties of the plot that can show certain elements of the data. For example, in Figure 3.1, color shows (is mapped to) gender, x-position shows height, and y-position shows weight in a sample data set of measurements of children in Nepal. Figure 3.1: Example of how different properties of a plot can show different elements to the data. Here, color indicates gender, position along the x-axis shows height, and position along the y-axis shows weight. This example is a subset of data from the nepali dataset in the faraway package. Any of these aesthetics could also be given a constant value, instead of being mapped to an element of the data. For example, all the points could be red, instead of showing gender. Which aesthetics are required for a plot depend on which geoms (more on those in a second) you’re adding to the plot. You can find out the aesthetics you can use for a geom in the “Aesthetics” section of the geom’s help file (e.g., ?geom_point). Required aesthetics are in bold in this section of the help file and optional ones are not. Common plot aesthetics you might want to specify include: Code Description x Position on x-axis y Position on y-axis shape Shape color Color of border of elements fill Color of inside of elements size Size alpha Transparency (1: opaque; 0: transparent) linetype Type of line (e.g., solid, dashed) 3.2.3 Adding geoms Next, you’ll want to add one or more geoms to create the plot. You can add these with + after the ggplot statement to initialize the ggplot object. Some of the most common geoms are: Plot type ggplot2 function Histogram (1 numeric variable) geom_histogram Scatterplot (2 numeric variables) geom_point Boxplot (1 numeric variable, possibly 1 factor variable) geom_boxplot Line graph (2 numeric variables) geom_line 3.2.4 Constant aesthetics Instead of mapping an aesthetic to an element of your data, you can use a constant value for it. For example, you may want to make all the points green, rather than having color map to gender: In this case, you’ll define that aesthetic when you add the geom, outside of an aes statement. In R, you can specify the shape of points with a number. Figure 3.2 shows the shapes that correspond to the numbers 1 to 25 in the shape aesthetic. This figure also provides an example of the difference between color (black for all these example points) and fill (red for these examples). You can see that some point shapes include a fill (21 for example), while some are either empty (1) or solid (19). Figure 3.2: Examples of the shapes corresponding to different numeric choices for the shape aesthetic. For all examples, color is set to black and fill to red. If you want to set color to be a constant value, you can do that in R using character strings for different colors. Figure 3.3 gives an example of some of the different blues available in R. To find links to listings of different R colors, google “R colors” and search by “Images”. Figure 3.3: Example of available shades of blue in R. 3.2.5 Useful plot additions There are also a number of elements that you can add onto a ggplot object using +. A few that are used very frequently are: Element Description ggtitle Plot title xlab, ylab x- and y-axis labels xlim, ylim Limits of x- and y-axis 3.2.6 Example dataset For the example plots, I’ll use a dataset in the faraway package called nepali. This gives data from a study of the health of a group of Nepalese children. library(faraway) data(nepali) I’ll be using functions from dplyr and ggplot2, so those need to be loaded: library(dplyr) library(ggplot2) Each observation is a single measurement for a child; there can be multiple observations per child. I used the following code to select only the columns for child id, sex, weight, height, and age. I also used distinct to limit the dataset to only include one measurement for each chile, the child’s first measurement in the dataset. nepali &lt;- nepali %&gt;% select(id, sex, wt, ht, age) %&gt;% mutate(id = factor(id), sex = factor(sex, levels = c(1, 2), labels = c(&quot;Male&quot;, &quot;Female&quot;))) %&gt;% distinct(id, .keep_all = TRUE) After this cleaning, the data looks like this: head(nepali) ## id sex wt ht age ## 1 120011 Male 12.8 91.2 41 ## 2 120012 Female 14.9 103.9 57 ## 3 120021 Female 7.7 70.1 8 ## 4 120022 Female 12.1 86.4 35 ## 5 120023 Male 14.2 99.4 49 ## 6 120031 Male 13.9 96.4 46 3.2.7 Histograms Histograms show the distribution of a single variable. Therefore, geom_histogram() requires only one main aesthetic, x, the (numeric) vector for which you want to create a histogram. For example, to create a histogram of children’s heights for the Nepali dataset (Figure 3.4), run: ggplot(nepali, aes(x = ht)) + geom_histogram() Figure 3.4: Basic example of plotting a histogram with ggplot2. This histogram shows the distribution of heights for the first recorded measurements of each child in the nepali dataset. If you run the code with no arguments for binwidth or bins in geom_histogram, you will get a message saying “stat_bin() using bins = 30. Pick better value with binwidth.”. This message is just saying that a default number of bins was used to create the histogram. You can use arguments to change the number of bins used, but often this default is fine. You may also get a message that observations with missing values were removed. You can add some elements to the histogram now to customize it a bit. For example (Figure @ref()), you can add a figure title (ggtitle) and clearer labels for the x-axis (xlab). You can also change the range of values shown by the x-axis (xlim). ggplot(nepali, aes(x = ht)) + geom_histogram(fill = &quot;lightblue&quot;, color = &quot;black&quot;) + ggtitle(&quot;Height of children&quot;) + xlab(&quot;Height (cm)&quot;) + xlim(c(0, 120)) Figure 3.5: Example of adding ggplot elements to customize a histogram. The geom geom_histogram also has special argument for setting the number of width of the bins used in the histogram. Figure ?? shows an example of how you can use the bins argument to change the number of bins that are used to make the histogram of height for the nepali dataset. ggplot(nepali, aes(x = ht)) + geom_histogram(fill = &quot;lightblue&quot;, color = &quot;black&quot;, bins = 40) Figure 3.6: Example of using the bins argument to change the number of bins used in a histogram. Similarly, the binwidth argument can be used to set the width of bins. Figure 3.7 shows an example of using this function to create a histogram of the Nepali children’s heights with binwidths of 10 centimeters (note that this argument is set in the same units as the x variable). ggplot(nepali, aes(x = ht)) + geom_histogram(fill = &quot;lightblue&quot;, color = &quot;black&quot;, binwidth = 10) Figure 3.7: Example of using the binwidth argument to set the width of each bin used in a histogram. 3.2.8 Scatterplots A scatterplot shows how one variable changes as another changes. You can use the geom_point geom to create a scatterplot. For example, to create a scatterplot of height versus age for the Nepali data (Figure 3.8), you can run the following code: ggplot(nepali, aes(x = ht, y = wt)) + geom_point() Figure 3.8: Example of creating a scatterplot. This scatterplot shows the relationship between children’s heights and weights within the nepali dataset. Again, you can use some of the options and additions to change the plot appearance. For example, to add a title, change the x- and y-axis labels, and change the color and size of the points on the scatterplot (Figure 3.9), you can run: ggplot(nepali, aes(x = ht, y = wt)) + geom_point(color = &quot;blue&quot;, size = 0.5) + ggtitle(&quot;Weight versus Height&quot;) + xlab(&quot;Height (cm)&quot;) + ylab(&quot;Weight (kg)&quot;) Figure 3.9: Example of adding ggplot elements to customize a scatterplot. You can also try mapping another variable in the dataset to the color aesthetic. For example, to use color to show the sex of each child in the scatterplot (Figure 3.10), you can run: ggplot(nepali, aes(x = ht, y = wt, color = sex)) + geom_point(size = 0.5) + ggtitle(&quot;Weight versus Height&quot;) + xlab(&quot;Height (cm)&quot;) + ylab(&quot;Weight (kg)&quot;) Figure 3.10: Example of mapping color to an element of the data in a scatterplot. 3.2.9 Boxplots Boxplots can be used to show the distribution of a continuous variable. To create a boxplot, you can use the geom_boxplot geom. To plot a boxplot for a single, continuous variable, you can map that variable to y in the aes call, and map x to the constant 1. For example, to create a boxplot of the heights of children in the Nepali dataset (Figure 3.11), you can run: ggplot(nepali, aes(x = 1, y = ht)) + geom_boxplot() + xlab(&quot;&quot;)+ ylab(&quot;Height (cm)&quot;) Figure 3.11: Example of creating a boxplot. The example shows the distribution of height data for children in the nepali dataset. You can also create separate boxplots, one for each level of a factor (Figure 3.12). In this case, you’ll need to include two aesthetics (x and y) when you initialize the ggplot object The y variable is the variable for which the distribution will be shown, and the x variable should be a discrete (categorical or TRUE/FALSE) variable, and will be used to group the variable. This x variable should also be specified as the grouping variable, using group within the aesthetic call. ggplot(nepali, aes(x = sex, y = ht, group = sex)) + geom_boxplot() + xlab(&quot;Sex&quot;)+ ylab(&quot;Height (cm)&quot;) Figure 3.12: Example of creating separate boxplots, divided by a categorical grouping variable in the data. 3.2.10 Extensions of ggplot2 There are lots of R extensions for creating other interesting plots. For example, you can use the ggpairs function from the GGally package to plot all pairs of scatterplots for several variables (Figure ??). library(GGally) ggpairs(nepali %&gt;% select(sex, wt, ht, age)) Notice how this output shows continuous and binary variables differently. For example, the center diagonal shows density plots for continuous variables, but a bar chart for the categorical variable. See https://www.ggplot2-exts.org to find more ggplot2 extensions. 3.3 Simple statistics functions 3.3.1 Summary statistics To explore your data, you’ll need to be able to calculate some simple statistics for vectors, including calculating the mean and range of continuous variables and counting the number of values in each category of a factor or logical vector. Here are some simple statistics functions you will likely use often: Function Description range() Range (minimum and maximum) of vector min(), max() Minimum or maximum of vector mean(), median() Mean or median of vector sd() Standard deviation of vector table() Number of observations per level for a factor vector cor() Determine correlation(s) between two or more vectors summary() Summary statistics, depends on class All of these functions take, as the main argument, the vector or vectors for which you want the statistic. If there are missing values in the vector, you’ll typically need to add an argument to say what to do with the missing values. The parameter name for this varies by function, but for many of these functions it’s na.rm = TRUE or use=&quot;complete.obs&quot;. mean(nepali$wt, na.rm = TRUE) ## [1] 10.18432 range(nepali$ht, na.rm = TRUE) ## [1] 52.4 104.1 sd(nepali$ht, na.rm = TRUE) ## [1] 12.64529 table(nepali$sex) ## ## Male Female ## 107 93 Most of these functions take a single vector as the input. The cor function, however, calculates the correlation between vectors and so takes two or more vectors. If you give it multiple values, it will give the correlation matrix for all the vectors. cor(nepali$wt, nepali$ht, use = &quot;complete.obs&quot;) ## [1] 0.9571535 cor((nepali %&gt;% select(wt, ht, age)), use = &quot;complete.obs&quot;) ## wt ht age ## wt 1.0000000 0.9571535 0.8931195 ## ht 0.9571535 1.0000000 0.9287129 ## age 0.8931195 0.9287129 1.0000000 R supports object-oriented programming. Your first taste of this shows up with the summary function. For the summary function, R does not run the same code every time. Instead, R first checks what type of object was input to summary, and then it runs a function (method) specific to that type of object. For example, if you input a continuous vector, like the ht column in nepali, to summary, the function will return the mean, median, range, and 25th and 75th percentile values: summary(nepali$wt) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 3.80 7.90 10.10 10.18 12.40 16.70 15 However, if you submit a factor vector, like the sex column in nepali, the summary function will return a count of how many elements of the vector are in each factor level (as a note, you could do the same thing with the table function): summary(nepali$sex) ## Male Female ## 107 93 The summary function can also input other data structures, including dataframes, lists, and special object types, like regression model objects. In each case, it performs different actions specific to the object type. Later in this section, we’ll cover regression models, and see what the summary function returns when it is used with regression model objects. 3.3.2 summarize function You will often want to use these functions in conjunction with the summarize function in dplyr. For example, to create a new dataframe with the mean weight of children in the nepali dataset, you can use mean inside a summarize function: nepali %&gt;% summarize(mean_wt = mean(wt, na.rm = TRUE)) ## mean_wt ## 1 10.18432 There are also some special functions that you can use with summarize. For example, the n function will calculate the number of observations and the first function will return the first value of a column: nepali %&gt;% summarize(n_children =n(), first_id = first(id)) ## n_children first_id ## 1 200 120011 See the “summary function” section of the the RStudio Data Wrangling cheatsheet for more examples of these special functions. Often, you will be more interested in summaries within certain groupings of your data, rather than overall summaries. For example, you may be interested in mean height and weight by sex, rather than across all children, for the nepali data. It is very easy to calculate these grouped summaries using dplyr– you just need to group data using the group_by function (also a dplyr function) before you run the summarize function: nepali %&gt;% group_by(sex) %&gt;% summarize(mean_wt = mean(wt, na.rm = TRUE), n_children =n(), first_id = first(id)) ## # A tibble: 2 × 4 ## sex mean_wt n_children first_id ## &lt;fctr&gt; &lt;dbl&gt; &lt;int&gt; &lt;fctr&gt; ## 1 Male 10.497980 107 120011 ## 2 Female 9.823256 93 120012 Don’t forget that you need to save the output to a new object if you want to use it later. The above code, which creates a dataframe with summaries for Nepali children by sex, will only be printed out to your console if run as-is. If you’d like to save this output as an object to use later (for example, for a plot or table), you need to assign it to an R object. 3.4 Logical vectors Last week, you learned a lot about logical statements and how to use them with the filter function. You can also use logical vectors, created with these logical statements, for a lot of other things. For example, you can use them directly in the square bracket indexing ([..., ...]) to pull out just the rows of a dataframe that meet a certain condition. When you run a logical statement on a vector, you create a logical vector the same length as the original vector: is_male &lt;- nepali$sex == &quot;Male&quot; length(nepali$sex) ## [1] 200 length(is_male) ## [1] 200 The logical vector (is_male in this example) will have the value TRUE at any position where the original vector (nepali$sex in this example) met the logical condition you tested, and FALSE anywhere else: head(nepali$sex) ## [1] Male Female Female Female Male Male ## Levels: Male Female head(is_male) ## [1] TRUE FALSE FALSE FALSE TRUE TRUE You can “flip” this logical vector (i.e., change every TRUE to FALSE and vice-versa) using the bang operator, !: head(is_male) ## [1] TRUE FALSE FALSE FALSE TRUE TRUE head(!is_male) ## [1] FALSE TRUE TRUE TRUE FALSE FALSE The bang operator turns out to be very useful. You will often find cases where it’s difficult to write a logical vector to get what you want, but fairly easy to write the inverse (find everything you don’t want). One example is filtering down to non-missing values– the is.na function will return TRUE for any value that is NA, so you can use !is.na() to identify any non-missing values. You can do a few cool things with a logical vector. For example, you can use it with indexing to pull out just the rows of a dataframe where is_male is TRUE: head(nepali[is_male, ]) ## id sex wt ht age ## 1 120011 Male 12.8 91.2 41 ## 5 120023 Male 14.2 99.4 49 ## 6 120031 Male 13.9 96.4 46 ## 7 120051 Male 8.3 69.5 8 ## 9 120053 Male 15.8 96.0 54 ## 11 120062 Male 12.1 89.9 57 Or, with !, just the rows where is_male is FALSE: head(nepali[!is_male, ]) ## id sex wt ht age ## 2 120012 Female 14.9 103.9 57 ## 3 120021 Female 7.7 70.1 8 ## 4 120022 Female 12.1 86.4 35 ## 8 120052 Female 11.8 83.6 32 ## 10 120061 Female 8.7 78.5 26 ## 15 120082 Female 11.2 79.8 36 For these cases, the length of the logical vector and the number of rows in the dataframe will match. You can also use sum() and table() with a logical vector to find out how many of the values in the vector are TRUE AND FALSE. In the example, you can use these functions to find out how many males and females are in the dataset: sum(is_male) ## [1] 107 sum(!is_male) ## [1] 93 table(is_male) ## is_male ## FALSE TRUE ## 93 107 Note that you could also achieve the same thing with dplyr functions. For example, you could use mutate with a logical statement to create an is_male column in the nepali dataframe, then group by the new is_male column and summarize, using the n function to count the number of observations in each group: nepali %&gt;% mutate(is_male = sex == &quot;Male&quot;) %&gt;% group_by(is_male) %&gt;% summarize(n_children = n()) ## # A tibble: 2 × 2 ## is_male n_children ## &lt;lgl&gt; &lt;int&gt; ## 1 FALSE 93 ## 2 TRUE 107 3.5 Regression models 3.5.1 Formula structure Regression models can be used to estimate how the expected value of a dependent variable changes as independent variables change. In R, regression formulas take this structure: ## Generic code [response variable] ~ [indep. var. 1] + [indep. var. 2] + ... Notice that a tilde, ~, is used to separate the independent and dependent variables and that a plus sign, +, is used to join independent variables. This format mimics the statistical notation: \\[ Y_i \\sim X_1 + X_2 + X_3 \\] You will use this type of structure in R fo a lot of different function calls, including those for linear models (fit with the lm function) and generalized linear models (fit with the glm function). There are some conventions that can be used in R formulas. Common ones include: Convention Meaning I() evaluate the formula inside I() before fitting (e.g., I(x1 + x2)) : fit the interaction between x1 and x2 variables * fit the main effects and interaction for both variables (e.g., x1*x2 equals x1 + x2 + x1:x2) . include as independent variables all variables other than the response (e.g., y ~ .) 1 intercept (e.g., y ~ 1 for an intercept-only model) - do not include a variable in the dataframe as an independent variables (e.g., y ~ . - x1); usually used in conjunction with . or 1 3.5.2 Linear models To fit a linear model, you can use the function lm(). This function is part of the stats package, which comes installed with base R. In this function, you can use the data option to specify the dataframe from which to get the vectors. mod_a &lt;- lm(wt ~ ht, data = nepali) This previous call fits the model: \\[ Y_{i} = \\beta_{0} + \\beta_{1}X_{1,i} + \\epsilon_{i} \\] where: \\(Y_{i}\\) : weight of child \\(i\\) \\(X_{1,i}\\) : height of child \\(i\\) If you run the lm function without saving it as an object, R will fit the regression and print out the function call and the estimated model coefficients: lm(wt ~ ht, data = nepali) ## ## Call: ## lm(formula = wt ~ ht, data = nepali) ## ## Coefficients: ## (Intercept) ht ## -8.6948 0.2351 However, to be able to use the model later for things like predictions and model assessments, you should save the output of the function as an R object: mod_a &lt;- lm(wt ~ ht, data = nepali) This object has a special class, lm: class(mod_a) ## [1] &quot;lm&quot; This class is a special type of list object. If you use is.list to check, you can confirm that this object is a list: is.list(mod_a) ## [1] TRUE There are a number of functions that you can apply to an lm object. These include: Function Description summary Get a variety of information on the model, including coefficients and p-values for the coefficients coefficients Pull out just the coefficients for a model fitted Get the fitted values from the model (for the data used to fit the model) plot Create plots to help assess model assumptions residuals Get the model residuals For example, you can get the coefficients from the model by running: coefficients(mod_a) ## (Intercept) ht ## -8.694768 0.235050 The estimated coefficient for the intercept is always given under the name “(Intercept)”. Estimated coefficients for independent variables are given based on their column names in the original data (“ht” here, for \\(\\beta_1\\), or the estimated increase in expected weight for a one unit increase in height). You can use the output from a coefficients call to plot a regression line based on the model fit on top of points showing the original data (Figure 3.13). mod_coef &lt;- coefficients(mod_a) ggplot(nepali, aes(x = ht, y = wt)) + geom_point(size = 0.2) + xlab(&quot;Height (cm)&quot;) + ylab(&quot;Weight (kg)&quot;) + geom_abline(aes(intercept = mod_coef[1], slope = mod_coef[2]), col = &quot;blue&quot;) Figure 3.13: Example of using the output from a coefficients call to add a regression line to a scatterplot. You can also add a linear regression line to a scatterplot by adding the geom geom_smooth using the argument method = “lm”. You can use the function residuals on an lm object to pull out the residuals from the model fit: head(residuals(mod_a)) ## 1 2 3 4 5 6 ## 0.05820415 -0.82693141 -0.08223993 0.48644436 -0.46920621 -0.06405608 The result of a residuals call is a vector with one element for each of the non-missing observations (rows) in the dataframe you used to fit the model. Each value gives the different between the model fitted value and the observed value for each of these observations, in the same order the observations show up in the dataframe. The residuals are in the same order as the observations in the original dataframe. You can also use the shorter function coef as an alternative to coefficients and the shorter function resid as an alternative to residuals. As noted in the subsection on simple statistics functions, the summary function returns different output depending on the type of object that is input to the function. If you input a regression model object to summary, the function gives you a lot of information about the model. For example, here is the output returned by running summary for the linear regression model object we just created: summary(mod_a) ## ## Call: ## lm(formula = wt ~ ht, data = nepali) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.44736 -0.55708 0.01925 0.49941 2.73594 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -8.694768 0.427398 -20.34 &lt;2e-16 *** ## ht 0.235050 0.005257 44.71 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9017 on 183 degrees of freedom ## (15 observations deleted due to missingness) ## Multiple R-squared: 0.9161, Adjusted R-squared: 0.9157 ## F-statistic: 1999 on 1 and 183 DF, p-value: &lt; 2.2e-16 This output includes a lot of useful elements, including (1) basic summary statistics for the residuals (to meet model assumptions, the median should be around zero and the absolute values fairly similar for the first and third quantiles), (2) coefficient estimates, standard errors, and p-values, and (3) some model summary statistics, including residual standard error, degrees of freedom, number of missing observations, and F-statistic. The object returned by the summary() function when it is applied to an lm object is a list, which you can confirm using the is.list function: is.list(summary(mod_a)) ## [1] TRUE With any list, you can use the names function to get the names of all of the different elements of the object: names(summary(mod_a)) ## [1] &quot;call&quot; &quot;terms&quot; &quot;residuals&quot; &quot;coefficients&quot; ## [5] &quot;aliased&quot; &quot;sigma&quot; &quot;df&quot; &quot;r.squared&quot; ## [9] &quot;adj.r.squared&quot; &quot;fstatistic&quot; &quot;cov.unscaled&quot; &quot;na.action&quot; You can use the $ operator to pull out any element of the list. For example, to pull out the table with information on the estimated model coefficients, you can run: summary(mod_a)$coefficients ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -8.694768 0.427397843 -20.34350 7.424640e-49 ## ht 0.235050 0.005256822 44.71334 1.962647e-100 The plot function, like the summary function, will give different output depending on the class of the object that you input. For an lm object, you can use the plot function to get a number of useful diagnostic plots that will help you check regression assumptions (Figure 3.14): plot(mod_a) Figure 3.14: Example output from running the plot function with an lm object as the input. You can also use binary variables or factors as independent variables in regression models. For example, in the nepali dataset, sex is a factor variable with the levels “Male” and “Female”. You can fit a linear model of weight regressed on sex for this data with the call: mod_b &lt;- lm(wt ~ sex, data = nepali) This call fits the model: \\[ Y_{i} = \\beta_{0} + \\beta_{1}X_{1,i} + \\epsilon_{i} \\] where \\(X_{1,i}\\) : sex of child \\(i\\), where 0 = male and 1 = female. Here are the estimated coefficients from fitting this model: summary(mod_b)$coefficients ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 10.497980 0.3110957 33.745177 1.704550e-80 ## sexFemale -0.674724 0.4562792 -1.478752 1.409257e-01 You’ll notice that, in addition to an estimated intercept ((Intercept)), the other estimated coefficient is sexFemale rather than just sex, although the column name in the dataframe input to lm for this variable is sex. This is because, when a factor or binary variable is input as an independent variable in a linear regression model, R will fit an estimated coefficient for all levels of factors except the first factor level. By default, this first factor level is used as the baseline level, and so its estimated mean is given by the estimated intercept, while the other model coefficients give the estimated difference from this baseline. For example, the model fit above tells us that the estimated mean weight of males is 10.5, while the estimated mean weight of females is 10.5 + -0.7 = 9.8. If you would prefer that a different level of the factor be the baseline (for example, “Female” rather than “Male” for the previous regression), you can do that by using the levels argument in the factor function to reset factor levels. For example: nepali_reset &lt;- nepali %&gt;% mutate(sex = factor(sex, levels = c(&quot;Female&quot;, &quot;Male&quot;))) mod_b_reset &lt;- lm(wt ~ sex, data = nepali_reset) summary(mod_b_reset)$coef ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 9.823256 0.3337816 29.430189 2.626719e-71 ## sexMale 0.674724 0.4562792 1.478752 1.409257e-01 Now, (Intercept) gives the estimated mean weight for females, while the second estimated coefficient gives the estimated mean difference for males compared to the expected value for females. 3.5.3 Generalized linear models (GLMs) You can fit a variety of models, including linear models, logistic models, and Poisson models, using generalized linear models (GLMs). For linear models, the only difference between lm and glm are the mechanics of how they estimate the model coefficients (lm uses least squares while glm uses maximum likelihood). You will (almost always) get exactly the same estimated coefficients regardless of whether you use glm or lm to fit a linear regression. For example, here is the code to fit a linear regression model for weight regressed on height from the nepali dataset: mod_c &lt;- glm(wt ~ ht, data = nepali) This call fits the same regression model I fit earlier with the lm function and saved as mod_a. You can see that the two methods give exactly the same coefficient estimates: coef(mod_c) ## (Intercept) ht ## -8.694768 0.235050 coef(mod_a) ## (Intercept) ht ## -8.694768 0.235050 Unlike the lm function, however, the glm function also allows you to fit other model types, including logistic and Poisson models. You can specify the model type using the family argument to the glm call: Model type family argument Linear family = gaussian(link = 'identity') Logistic family = binomial(link = 'logit') Poisson family = poisson(link = 'log') For example, say we wanted to fit a logistic regression for the nepali data of whether the probability of a child weighing more than 13 kg is associated with the child’s sex. First, create a binary variable in the nepali dataset, wt_over_13, that is TRUE if a child weighed more than 13 kilograms and FALSE otherwise. You can use the mutate function from dplyr to add this new column (which, as a note, is a logical vector): nepali &lt;- nepali %&gt;% mutate(wt_over_13 = wt &gt; 13) head(nepali) ## id sex wt ht age wt_over_13 ## 1 120011 Male 12.8 91.2 41 FALSE ## 2 120012 Female 14.9 103.9 57 TRUE ## 3 120021 Female 7.7 70.1 8 FALSE ## 4 120022 Female 12.1 86.4 35 FALSE ## 5 120023 Male 14.2 99.4 49 TRUE ## 6 120031 Male 13.9 96.4 46 TRUE Now you can fit a logistic regression of wt_over_13 regressed on sex, using a logistic model: mod_d &lt;- glm(wt_over_13 ~ sex, data = nepali, family = binomial(link = &quot;logit&quot;)) Elements of a GLM can be pulled out in the same way that we looked at elements from the linear model fit with lm. For example, to see a table of estimated model coefficients, you can run: summary(mod_d)$coef ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.3121864 0.2458445 -5.337465 9.425485e-08 ## sexFemale -0.4133237 0.3886659 -1.063442 2.875815e-01 Because this model was a logistic model, fit with a log link, here the model coefficient estimate for sexFemale gives an estimate of the log odds of weight higher than 13 kg associated with females versus males. The p-value for this estimate (Pr(&gt;|z|) = 0.29) isn’t very small, suggesting that the difference between male and female children in the odds of weighing more than 13 kg is not statistically significant. 3.5.4 References– statistics in R One great (and free online for CSU students through our library) book to find out more about using R for basic statistics is: Introductory Statistics with R If you want all the details about fitting linear models and GLMs in R, Julian Faraway’s books are fantastic. He has one on linear models and one on extensions including logistic and Poisson models: Linear Models with R (also free online through the CSU library) Extending the Linear Model with R 3.6 In-course exercise 3.6.1 Loading data from an R package The data we’ll be using today is from a dataset called worldcup in the package faraway. Load that data so you can use it on your computer (note: you will need to load and install the faraway package to do this). Use the help file for the data to find out more about the dataset. Use some basic functions, like head, tail, colnames, str, and summary to check out the data a bit. See if you can figure out: What variables are included in this dataset? (Check the column names.) What class is each column currently? In particular, which are numbers and which are factors? 3.6.1.1 Example R code: Load the faraway package using load() and then load the data using data(): ## Uncomment the next line if you need to install the package # install.packages(&quot;faraway&quot;) library(faraway) data(&quot;worldcup&quot;) Check out the help file for the worldcup dataset to find out more about the data. (Note: Only datasets that are parts of packages will have help files.) ?worldcup Check out the data a bit: str(worldcup) ## &#39;data.frame&#39;: 595 obs. of 7 variables: ## $ Team : Factor w/ 32 levels &quot;Algeria&quot;,&quot;Argentina&quot;,..: 1 16 9 9 5 32 11 11 18 20 ... ## $ Position: Factor w/ 4 levels &quot;Defender&quot;,&quot;Forward&quot;,..: 4 4 1 4 2 2 1 2 4 1 ... ## $ Time : int 16 351 180 270 46 72 138 33 21 103 ... ## $ Shots : int 0 0 0 1 2 0 0 0 5 0 ... ## $ Passes : int 6 101 91 111 16 15 51 9 22 38 ... ## $ Tackles : int 0 14 6 5 0 0 2 0 0 1 ... ## $ Saves : int 0 0 0 0 0 0 0 0 0 0 ... head(worldcup) ## Team Position Time Shots Passes Tackles Saves ## Abdoun Algeria Midfielder 16 0 6 0 0 ## Abe Japan Midfielder 351 0 101 14 0 ## Abidal France Defender 180 0 91 6 0 ## Abou Diaby France Midfielder 270 1 111 5 0 ## Aboubakar Cameroon Forward 46 2 16 0 0 ## Abreu Uruguay Forward 72 0 15 0 0 tail(worldcup) ## Team Position Time Shots Passes Tackles Saves ## van Bommel Netherlands Midfielder 540 2 307 31 0 ## van Bronckhorst Netherlands Defender 540 1 271 10 0 ## van Persie Netherlands Forward 479 14 108 1 0 ## von Bergen Switzerland Defender 234 0 79 3 0 ## Alvaro Pereira Uruguay Midfielder 409 6 140 17 0 ## Ozil Germany Midfielder 497 7 266 3 0 colnames(worldcup) ## [1] &quot;Team&quot; &quot;Position&quot; &quot;Time&quot; &quot;Shots&quot; &quot;Passes&quot; &quot;Tackles&quot; ## [7] &quot;Saves&quot; summary(worldcup) ## Team Position Time Shots ## Slovakia : 21 Defender :188 Min. : 1.0 Min. : 0.000 ## Uruguay : 21 Forward :143 1st Qu.: 88.0 1st Qu.: 0.000 ## Argentina: 20 Goalkeeper: 36 Median :191.0 Median : 1.000 ## Cameroon : 20 Midfielder:228 Mean :208.9 Mean : 2.304 ## Chile : 20 3rd Qu.:270.0 3rd Qu.: 3.000 ## Paraguay : 20 Max. :570.0 Max. :27.000 ## (Other) :473 ## Passes Tackles Saves ## Min. : 0.00 Min. : 0.000 Min. : 0.0000 ## 1st Qu.: 29.00 1st Qu.: 1.000 1st Qu.: 0.0000 ## Median : 61.00 Median : 3.000 Median : 0.0000 ## Mean : 84.52 Mean : 4.192 Mean : 0.6672 ## 3rd Qu.:115.50 3rd Qu.: 6.000 3rd Qu.: 0.0000 ## Max. :563.00 Max. :34.000 Max. :20.0000 ## 3.6.2 Basic plots of the data Use some basic plots to check out this data. Try the following: Plot histograms of all the numeric variables (Time, Shot, Passes, Tackles, Saves) Plot scatterplots of different combinations of numeric variables (e.g., Time vs. Shots). Try doing this using the geom_point() geom from ggplot2. Also try doing it using the ggpairs() function from the GGally package, to plot several of these at the same time. Try using different constant or mapped values with the color aesthetic. Create boxplots of Time, Shots, Passes and Saves by position. Go online and find out which teams were the top four teams in this World Cup (i.e., first through fourth places). Create a top_teams subset with just these teams. Plot boxplots of Shots and Saves by team for just these teams. Did you notice any interesting features of the data when you did any of the graphs in this section? 3.6.2.1 Example R code: Use histograms to explore the distribution of different variables. If you want to change the number of bins in the histogram, try playing around with the bins and binwidth arguments. You can use the bins argument to say how many bins you want (e.g., bins = 50). You can use the binwidth argument to say how wide you want the bins to be (e.g., binwidth = 10 if you wanted bins to be 10 units wide, in the units of the variable mapped to the x aesthetic. Try using fill and color to change the appearance of the plot. Google “R colors” and search the images to find links to listings of different R colors. library(ggplot2) ggplot(worldcup, aes(x = Time)) + geom_histogram() ggplot(worldcup, aes(x = Time)) + geom_histogram(bins = 50) ggplot(worldcup, aes(x = Time)) + geom_histogram(binwidth = 100) ggplot(worldcup, aes(x = Time)) + geom_histogram(binwidth = 50, color = &quot;white&quot;, fill = &quot;cyan4&quot;) Create a scatterplot of Time versus Passes. To change the size of the points, use the size argument (use a number lower than 1 for smaller points, higher than 1 for larger points). Try changing the color and transparency of the points using the aesthetics color and alpha. Try using color to show each player’s position by mapping Position to the color aesthetic. ggplot(worldcup, aes(x = Time, y = Passes)) + geom_point() ggplot(worldcup, aes(x = Time, y = Passes)) + geom_point(size = 0.5) ggplot(worldcup, aes(x = Time, y = Passes)) + geom_point(size = 2, color = &quot;blue&quot;, alpha = 0.25) ggplot(worldcup, aes(x = Time, y = Passes, color = Position)) + geom_point() Use the ggpairs function from the GGally package to plot scatterplots of all combinations of several numeric variables. library(GGally) library(dplyr) ggpairs(select(worldcup, Time, Shots, Passes, Tackles, Saves)) To create a boxplot of Shots by Position, you can use geom_boxplot: ggplot(worldcup, aes(x = Position, y = Shots)) + geom_boxplot() The top four teams in this World Cup were Spain, the Netherlands, Germany, and Uruguay. Create a subset with just the data for these four teams: top_teams &lt;- worldcup %&gt;% filter(Team %in% c(&quot;Spain&quot;, &quot;Netherlands&quot;, &quot;Germany&quot;, &quot;Uruguay&quot;)) %&gt;% mutate(Team = factor(Team)) This dataset will still have all the levels saved for the Team factor, even though it isn’t using them all. You can re-set this by resetting Team as a factor, which is what I’ve done with the mutate line. When R creates a factor from a vector, its default is to only use as levels the values that show up in the vector. Now, you can plot the boxplots, mapping Team to the x aesthetic and Shots or Saves to the y aesthetic: ggplot(top_teams, aes(x = Team, y = Shots)) + geom_boxplot() + ggtitle(&quot;Shots&quot;) ggplot(top_teams, aes(x = Team, y = Saves)) + geom_boxplot() + ggtitle(&quot;Saves&quot;) 3.6.2.2 If you have extra time: If you wanted to do the same plot for several different variables, you could loop through your code (we’ll be covering more about loops in a few weeks). For example, you could create histograms for all of the numeric variables (if you do this in RStudio, you’ll need to use the arrows on the plot window to move through and see all the different plots once you’ve created them): ## Create an object with the column names for all of the numeric variables my_vars &lt;- colnames(worldcup)[3:7] ## Loop through all of those variables. Print out a histogram with the ## variable, and have it print on the plot, as the main title, the ## column name for that variable for(var in my_vars){ worldcup$to_plot &lt;- worldcup[ , var] a &lt;- ggplot(worldcup, aes(x = to_plot)) + geom_histogram(bins = 20, color = &quot;white&quot;, fill = &quot;navy&quot;) + xlab(var) + ggtitle(paste(&quot;Histogram of&quot;, var)) plot(a) } A few things to note in this example: To map an element of the data to an aesthetic, it’s easiest if that element is saved in a column in the dataframe. Within this loop, I’m making an extra column called to_plot, where I’m copying the column of the variable I want to plot each time the loop runs. That way, I can always use x = to_plot in the aesthetic mapping for the ggplot object. If you run code to create a ggplot object within a loop, it won’t automatically print. Instead, you need to use print to get the object to print out. One way to do that is to save the final ggplot object as an R object (here I’m saving it to a) and then use the print function to print that object. Next week, we’ll talk some about faceting, which can create multiple plots by variable like this in a lot less code. However, it’s useful at this point to start thinking about how to extend code to use in loops, to save yourself time when you need to repeat something similar many times. 3.6.3 Exploring the data using simple statistics and logical statements Next, try checking out the data using some basic commands for simple statistics, like mean(), range(), max(), and min(). Use these, along with some logical statements, to help you answer the following questions: What is the range of time that players spent in the game? Who played the most World Cup time in this World Cup? For the minimum of the range of Time, how many players played this amount of time? What is the mean number of saves that players made? What is the mean number of saves just among the goalkeepers? How many of the players are goalkeepers? Did any non-goalkeeper make a save? 3.6.3.1 Example R code: Use range() to find out the range of time these players played in the World Cup. range(worldcup$Time) ## [1] 1 570 To figure out who played the most time, you need to subset out the rows of the dataset where the Time variable equals the maximum of the Time variable for the whole dataset. There are a few ways to do that. Here I’m showing two: (1) using logic within the “square-bracket indexing”, to pull out just rows where it is TRUE that the Time for that row equals max(worldcup$Time) and (2) using filter from the dplyr package to filter down to rows where where it is TRUE that the Time for that row equals max(Time) for the whole dataset. max(worldcup$Time) ## [1] 570 head(worldcup$Time == max(worldcup$Time)) ## [1] FALSE FALSE FALSE FALSE FALSE FALSE worldcup[worldcup$Time == max(worldcup$Time), ] ## Team Position Time Shots Passes Tackles Saves to_plot ## Arevalo Rios Uruguay Midfielder 570 5 195 21 0 0 ## Maxi Pereira Uruguay Midfielder 570 5 182 15 0 0 ## Muslera Uruguay Goalkeeper 570 0 75 0 16 16 worldcup %&gt;% filter(Time == max(Time)) ## Team Position Time Shots Passes Tackles Saves to_plot ## 1 Uruguay Midfielder 570 5 195 21 0 0 ## 2 Uruguay Midfielder 570 5 182 15 0 0 ## 3 Uruguay Goalkeeper 570 0 75 0 16 16 Note: You may have noticed that you lost the players names when you did this using the dplyr pipechain. That’s because dplyr functions convert the data to a dataframe format that does not include rownames. If you want to keep players’ names, use mutate to move those names from the rownames of the data into a column in the dataframe: worldcup %&gt;% mutate(Name = rownames(worldcup)) %&gt;% filter(Time == max(Time)) ## Team Position Time Shots Passes Tackles Saves to_plot Name ## 1 Uruguay Midfielder 570 5 195 21 0 0 Arevalo Rios ## 2 Uruguay Midfielder 570 5 182 15 0 0 Maxi Pereira ## 3 Uruguay Goalkeeper 570 0 75 0 16 16 Muslera To calculate the mean number of saves among all the players, use the mean function, either by itself or within a summarize call: mean(worldcup$Saves) ## [1] 0.6672269 worldcup %&gt;% summarize(mean_saves = mean(Saves)) ## mean_saves ## 1 0.6672269 For the next parts of the question, it will be convenient to have a logical vector for whether each player is a goalkeeper, so here’s how you would create that: goalie &lt;- worldcup$Position == &quot;Goalkeeper&quot; This new object, goalie, is a vector the same length as worldcup$Position. Each element of goalie says whether it is TRUE or FALSE that worldcup$Position is equal to “Goalkeeper” at that spot on the worldcup$Position vector. head(goalie) ## [1] FALSE FALSE FALSE FALSE FALSE FALSE The summary() function will count up the total number of times that goalie is TRUE and FALSE. summary(goalie) ## Mode FALSE TRUE NA&#39;s ## logical 559 36 0 There are a few ways to use this vector to figure out how many players were goalkeepers. First, you could use summary (which I just showed) or table, and just read how many times this vector has the value TRUE. Second, since R saves logical vectors with TRUE as 1 and FALSE as 0, you could just the sum function to add up the vector to find out how often it’s TRUE (sum adds up every value in the vector). table(goalie) ## goalie ## FALSE TRUE ## 559 36 sum(goalie) ## [1] 36 You could also answer this question by using summarize from dplyr. You need to group_by player position and then you can use the n function in summarize to count up the total number of observations in each group: worldcup %&gt;% group_by(Position) %&gt;% summarize(n_players = n()) ## # A tibble: 4 × 2 ## Position n_players ## &lt;fctr&gt; &lt;int&gt; ## 1 Defender 188 ## 2 Forward 143 ## 3 Goalkeeper 36 ## 4 Midfielder 228 Now, you can answer the questions about mean saves for goalies and max saves for non-goalies. First, try doing that using the goalie logical vector you created. If you put goalie in the square bracket indexing for the dataframe as the rows value (i.e., the index before the comma), R will subset out just the rows where goalie is equal to TRUE. If you put !goalie in the square bracket indexing as the rows value, R will just subset out the rows where goalie is equal to FALSE. You can use this index subsetting to figure out the mean number of saves per goalie and also whether any non-goalie made a save (by checking the maximum value or range of saves for non-goalies). head(worldcup[goalie, ]) ## Team Position Time Shots Passes Tackles Saves to_plot ## Barry Ivory Coast Goalkeeper 270 0 23 0 8 8 ## Benaglio Switzerland Goalkeeper 270 0 75 0 11 11 ## Bravo Chile Goalkeeper 360 0 58 0 4 4 ## Buffon Italy Goalkeeper 45 0 4 0 0 0 ## Casillas Spain Goalkeeper 540 0 67 0 11 11 ## Chaouchi Algeria Goalkeeper 90 0 17 0 2 2 mean(worldcup[goalie, &quot;Saves&quot;]) ## [1] 11.02778 range(worldcup[!goalie, &quot;Saves&quot;]) ## [1] 0 0 You could also answer this quesiton using a dplyr pipe chain to summarize the data after grouping it by position: worldcup %&gt;% group_by(Position) %&gt;% summarize(number_players = n(), mean_saves = mean(Saves), max_saves = max(Saves)) ## # A tibble: 4 × 4 ## Position number_players mean_saves max_saves ## &lt;fctr&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Defender 188 0.00000 0 ## 2 Forward 143 0.00000 0 ## 3 Goalkeeper 36 11.02778 20 ## 4 Midfielder 228 0.00000 0 3.6.4 Using regression models to explore data For this part of the exercise, you’ll use a dataset on weather, air pollution, and mortality counts in Chicago, IL. This dataset is called chicagoNMMAPS and is part of the dlnm package. Change the name of the dataframe to something shorter, like chic. Check out the data a bit to see what variables you have, and then perform the following tasks: Write out (on paper, not in R) the regression equation for regressing dewpoint temperature on temperature. Try fitting a linear regression of dew point temperature (dptp) on temperature (temp). (Bonus points: Notice anything that seems unusual about these two variables in this dataset? You can find out with summary, but it helps if you know a bit about what dewpoint temperature measures.) Save this model as the object mod_1. Based on this regression, does there seem to be a relationship between temperature and dewpoint temperature in Chicago? (Hint: Try using summary() on the model object to get more information about the model you fit.) What is the p-value for the coefficient for temperature? Plot temperature (x-axis) versus dewpoint temperature (y-axis) for Chicago. Add in the regression line from the model you fit. Use plot() on the model object to check if some of the assumptions for the regression model seem appropriate. Try fitting the regression as a GLM, using glm(). Are your coefficients different? Does \\(PM_{10}\\) vary by day of the week? (Hint: The dow variable is a factor that gives day of the week. You can do an ANOVA analysis by fitting a linear model using this variable as the independent variable, and then run anova() on that model, and R will compare it to an intercept-only model.) What day of the week is PM10 generally highest? (Check the model coefficients to figure this out.) Try to write out (on paper) the regression equation for the model you’re fitting. Try using glm() to run a Poisson regression of respiratory deaths (resp) on temperature during summer days. Start by creating a subset with just summer days called summer. (Hint: Use the month variable to do this– just pull out the subset where the month is 6, 7, or 8, for June, July, and August.) Try to write out the regression equation for the model you’re fitting. The coefficient for the temperature variable in this model is our best estimate (based on this model) of the log relative risk for a one degree Celcius increase in temperature. What is the relative risk associated with a one degree Celsius increase? 3.6.4.1 Example R code: Install and load the dlnm package and then load the chicagoNMMAPS data. Change the name of the dataframe to chic, so it will be shorter to call for the rest of your work. # install.packages(&quot;dlnm&quot;) library(dlnm) data(&quot;chicagoNMMAPS&quot;) chic &lt;- chicagoNMMAPS Fit a linear regression of dptp on temp and save as the object mod_1: mod_1 &lt;- lm(dptp ~ temp, data = chic) mod_1 ## ## Call: ## lm(formula = dptp ~ temp, data = chic) ## ## Coefficients: ## (Intercept) temp ## 24.025 1.621 Use summary() to check out a bit more about the model you fit. summary(mod_1) ## ## Call: ## lm(formula = dptp ~ temp, data = chic) ## ## Residuals: ## Min 1Q Median 3Q Max ## -24.3093 -3.7470 0.4687 4.0738 18.6518 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 24.024869 0.112933 212.7 &lt;2e-16 *** ## temp 1.620650 0.007631 212.4 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 5.899 on 5112 degrees of freedom ## Multiple R-squared: 0.8982, Adjusted R-squared: 0.8982 ## F-statistic: 4.511e+04 on 1 and 5112 DF, p-value: &lt; 2.2e-16 There does seem to be an association between temperature and dewpoint temperature: a unit increase in temperature is associated with a 1.6 unit increase in dewpoint temperature. The p-value for the temperature coefficient is &lt;2e-16. This is far below 0.05, which suggests we would be very unlikely to see such a strong association by chance if the null hypothesis, that the two variables are not associated, were true. Plot these two variables and add in the regression line from the model (note: I’ve used the color option to make the color of the points gray). Use the values from coef with a geom_abline to add the regression line for the model you fit. mod_coefs &lt;- coef(mod_1) ggplot(chic, aes(x = temp, y = dptp)) + geom_point(size = 0.5, col = &quot;gray&quot;) + geom_abline(aes(intercept = mod_coefs[1], slope = mod_coefs[2])) Plot some plots to check model assumptions for the model you fit using the plot() function on your model object: par(mfrow = c(2, 2)) # Set to four plots per panel -- 2 rows, 2 columns plot(mod_1) par(mfrow = c(1, 1)) # Reset to one plot per panel Try fitting the model using glm(). Call it mod_1a. Compare the coefficients for the two models. You can use the coef() function on an lm or glm object to pull out just the model coefficients. mod_1a &lt;- glm(dptp ~ temp, data = chic) coef(mod_1) ## (Intercept) temp ## 24.02487 1.62065 coef(mod_1a) ## (Intercept) temp ## 24.02487 1.62065 The results from the two models are identical. Fit a model of \\(PM_{10}\\) regressed on day of week, where day of week is a factor. mod_2 &lt;- lm(pm10 ~ dow, data = chic) summary(mod_2) ## ## Call: ## lm(formula = pm10 ~ dow, data = chic) ## ## Residuals: ## Min 1Q Median 3Q Max ## -39.05 -12.55 -3.34 8.80 328.66 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 27.5217 0.7303 37.684 &lt; 2e-16 *** ## dowMonday 6.1322 1.0340 5.931 3.22e-09 *** ## dowTuesday 6.7954 1.0269 6.617 4.05e-11 *** ## dowWednesday 8.4768 1.0262 8.261 &lt; 2e-16 *** ## dowThursday 8.8047 1.0240 8.598 &lt; 2e-16 *** ## dowFriday 9.4816 1.0262 9.240 &lt; 2e-16 *** ## dowSaturday 3.6602 1.0269 3.564 0.000368 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 19.07 on 4856 degrees of freedom ## (251 observations deleted due to missingness) ## Multiple R-squared: 0.02588, Adjusted R-squared: 0.02467 ## F-statistic: 21.5 on 6 and 4856 DF, p-value: &lt; 2.2e-16 Use the anova() command to compare this model to a model with only an intercept (i.e., one that only fits a global mean and uses that as the expected value for all of the observations). anova(mod_2) ## Analysis of Variance Table ## ## Response: pm10 ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## dow 6 46924 7820.6 21.5 &lt; 2.2e-16 *** ## Residuals 4856 1766407 363.8 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The p-value for an ANOVA of the model with day-of-week coefficients versus the model that just has an intercept is &lt; 2.2e-16. This is well below 0.05, which suggests that day-of-week is associated with PM10 concentration, as a model that includes day-of-week does a much better job of explaining variation in PM10 than a model without it does. (Note, too, that the F value and Pr(&gt;F) for the anova() call are identical to the F-statistic information given in the summary() of the model object. This will always be true when you’re using anova() to compare a model to a model with just an intercept.) Use a boxplot to visually compare PM10 by day of week. ggplot(chic, aes(x = dow, y = pm10)) + geom_boxplot() Now try the same plot, but try using the ylim = option to change the limits on the y-axis for the graph, so you can get a better idea of the pattern by day of week (some of the extreme values are very high, which makes it hard to compare by eye when the y-axis extends to include them all). ggplot(chic, aes(x = dow, y = pm10)) + geom_boxplot() + ylim(c(0, 100)) ## Warning: Removed 292 rows containing non-finite values (stat_boxplot). Create a subset called summer with just the summer days: summer &lt;- chic %&gt;% filter(month %in% 6:8) Use glm() to fit a Poisson model of respiratory deaths regressed on temperature. Since you want to fit a Poisson model, use the option family = poisson(link = &quot;log&quot;). mod_3 &lt;- glm(resp ~ temp, data = summer, family = poisson(link = &quot;log&quot;)) summary(mod_3) ## ## Call: ## glm(formula = resp ~ temp, family = poisson(link = &quot;log&quot;), data = summer) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -3.9755 -0.7162 -0.1807 0.6927 3.6555 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 1.910317 0.058373 32.726 &lt;2e-16 *** ## temp 0.006137 0.002581 2.378 0.0174 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 1499.4 on 1287 degrees of freedom ## Residual deviance: 1493.8 on 1286 degrees of freedom ## AIC: 6425.4 ## ## Number of Fisher Scoring iterations: 4 Use the fitted model coefficient to determine the relative risk for a one degree Celcius increase in temperature. First, remember that you can use the coef() function to read out the model coefficients. The second of these is the value for the temperature coefficient. That means that you can use indexing ([2]) to get just that value. That’s the log relative risk; take the exponent to get the relative risk. coef(mod_3) ## (Intercept) temp ## 1.910316958 0.006136743 coef(mod_3)[2] ## temp ## 0.006136743 exp(coef(mod_3)[2]) ## temp ## 1.006156 "],
["reporting-data-results-1.html", "Chapter 4 Reporting data results #1 4.1 Guidelines for good plots 4.2 High data density 4.3 Meaningful labels 4.4 References 4.5 Highlighting 4.6 Order 4.7 Small multiples 4.8 Advanced customization 4.9 To find out more 4.10 In-course exercise", " Chapter 4 Reporting data results #1 Download a pdf of the lecture slides covering this topic. 4.1 Guidelines for good plots There are a number of very thoughtful books and articles about creating graphics that effectively communicate information. Some of the authors I highly recommend (and from whose work I’ve pulled the guidelines for good graphics we’ll talk about this week) are: Edward Tufte Howard Wainer Stephen Few Nathan Yau You should plan, in particular, to read The Visual Display of Quantitative Information by Edward Tufte before you graduate. This week, we’ll focus on six guidelines for good graphics, based on the writings of these and other specialists in data display. The guidelines are: Aim for high data density. Use clear, meaningful labels. Provide useful references. Highlight interesting aspects of the data. Make order meaningful. When possible, use small multiples. For the examples, I’ll use dplyr for data cleaning and, for plotting, the packages ggplot2, gridExtra, and ggthemes. library(tidyverse) ## Loads `dplyr` and `ggplot2` library(gridExtra) library(ggthemes) You can load the data for today’s examples with the following code: library(faraway) data(nepali) data(worldcup) library(dlnm) data(chicagoNMMAPS) chic &lt;- chicagoNMMAPS chic_july &lt;- chic %&gt;% filter(month == 7 &amp; year == 1995) 4.2 High data density Guideline 1: Aim for high data density. You should try to increase, as much as possible, the data to ink ratio in your graphs. This is the ratio of “ink” providing information to all ink used in the figure. One way to think about this is that the only graphs you make that use up a lot of your printer’s ink should be packed with information. The two graphs in Figure 4.1 show the same information, but use very different amounts of ink. Each shows the number of players in each of four positions in the worldcup dataset. Notice how, in the plot on the right, a single dot for each category shows the same information that a whole filled bar is showing on the left. Further, the plot on the right has removed the gridded background, removing even more “ink”. Figure 4.1: Example of plots with lower (left) and higher (right) data-to-ink ratios. Each plot shows the number of players in each position in the worldcup dataset from the faraway package. Figure 4.2 gives another example of two plots that show the same information but with very different data densities. This figure uses the chicagoNMMAPS data from the dlnm package, which includes daily mortality, weather, and air pollution data for Chicago, IL. Both plots show daily mortality counts during July 1995, when a very severe heat wave hit Chicago. Notice how many of the elements in the plot on the left, including the shading under the mortality time series and the colored background and grid lines, are unnecessary for interpreting the message from the data. Figure 4.2: Example of plots with lower (left) and higher (right) data-to-ink ratios. Each plot shows daily mortality in Chicago, IL, in July 1995 using the chicagoNMMAPS data from the dlnm package. By increasing the data-to-ink ratio in a plot, you can help viewers see the message of the data more quickly. A cluttered plot is harder to interpret. Further, you leave room to add some of the other elements I’ll talk about, including highlighting interesting data and adding useful references. Notice how the plots on the left in Figures 4.1 and 4.2 are already cluttered and leave little room for adding extra elements, while the plots on the right of those figures have much more room for additions. One quick way to increase data density in ggplot2 is to change the theme for the plot. The theme specifies a number of the “background” elements to a plot, including elements like the plot grid, background color, and the font used for labeling. Some themes come with ggplot2, including: theme_bw theme_minimal theme_void You can find more themes in packages that extend ggplot2. The ggthemes package, in particular, has some excellent additional themes. Figures 4.3 shows some examples of the effects of using different themes. All show the same information– a plot of daily deaths in Chicago in July 1995. The top left graph shows the graph with the default theme. The other plots show the effects of adding different themes, including the black-and-white theme that comes with ggplot2 (top right) and various themes from the ggthemes package. You can even use themes to add some questionable choices for different elements, like the Excel theme (bottom left). Figure 4.3: Daily mortality in Chicago, IL, in July 1995. This figure gives an example of the plot using different themes. 4.3 Meaningful labels Guideline 2: Use clear, meaningful labels. Graphs often default to use abbreviations for axis labels and other labeling. For example, the default is for ggplot2 plots to use column names for the x- and y-axes of a scatterplot. While this is convenient for exploratory plots, it’s often not adequate for plots for presentations and papers. You’ll want to use short and easy-to-type column names in your dataframe to make coding easier, but you should use longer and more meaningful labeling in plots and tables that others need to interpret. Furthermore, text labels can sometimes be aligned in a way that makes them hard to read. For example, when plotting a categorical variable along the x-axis, it can be difficult to fit labels for each category that are long enough to be meaningful. Figure 4.4 gives an example of the same information shown with labels that are harder to interpret (left) versus with clear, meaningful labels (right). Notice how the graph on the left is using abbreviations for the categorical variable (“DF” for “Defense”), abbreviations for axis labels (“Pos” for “Position” and “Pls” for “Number of players”), and has the player position labels in a vertical alignment. On the right graph, I have made the graph easier to quickly read and interpret by spelling out all labels and switching the x- and y-axes, so that there’s room to fully spell out each position while still keeping the alignment horizontal, so the reader doesn’t have to turn the page (or their head) to read the values. Figure 4.4: The number of players in each position in the worldcup data from the faraway package. Both graphs show the same information, but the left graph has murkier labels, while the right graph has labels that are easier to read and interpret. There are a few strategies you can use to make labels clearer when plotting with ggplot2: Add xlab and ylab elements to the plot, rather than relying on the column names in the original data. You can also relabel x- and y-axes with scale elements (e.g., scale_x_continuous), and the scale functions give you more power to also make other changes to the x- and y-axes (e.g., changing break points for the axis ticks). However, if you only need to change axis labels, xlab and ylab are often quicker. Include units of measurement in axis titles when relevant. If units are dollars or percent, check out the scales package, which allows you to add labels directly to axis elements by including arguments like labels = percent in scale elements. See the helpfile for scale_x_continuous for some examples. If the x-variable requires longer labels, as is often the case with categorical data (for example, player positions Figure 4.4), consider flipping the coordinates, rather than abbreviating or rotating the labels. You can use coord_flip to do this. 4.4 References Guideline 3: Provide useful references. Data is easier to interpret when you add references. For example, if you show what it typical, it helps viewers interpret how unusual outliers are. Figure 4.5 shows daily mortality during July 1995 in Chicago, IL. The graph on the right has added shading showing the range of daily death counts in July in Chicago for neighboring years (1990–1994 and 1996–2000). This added reference helps clarify for viewers how unusual the number of deaths during the July 1995 heat wave was. Figure 4.5: Daily mortality during July 1995 in Chicago, IL. In the graph on the right, I have added a shaded region showing the range of daily mortality counts for neighboring years, to show how unusual this event was. Another useful way to add references is to add a linear or smooth fit to the data, to help clarify trends in the data. Figure 4.6 shows the relationship between passes and shots for Forwards in the worldcup dataset. The plot on the right has added a smooth function of the relationship between these two variables. Figure 4.6: Relationship between passes and shots taken among Forwards in the worldcup dataset from the faraway package. The plot on the right has a smooth function added to help show the relationship between these two variables. For scatterplots created with ggplot2, you can use the function geom_smooth to add a smooth or linear reference line. Here is the code that produces Figure 4.6: ggplot(filter(worldcup, Position == &quot;Forward&quot;), geom_point(size = 1.5) + theme_few() + geom_smooth() The most useful geom_smooth parameters to know are: method: The default is to add a loess curve if the data includes less than 1000 points and a generalized additive model for 1000 points or more. However, you can change to show the fitted line from a linear model using method = &quot;lm&quot; or from a generalized linear model using method = &quot;glm&quot;. span: How wiggly or smooth the smooth line should be (smaller value: more wiggly; larger value: more smooth) se: TRUE or FALSE, indicating whether to include shading for 95% confidence intervals. level: Confidence level for confidence interval (e.g., 0.90 for 90% confidence intervals) Lines and polygons can also be useful for adding references, as in Figure 4.5. Useful geoms for such shapes include: geom_hline, geom_vline: Add a horizontal or vertical line geom_abline: Add a line with an intercept and slope geom_polygon: Add a filled polygon geom_path: Add an unfilled polygon You want these references to support the main data shown in the plot, but not overwhelm it. When adding these references: Add reference elements first, so they will be plotted under the data, instead of on top of it. Use alpha to add transparency to these elements. Use colors that are unobtrusive (e.g., grays). For lines, consider using non-solid line types (e.g., linetype = 3). 4.5 Highlighting Guideline 4: Highlight interesting aspects. Consider adding elements to highlight noteworthy elements of the data. For example, in the graph on the right of Figure 4.7, the days of the heat wave (based on temperature measurements) have been highlighted over the mortality time series by using a thick red line. Figure 4.7: Mortality in Chicago, July 1995. In the plot on the right, a thick red line has been added to show the dates of a heat wave. In the below graphs, the names of the players with the most shots and passes have been added to highlight these unusual points. One helpful way to annotate is with text, using geom_text(). For this, you’ll first need to create a dataframe with the hottest day in the data: hottest_day &lt;- chic_july %&gt;% filter(temp == max(temp)) hottest_day[ , 1:6] ## date time year month doy dow ## 1 1995-07-13 3116 1995 7 194 Thursday chic_plot + geom_text(data = hottest_day, label = &quot;Max&quot;, size = 3) With geom_text, you’ll often want to use position adjustment (the position parameter) to move the text so it won’t be right on top of the data points: chic_plot + geom_text(data = hottest_day, label = &quot;Max&quot;, size = 3, hjust = 0, vjust = -1) You can also use lines to highlight. For this, it is often useful to create a new dataframe with data for the reference. To add a line for the Chicago heat wave, I’ve added a dataframe called hw with the relevant date range. I’m setting the y-value to be high enough (425) to ensure the line will be placed above the mortality data. hw &lt;- data.frame(date = c(as.Date(&quot;1995-07-12&quot;), as.Date(&quot;1995-07-16&quot;)), death = c(425, 425)) b &lt;- chic_plot + geom_line(data = hw, aes(x = date, y = death), size = 2) b 4.6 Order Guideline 5: Make order meaningful. You can make the ranking of data clearer from a graph by using order to show rank. Often, factor or categorical variables are ordered by something that is not interesting, like alphabetical order. You can re-order factor variables in a graph by resetting the factor using the factor function and changing the order that levels are included in the levels parameter. 4.7 Small multiples Guideline 6: When possible, use small multiples. Small multiples are graphs that use many small plots showing the same thing for different facets of the data. For example, instead of using color in a single plot to show data for males and females, you could use two small plots, one each for males and females. Typically, in small multiples, all plots use the same x- and y-axes. This makes it easier to compare across plots, and it also allows you to save room by limiting axis annotation. You can use the facet functions to create small multiples. This separates the graph into several small graphs, one for each level of a factor. The facet functions are: facet_grid() facet_wrap() For example, to create small multiples by sex for the Nepali dataset, when plotting height versus weight, you can call: ggplot(nepali, aes(ht, wt)) + geom_point() + facet_grid(. ~ sex) The facet_grid function can facet by one or two variables. One will be shown by rows, and one by columns: ## Generic code facet_grid([factor for rows] ~ [factor for columns]) The facet_wrap() function can only facet by one variable, but it can “wrap” the small graphs for that variable, so the don’t all have to be in one row or column: ## Generic code facet_wrap(~ [factor for faceting], ncol = [number of columns]) Often, when you do faceting, you’ll want to re-name your factors levels or re-order them. For this, you’ll need to use the factor() function on the original vector. For example, to rename the sex factor levels from “1” and “2” to “Male” and “Female”, you can run: nepali &lt;- nepali %&gt;% mutate(sex = factor(sex, levels = c(1, 2), labels = c(&quot;Male&quot;, &quot;Female&quot;))) Notice that the labels for the two graphs have now changed: ggplot(nepali, aes(ht, wt)) + geom_point() + facet_grid(. ~ sex) To re-order the factor, and show the plot for “Female” first, you can use factor to change the order of the levels: nepali &lt;- nepali %&gt;% mutate(sex = factor(sex, levels = c(&quot;Female&quot;, &quot;Male&quot;))) Now notice that the order of the plots has changed: ggplot(nepali, aes(ht, wt)) + geom_point() + facet_grid(. ~ sex) 4.8 Advanced customization 4.8.1 Scales There are a number of different functions for adjusting scales. These follow the following convention: ## Generic code scale_[aesthetic]_[vector type] For example, to adjust the x-axis scale for a continuous variable, you’d use scale_x_continuous. You can use a scale function for an axis to change things like the axis label (which you could also change with xlab or ylab) as well as position and labeling of breaks. For example, here is the default for plotting time versus passes for the worldcup dataset, with the number of shots taken shown by size and position shown by color: ggplot(worldcup, aes(x = Time, y = Passes, color = Position, size = Shots)) + geom_point(alpha = 0.5) ggplot(worldcup, aes(x = Time, y = Passes, color = Position, size = Shots)) + geom_point(alpha = 0.5) + scale_x_continuous(name = &quot;Time played (minutes)&quot;, breaks = 90 * c(2, 4, 6), minor_breaks = 90 * c(1, 3, 5)) Parameters you might find useful in scale functions include: Parameter Description name Label or legend name breaks Vector of break points minor_breaks Vector of minor break points labels Labels to use for each break limits Limits to the range of the axis For dates, you can use scale functions like scale_x_date and scale_x_datetime. For example, here’s a plot of deaths in Chicago in July 1995 using default values for the x-axis: ggplot(chic_july, aes(x = date, y = death)) + geom_line() And here’s an example of changing the formating and name of the x-axis: ggplot(chic_july, aes(x = date, y = death)) + geom_line() + scale_x_date(name = &quot;Date in July 1995&quot;, date_labels = &quot;%m-%d&quot;) You can also use the scale functions to transform an axis. For example, to show the Chicago plot with “deaths” on a log scale, you can run: ggplot(chic_july, aes(x = date, y = death)) + geom_line() + scale_y_log10() For colors and fills, the conventions for the names of the scale functions can vary. For example, to adjust the color scale when you’re mapping a discrete variable (i.e., categorical, like gender or animal breed) to color, you’d use scale_color_hue. To adjust the color scale for a continuous variable, like age, you’ll use scale_color_gradient. For any color scales, consider starting with brewer first (e.g., scale_color_brewer, scale_color_distiller). Scale functions from brewer allow you to set colors using different palettes. You can explore these palettes at http://colorbrewer2.org/. The Brewer palettes fall into three categories: sequential, divergent, and qualitative. You should use sequential or divergent for continuous data and qualitative for categorical data. Use display.brewer.pal to show the palette for a given number of colors. library(RColorBrewer) display.brewer.pal(name = &quot;Set1&quot;, n = 8) display.brewer.pal(name = &quot;PRGn&quot;, n = 8) display.brewer.pal(name = &quot;PuBuGn&quot;, n = 8) Use the palette argument within a scales function to customize the palette: a &lt;- ggplot(data.frame(x = 1:5, y = rnorm(5), group = letters[1:5]), aes(x = x, y = y, color = group)) + geom_point() b &lt;- a + scale_color_brewer(palette = &quot;Set1&quot;) c &lt;- a + scale_color_brewer(palette = &quot;Pastel2&quot;) + theme_dark() grid.arrange(a, b, c, ncol = 3) ggplot(worldcup, aes(x = Time, y = Passes, color = Position, size = Shots)) + geom_point(alpha = 0.5) + scale_color_brewer(palette = &quot;Dark2&quot;, name = &quot;Player position&quot;) You can also set colors manually: ggplot(worldcup, aes(x = Time, y = Passes, color = Position, size = Shots)) + geom_point(alpha = 0.5) + scale_color_manual(values = c(&quot;blue&quot;, &quot;red&quot;, &quot;darkgreen&quot;, &quot;darkgray&quot;)) 4.9 To find out more Some excellent further references for plotting are: R Graphics Cookbook (book and website) Google images For more technical details about plotting in R: ggplot2: Elegant Graphics for Data Analysis, Hadley Wickham R Graphics, Paul Murrell 4.10 In-course exercise 4.10.1 Designing a plot For today’s exercise, you’ll be building a plot using the worldcup data from the faraway package. First, load in that data. Next, say you want to look at the relationship between the number of minutes that a player played in the 2010 World Cup (Time) and the number of shots the player took on goal (Shots). On a sheet of paper, and talking with your partner, decide how the two of you would design a plot to explore and present this relationship. How would you incorporate some of the principles of creating good graphs? 4.10.1.1 Example R code library(faraway) data(worldcup) head(worldcup, 2) ## Team Position Time Shots Passes Tackles Saves ## Abdoun Algeria Midfielder 16 0 6 0 0 ## Abe Japan Midfielder 351 0 101 14 0 This dataset has the players’ names as rownames, rather than in a column. Once we start using dplyr functions, we’ll lose these rownames. Therefore, start by converting the rownames to a column called Player: library(dplyr) worldcup &lt;- worldcup %&gt;% mutate(Player = rownames(worldcup)) head(worldcup, 2) ## Team Position Time Shots Passes Tackles Saves Player ## 1 Algeria Midfielder 16 0 6 0 0 Abdoun ## 2 Japan Midfielder 351 0 101 14 0 Abe 4.10.2 Basic scatterplots with R base graphics and ggplot Perform the following tasks: Install and load the ggplot2 and ggthemes packages. For the worldcup data, plot a scatterplot of Time (on the x-axis) versus Shots. Add some ggplot elements to make this plot a bit more attractive. (For example, change the x- and y-axis labels and add a title.) 4.10.2.1 Example R code Install and load the ggplot2 package: # install.packages(&quot;ggplot2&quot;) library(ggplot2) # install.packages(&quot;ggthemes&quot;) library(ggthemes) Use the R base graphics to create a scatterplot of Time versus Shots: ggplot(worldcup, aes(x = Time, y = Shots)) + geom_point() Use some of the plotting options to improve the graph’s appearance: ggplot(worldcup, aes(x = Time, y = Shots)) + geom_point() + xlab(&quot;Minutes played&quot;) + ylab(&quot;Shots on goal&quot;) + ggtitle(&quot;Time played vs. shots taken\\n World Cup 2010&quot;) 4.10.3 Fancier graphs In this section, we’ll work on creating a plot like this: Try the following tasks: First, before you start coding, talk with your group members about how this graph is different from the simple one you created with ggplot in the last section. Also discuss what you can figure out from this new graph that was less clear from the simpler one you created before. Use the xlab() function to make a clearer title for the x-axis. (You may have already written this code in the last section of this exercise.) Often, in graphs with a lot of points, it’s hard to see some of the points, because they overlap other points. Three strategies to address this are: (a) make the points smaller; (b) make the points somewhat transparent; and (c) jitter the points. Try doing the first two with the simple ggplot scatterplot you created in the previous section of Shots by Time. Create a new column in the worldcup data called top_four that specifies whether or not the Team for that observation was one of the top four teams in the tournament (Netherlands, Uruguay, Spain, and Germany). Make the colors of the points correspond to whether the team was a top-four team. Create small multiples. The relationship between time played and shots taken is probably different by the players’ positions. Use faceting to create different graphs for each position. Make order count: What order are the faceted graphs currently in? Offensive players have more chances to take shots than defensive players, so that might be a useful ordering for the facets. Re-order the Position factor column to go from nearest your own goal to nearest the opponents goal, and then re-plot the graph from the previous step. Highlighting interesting data: Who had the most shots in the 2010 World Cup? Was he on a top-four team? Use geom_text() to label his point on the graph with his name. Increase data density: Try changing the theme, to come up with a graph with a bit less non-data ink. From the ggthemes package (you’ll need to install it if you don’t already have it), try some of the following themes: theme_few(), theme_tufte(), theme_stata(), theme_fivethirtyeight(), theme_economist_white(), and theme_wsj(). 4.10.3.1 Example R code As a reminder, here’s the code to do a simple scatterplot ot Shots by Time for the worldcup data: ggplot(worldcup, aes(x = Time, y = Shots)) + geom_point() To add a clearer x-axis label that the current one, use xlab(): ggplot(worldcup, aes(x = Time, y = Shots)) + geom_point() + xlab(&quot;Time played in World Cup (minutes)&quot;) To make the points smaller, use the size option in geom_point() (smaller than about 2 = smaller than default, larger than about 2 = larger than default): ggplot(worldcup, aes(x = Time, y = Shots)) + geom_point(size = 1) + xlab(&quot;Time played in World Cup (minutes)&quot;) To make the points semi-transparent, use the alpha option in geom_point() (closer to 0 = more tranparent, closer to 1 = more opaque): ggplot(worldcup, aes(x = Time, y = Shots)) + geom_point(alpha = 0.5) + xlab(&quot;Time played in World Cup (minutes)&quot;) To jitter the points some, use the position = &quot;jitter&quot; option in geom_point(): ggplot(worldcup, aes(x = Time, y = Shots)) + geom_point(size = 1.5, position = &quot;jitter&quot;, alpha = 0.5) + xlab(&quot;Time played in World Cup (minutes)&quot;) As an alternative, you could also jitter the points by using geom_jitter rather than geom_point: ggplot(worldcup, aes(x = Time, y = Shots)) + geom_jitter(size = 1.5, alpha = 0.5, width = 0.25) + xlab(&quot;Time played in World Cup (minutes)&quot;) “Jittering” the points means adding some extra random noise in either the x- or y-direction, or in both directions. This technique can be particularly useful when you are trying to plot points for which one dimension is categorical, rather than continuous. You can specify which direction (x and / or y) is jittered, as well as the amount of noise to add. To find out more, check out the helpfile for the geom_jitter function. To create a new column called top_four, first create vector that lists those top four teams, then create a logical vector in the dataframe for whether the team for that observation is in one of the top four teams: worldcup &lt;- worldcup %&gt;% mutate(top_four = Team %in% c(&quot;Spain&quot;, &quot;Germany&quot;, &quot;Uruguay&quot;, &quot;Netherlands&quot;)) summary(worldcup$top_four) ## Mode FALSE TRUE NA&#39;s ## logical 517 78 0 To color points by this variable, use color = in the aes() part of the ggplot() call: ggplot(worldcup, aes(x = Time, y = Shots, color = top_four)) + geom_point(size = 1.5, position = &quot;jitter&quot;, alpha = 0.5) + xlab(&quot;Time played in World Cup (minutes)&quot;) To create nicer labels for the legend for color, convert the top_four column into the factor class, with the labels you want to use in the figure legend: worldcup &lt;- worldcup %&gt;% mutate(top_four = factor(top_four, levels = c(TRUE, FALSE), labels = c(&quot;Top 4&quot;, &quot;Other&quot;))) summary(worldcup$top_four) ## Top 4 Other ## 78 517 ggplot(worldcup, aes(x = Time, y = Shots, color = top_four)) + geom_point(size = 1.5, position = &quot;jitter&quot;, alpha = 0.5) + xlab(&quot;Time played in World Cup (minutes)&quot;) As a note, you can use the scale_color_discrete() function to put in a nicer legend title: ggplot(worldcup, aes(x = Time, y = Shots, color = top_four)) + geom_point(size = 1.5, position = &quot;jitter&quot;, alpha = 0.5) + xlab(&quot;Time played in World Cup (minutes)&quot;) + scale_color_discrete(name = &quot;Team&#39;s final\\n ranking&quot;) To create small multiples, use the facet_grid() command: ggplot(worldcup, aes(x = Time, y = Shots, color = top_four)) + geom_point(size = 1.5, position = &quot;jitter&quot;, alpha = 0.5) + xlab(&quot;Time played in World Cup (minutes)&quot;) + scale_color_discrete(name = &quot;Team&#39;s final\\n ranking&quot;) + facet_grid(. ~ Position) To re-order the Position column of the dataframe, use the levels option of the factor() function. This re-sets how R saves the order of the levels of this factor. worldcup &lt;- worldcup %&gt;% mutate(Position = factor(Position, levels = c(&quot;Goalkeeper&quot;, &quot;Defender&quot;, &quot;Midfielder&quot;, &quot;Forward&quot;))) levels(worldcup$Position) ## [1] &quot;Goalkeeper&quot; &quot;Defender&quot; &quot;Midfielder&quot; &quot;Forward&quot; Note from this code example that you can use the levels function to find out the levels and their order for a factor-class vector. Then use the same code from before for your plot: ggplot(worldcup, aes(x = Time, y = Shots, color = top_four)) + geom_point(size = 1.5, position = &quot;jitter&quot;, alpha = 0.5) + xlab(&quot;Time played in World Cup (minutes)&quot;) + scale_color_discrete(name = &quot;Team&#39;s final\\n ranking&quot;) + facet_grid(. ~ Position) You can use the filter function with a logical statement comparing Shots to the maximum value of Shots (max(Shots)) to filter down to the row or rows of the player or players with the most shots: most_shots &lt;- worldcup %&gt;% filter(Shots == max(Shots)) most_shots ## Team Position Time Shots Passes Tackles Saves Player top_four ## 1 Ghana Forward 501 27 151 1 0 Gyan Other Use geom_text() to label his point on the graph with his name. You may need to mess around with some of the options in geom_text(), like size, hjust, and vjust (hjust and vjust say where, in relation to the point location, to put the label), to get something you’re happy with. Also, I pasted on an extra space at the end of the player’s name, to add some padding so the label wouldn’t be right on top of the point. ggplot(worldcup, aes(x = Time, y = Shots, color = top_four)) + geom_point(size = 1.5, position = &quot;jitter&quot;, alpha = 0.5) + xlab(&quot;Time played in World Cup (minutes)&quot;) + scale_color_discrete(name = &quot;Team&#39;s final\\n ranking&quot;) + facet_grid(. ~ Position) + geom_text(data = most_shots, aes(label = paste(Player, &quot; &quot;)), colour = &quot;black&quot;, size = 3, hjust = 1, vjust = 0.4) Try out different themes for the plot. First, I’ll save everything we’ve done so far as the object shot_plot, then I’ll try adding different themes: shot_plot &lt;- ggplot(worldcup, aes(x = Time, y = Shots, color = top_four)) + geom_point(size = 1.5, position = &quot;jitter&quot;, alpha = 0.5) + xlab(&quot;Time played in World Cup (minutes)&quot;) + scale_color_discrete(name = &quot;Team&#39;s final\\n ranking&quot;) + facet_grid(. ~ Position) + geom_text(data = most_shots, aes(label = paste(Player, &quot; &quot;)), colour = &quot;black&quot;, size = 3, hjust = 1, vjust = 0.4) shot_plot + theme_few() shot_plot + theme_tufte() shot_plot + theme_wsj() shot_plot + theme_fivethirtyeight() shot_plot + theme_stata() ## Warning: `panel.margin` is deprecated. Please use `panel.spacing` property ## instead ## Warning: `legend.margin` must be specified using `margin()`. For the old ## behavior use legend.spacing shot_plot + theme_economist_white() ## Warning: `panel.margin` is deprecated. Please use `panel.spacing` property ## instead ## Warning: `legend.margin` must be specified using `margin()`. For the old ## behavior use legend.spacing 4.10.4 Data visualization cheatsheet RStudio comes with some excellent cheatsheets, which provide quick references to functions and code you might find useful for different tasks. For this part of the group exercise, you’ll explore their cheatsheet for data visualization, both to learn some new ggplot2 code and to become familiar with how to use this cheatsheet as you do your own analysis. Open the data visualization cheatsheet. You can do this from RStudio by going to “Help” -&gt; “Cheatsheets” -&gt; “Data Visualization with ggplot2”. Notice that different sections give examples with some datasets that come with either base R or ggplot2. For example, under the “Graphical Primitives” section, there is code defining the object a as a ggplot object using the “seals” dataset: a &lt;- ggplot(seals, aes(x = long, y = lat)). Go through the cheatsheet and list all of the example datasets that are used in this cheatsheet. Open their helpfiles to learn more about the data. Create the example datasets a through l and s through t using the code given on the cheatsheet. Pick at least one example to try out from each of the following sections: “Graphical Primitives”, “One Variable”, at least three subsections of “Two Variables”, “Three Variables”, “Scales”, “Faceting”, and “Position Adjustments”. As you try these, try to figure out any aesthetics that you aren’t familiar with (e.g., ymin, ymax). Also, use helpfiles for the geoms to look up parameters you aren’t familiar with (e.g., stat for geom_area). If you can’t figure out how to interpret a plot, check the helpfile for the associated geom. Note: For the n geom used in “scales”, it should be defined as n &lt;- d + geom_bar(aes(fill = fl)). 4.10.4.1 Example R code The code for opening the helpfiles for the example datasets is: ?seals ?economics ?mpg ?diamonds ?USArrests Note that, for USArrests, only some of the columns are pulled out (e.g., murder = USArrests$murder) to use in the data example dataframe. Further, the “Visualizing error” examples use a dataframe created specifically for these examples, called df. Some of the base R and ggplot2 example datasets have become fairly well-known. Some that you’ll see very often in examples are the iris, mpg, and diamonds datasets. All of the code to create the datasets a through l and s through t is given somewhere on the cheatsheet. Here it is in full: a &lt;- ggplot(seals, aes(x = long, y = lat)) b &lt;- ggplot(economics, aes(date, unemploy)) c &lt;- ggplot(mpg, aes(hwy)) d &lt;- ggplot(mpg, aes(fl)) e &lt;- ggplot(mpg, aes(cty, hwy)) f &lt;- ggplot(mpg, aes(class, hwy)) g &lt;- ggplot(diamonds, aes(cut, color)) h &lt;- ggplot(diamonds, aes(carat, price)) i &lt;- ggplot(economics, aes(date, unemploy)) df &lt;- data.frame(grp = c(&quot;A&quot;, &quot;B&quot;), fit = 4.5, se = 1:2) j &lt;- ggplot(df, aes(grp, fit, ymin = fit - se, ymax = fit + se)) data &lt;- data.frame(murder = USArrests$Murder, state = tolower(rownames(USArrests))) map &lt;- map_data(&quot;state&quot;) k &lt;- ggplot(data, aes(fill = murder)) seals$z &lt;- with(seals, sqrt(delta_long^2 + delta_lat^2)) l &lt;- ggplot(seals, aes(long, lat)) s &lt;- ggplot(mpg, aes(fl, fill = drv)) t &lt;- ggplot(mpg, aes(cty, hwy)) + geom_point() Notice that, in some places, the aesthetics are defined using the full aesthetic name-value pair (e.g., aes(x = long, y = lat)), while in other places the code relies on position for defining which column of a dataframe maps to which aesthetic (e.g., aes(cty, hwy) or aes(fl)). Either is fine, although relying on position can result in errors if you are not very familiar with the order in which parameters are defined for a function. This code will vary based on the examples you try, but here is some code for one set of examples: b + geom_ribbon(aes(ymin = unemploy - 900, ymax = unemploy + 900)) c + geom_dotplot() f + geom_violin(scale = &quot;area&quot;) h + geom_hex() j + geom_pointrange() k + geom_map(aes(map_id = state), map = map) + expand_limits(x = map$long, y = map$lat) l + geom_contour(aes(z = z)) n &lt;- d + geom_bar(aes(fill = fl)) n + scale_fill_brewer(palette = &quot;Blues&quot;) o &lt;- c + geom_dotplot(aes(fill = ..x..)) o + scale_fill_gradient(low = &quot;red&quot;, high = &quot;yellow&quot;) t + facet_grid(year ~ fl) s + geom_bar(position = &quot;fill&quot;) "],
["reproducible-research-1.html", "Chapter 5 Reproducible research #1 5.1 What is reproducible research? 5.2 Markdown 5.3 Literate programming in R 5.4 Style guidelines 5.5 More with knitr 5.6 Saving graphics files 5.7 In-course exercise", " Chapter 5 Reproducible research #1 Download a pdf of the lecture slides covering this topic. 5.1 What is reproducible research? A data analysis is reproducible if all the information (data, files, etc.) required is available for someone else to re-do your entire analysis. This includes: Data available All code for cleaning raw data All code and software (specific versions, packages) for analysis Some advantages of making your research reproducible are: You can (easily) figure out what you did six months from now. You can (easily) make adjustments to code or data, even early in the process, and re-run all analysis. When you’re ready to publish, you can (easily) do a last double-check of your full analysis, from cleaning the raw data through generating figures and tables for the paper. You can pass along or share a project with others. You can give useful code examples to people who want to extend your research. Here is a famous research example of the dangers of writing code that is hard to double-check or confirm: The Economist The New York Times Simply Statistics Some of the steps required to making research reproducible are: All your raw data should be saved in the project directory. You should have clear documentation on the source of all this data. Scripts should be included with all the code used to clean this data into the data set(s) used for final analyses and to create any figures and tables. You should include details on the versions of any software used in analysis (for R, this includes the version of R as well as versions of all packages used). If possible, there should be no “by hand” steps used in the analysis; instead, all steps should be done using code saved in scripts. For example, you should use a script to clean data, rather than cleaning it by hand in Excel. If any “non-scriptable” steps are unavoidable, you should very clearly document those steps. There are several software tools that can help you improve the reproducibility of your research: knitr: Create files that include both your code and text. These can be rendered to create final reports and papers. They keep code within the final file for the report. knitr complements: Create fancier tables and figures within RMarkdown documents. Packages include tikzDevice, animate, xtables, and pander. packrat: Save versions of each package used for the analysis, then load those package versions when code is run again in the future. In this section, I will focus on using knitr and RMarkdown files. 5.2 Markdown R Markdown files are mostly written using Markdown. To write R Markdown files, you need to understand what markup languages like Markdown are and how they work. In Word and other word processing programs you have used, you can add formatting using buttons and keyboard shortcuts (e.g., “Ctrl-B” for bold). The file saves the words you type. It also saves the formatting, but you see the final output, rather than the formatting markup, when you edit the file (WYSIWYG – what you see is what you get). In markup languages, on the other hand, you markup the document directly to show what formatting the final version should have (e.g., you type **bold** in the file to end up with a document with bold). Examples of markup languages include: HTML (HyperText Markup Language) LaTex Markdown (a “lightweight” markup language) For example, Figure 5.1 some marked-up HTML code from CSU’s website, while Figure 5.2 shows how that file looks when it’s rendered by a web browser. Figure 5.1: Example of the source of an HTML file. Figure 5.2: Example of a rendered HTML file. To write a file in Markdown, you’ll need to learn the conventions for creating formatting. This table shows what you would need to write in a flat file for some common formatting choices: Code Rendering Explanation **text** text boldface *text* text italicized [text](www.google.com) text hyperlink # text first-level header ## text second-level header Some other simple things you can do in Markdown include: Lists (ordered or bulleted) Equations Tables Figures from file Block quotes Superscripts For more Markdown conventions, see RStudio’s R Markdown Reference Guide (link also available through “Help” in RStudio). 5.3 Literate programming in R Literate programming, an idea developed by Donald Knuth, mixes code that can be executed with regular text. The files you create can then be rendered, to run any embedded code. The final output will have results from your code and the regular text. The knitr package can be used for literate programming in R. In essence, knitr allows you to write an R Markdown file that can be rendered into a pdf, Word, or HTML document. Here are the basics of opening and rendering an R Markdown file in RStudio: To open a new R Markdown file, go to “File” -&gt; “New File” -&gt; “RMarkdown…” -&gt; for now, chose a “Document” in “HTML” format. This will open a new R Markdown file in RStudio. The file extension for R Markdown files is “.Rmd”. The new file comes with some example code and text. You can run the file as-is to try out the example. You will ultimately delete this example code and text and replace it with your own. Once you “knit” the R Markdown file, R will render an HTML file with the output. This is automatically saved in the same directory where you saved your .Rmd file. Write everything besides R code using Markdown syntax. To include R code in an RMarkdown document, you need to separate off the code chunk using the following syntax: ```{r} my_vec &lt;- 1:10 ``` This syntax tells R how to find the start and end of pieces of R code when the file is rendered. R will walk through, find each piece of R code, run it and create output (printed output or figures, for example), and then pass the file along to another program to complete rendering (e.g., Tex for pdf files). You can specify a name for each chunk, if you’d like, by including it after “r” when you begin your chunk. For example, to give the name load_nepali to a code chunk that loads the nepali dataset, specify that name in the start of the code chunk: ```{r load_nepali} library(faraway) data(nepali) ``` Here are a couple of tips for naming code chunks: Chunk names must be unique across a document. Any chunks you don’t name are given numbers by knitr. You do not have to name each chunk. However, there are some advantages: It will be easier to find any errors. You can use the chunk labels in referencing for figure labels. You can reference chunks later by name. You can add options when you start a chunk. Many of these options can be set as TRUE / FALSE and include: Option Action echo Print out the R code? eval Run the R code? messages Print out messages? warnings Print out warnings? include If FALSE, run code, but don’t print code or results Other chunk options take values other than TRUE / FALSE. Some you might want to include are: Option Action results How to print results (e.g., hide runs the code, but doesn’t print the results) fig.width Width to print your figure, in inches (e.g., fig.width = 4) fig.height Height to print your figure Add these options in the opening brackets and separate multiple ones with commas: ```{r messages = FALSE, echo = FALSE} nepali[1, 1:3] ``` I will cover other chunk options later, once you’ve gotten the chance to try writting R Markdown files. You can set “global” options at the beginning of the document. This will create new defaults for all of the chunks in the document. For example, if you want echo, warning, and message to be FALSE by default in all code chunks, you can run: ```{r global_options} knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE) ``` If you set both global and local chunk options that you set specifically for a chunk will take precedence over global options. For example, running a document with: ```{r global_options} knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE) ``` ```{r check_nepali, echo = TRUE} head(nepali, 1) ``` would print the code for the check_nepali chunk, because the option specified for that specific chunk (echo = TRUE) would override the global option (echo = FALSE). You can also include R output directly in your text (“inline”) using backticks: “There are `r nrow(nepali)` observations in the nepali data set. The average age is `r mean(nepali$age, na.rm = TRUE)` months.” Once the file is rendered, this gives: “There are 1000 observations in the nepali data set. The average age is 37.662 months.” Here are two tips that will help you diagnose some problems rendering R Markdown files: Be sure to save your R Markdown file before you run it. All the code in the file will run “from scratch”– as if you just opened a new R session. The code will run using, as a working directory, the directory where you saved the R Markdown file. You’ll want to try out pieces of your code as you write an R Markdown document. There are a few ways you can do that: You can run code in chunks just like you can run code from a script (Ctrl-Return or the “Run” button). You can run all the code in a chunk (or all the code in all chunks) using the different options under the “Run” button in RStudio. All the “Run” options have keyboard shortcuts, so you can use those. You can render R Markdown documents to other formats: Word Pdf (requires that you’ve installed “Tex” on your computer.) Slides (ioslides) Click the button to the right of “Knit” to see different options for rendering on your computer. You can freely post your RMarkdown documents at RPubs. If you want to post to RPubs, you need to create an account. Once you do, you can click the “Publish” button on the window that pops up with your rendered file. RPubs can also be a great place to look for interesting example code, although it sometimes can be pretty overwhelmed with MOOC homework. If you’d like to find out more, here are two good how-to books on reproducible research in R (the CSU library has both in hard copy): Reproducible Research with R and RStudio, Christopher Gandrud Dynamic Documents with R and knitr, Yihui Xie 5.4 Style guidelines R style guidelines provide rules for how to format code in an R script. Some people develop their own style as they learn to code. However, it is easy to get in the habit of following style guidelines, and they offer some important advantages: Clean code is easier to read and interpret later. It’s easier to catch and fix mistakes when code is clear. Others can more easily follow and adapt your code if it’s clean. Some style guidelines will help prevent possible problems (e.g., avoiding . in function names). For this course, we will use R style guidelines from two sources: Google’s R style guidelines Hadley Wickham’s R style guidelines These two sets of style guidelines are very similar. Hear are a few guidelines we’ve already covered in class: Use &lt;-, not =, for assignment. Guidelines for naming objects: All lowercase letters or numbers Use underscore (_) to separate words, not camelCase or a dot (.) (this differs for Google and Wickham style guides) Have some consistent names to use for “throw-away” objects (e.g., df, ex, a, b) Make names meaningful Descriptive names for R scripts (“random_group_assignment.R”) Nouns for objects (todays_groups for an object with group assignments) Verbs for functions (make_groups for the function to assign groups) 5.4.1 Line length Google: Keep lines to 80 characters or less To set your script pane to be limited to 80 characters, go to “RStudio” -&gt; “Preferences” -&gt; “Code” -&gt; “Display”, and set “Margin Column” to 80. # Do my_df &lt;- data.frame(n = 1:3, letter = c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;), cap_letter = c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;)) # Don&#39;t my_df &lt;- data.frame(n = 1:3, letter = c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;), cap_letter = c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;)) This guideline helps ensure that your code is formatted in a way that you can see all of the code without scrolling horizontally (left and right). 5.4.2 Spacing Binary operators (e.g., &lt;-, +, -) should have a space on either side A comma should have a space after it, but not before. Colons should not have a space on either side. Put spaces before and after = when assigning parameter arguments # Do shots_per_min &lt;- worldcup$Shots / worldcup$Time #Don&#39;t shots_per_min&lt;-worldcup$Shots/worldcup$Time #Do ave_time &lt;- mean(worldcup[1:10 , &quot;Time&quot;]) #Don&#39;t ave_time&lt;-mean(worldcup[1 : 10 ,&quot;Time&quot;]) 5.4.3 Semicolons Although you can use a semicolon to put two lines of code on the same line, you should avoid it. # Do a &lt;- 1:10 b &lt;- 3 # Don&#39;t a &lt;- 1:10; b &lt;- 3 5.4.4 Commenting For a comment on its own line, use #. Follow with a space, then the comment. You can put a short comment at the end of a line of R code. In this case, put two spaces after the end of the code, one #, and one more space before the comment. If it helps make it easier to read your code, separate sections using a comment character followed by many hyphens (e.g., #------------). Anything after the comment character is “muted”. # Read in health data --------------------------- # Clean exposure data --------------------------- 5.4.5 Indentation Google: Within function calls, line up new lines with first letter after opening parenthesis for parameters to function calls: Example: # Relabel sex variable nepali$sex &lt;- factor(nepali$sex, levels = c(1, 2), labels = c(&quot;Male&quot;, &quot;Female&quot;)) 5.4.6 Code grouping Group related pieces of code together. Separate blocks of code by empty spaces. # Load data library(faraway) data(nepali) # Relabel sex variable nepali$sex &lt;- factor(nepali$sex, levels = c(1, 2), labels = c(&quot;Male&quot;, &quot;Female&quot;)) Note that this grouping often happens naturally when using tidyverse functions, since they encourage piping (%&gt;% and +). 5.4.7 Broader guidelines Omit needless code. Don’t repeat yourself. We’ll learn more about satisfying these guidelines when we talk about writing your own functions in the next part of the class. 5.5 More with knitr 5.5.1 Equations in knitr You can write equations in RMarkdown documents by setting them apart with dollar signs ($). For an equation on a line by itself (display equation), you two $s before and after the equation, on separate lines, then use LaTex syntax for writing the equations. To help with this, you may want to use this LaTex math cheat sheet.. You may also find an online LaTex equation editor like Codecogs.com helpful. Note: Equations denoted this way will always compile for pdf documents, but won’t always come through on Markdown files (for example, GitHub won’t compile math equations). For example, writing this in your R Markdown file: $$ E(Y_{t}) \\sim \\beta_{0} + \\beta_{1}X_{1} $$ will result in this rendered equation: \\[ E(Y_{t}) \\sim \\beta_{0} + \\beta_{1}X_{1} \\] To put math within a sentence (inline equation), just use one $ on either side of the math. For example, writing this in a R Markdown file: &quot;We are trying to model $E(Y_{t})$.&quot; The rendered document will show up as: “We are trying to model \\(E(Y_{t})\\).” 5.5.2 Figures from file You can include not only figures that you create with R, but also figures that you have saved on your computer. The best way to do that is with the include_graphics function in knitr: library(knitr) include_graphics(&quot;figures/CSU_ram.png&quot;) This example would include a figure with the filename “MyFigure.png” that is saved in the “figures” sub-directory of the parent directory of the directory where your .Rmd is saved. Don’t forget that you will need to give an absolute pathway or the relative pathway from the directory where the .Rmd file is saved. 5.5.3 Saving graphics files You can save figures that you create in R. Typically, you won’t need to save figures for an R Markdown file, since you can include figure code directly. However, you will sometimes want to save a figure from a script. You have two options: Use the “Export” choice in RStudio Write code to export the figure in your R script To make your research more reproducible, use the second choice. To use code export a figure you created in R, take three steps: Open a graphics device (e.g., pdf(&quot;MyFile.pdf&quot;)). Write the code to print your plot. Close the graphics device using dev.off(). For example, the following code would save a scatterplot of time versus passes as a pdf named “MyFigure” in the “figures” subdirectory of the current working directory: pdf(&quot;figures/MyFigure.pdf&quot;, width = 8, height = 6) ggplot(worldcup, aes(x = Time, y = Passes)) + geom_point(aes(color = Position)) + theme_bw() dev.off() If you create multiple plots before you close the device, they’ll all save to different pages of the same pdf file. You can open a number of different graphics devices. Here are some of the functions you can use to open graphics devices: pdf png bmp jpeg tiff svg 5.6 Saving graphics files You will use a device-specific function to open a graphics device (e.g., pdf). However, you will always close these devices with dev.off. Most of the functions to open graphics devices include parameters like height and width. These can be used to specify the size of the output figure. The units for these depend on the device (e.g., inches for pdf, pixels by default for png). Use the helpfile for the function to determine these details. 5.6.1 Tables in R Markdown If you want to create a nice, formatted table from an R dataframe, you can do that using kable from the knitr package. my_df &lt;- data.frame(letters = c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;), numbers = 1:3) kable(my_df) letters numbers a 1 b 2 c 3 There are a few options for the kable function: arg expl colnames Column names (default: column name in the dataframe) align A vector giving the alignment for each column (‘l’, ‘c’, ‘r’) caption Table caption digits Number of digits to round to. If you want to round columns different amounts, use a vector with one element for each column. my.df &lt;- data.frame(letters = c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;), numbers = rnorm(3)) kable(my.df, digits = 2, align = c(&quot;r&quot;, &quot;c&quot;), caption = &quot;My new table&quot;, col.names = c(&quot;First 3 letters&quot;, &quot;First 3 numbers&quot;)) Table 5.1: My new table First 3 letters First 3 numbers a 0.55 b 1.22 c 1.99 From Yihui: “Want more features? No, that is all I have. You should turn to other packages for help. I’m not going to reinvent their wheels.” If you want to do fancier tables, you may want to explore the xtable and pander packages. As a note, these might both be more effective when compiling to pdf, rather than html. 5.7 In-course exercise For all of today’s tasks, you’ll use the code from last week’s in-course exercise to do the exercises. This week we are not focusing on writing new code, but rather on how to take R code and put it in an R Markdown file, so we can create reports from files that include the original code. 5.7.1 Creating a Markdown document First, you’ll create a Markdown document, without any R code in it yet. In RStudio, go to “File” -&gt; “New File” -&gt; “R Markdown”. From the window that brings up, choose “Document” on the left-hand column and “HTML” as the output format. A new file will open in the script pane of your RStudio session. Save this file (you may pick the name and directory). The file extension should be “.Rmd”. First, before you try to write your own Markdown, try rendering the example that the script includes by default. (This code is always included, as a template, when you first open a new RMarkdown file using the RStudio “New file” interface we used in this example.) Try rendering this default R Markdown example by clicking the “Knit” button at the top of the script file. For some of you, you may not yet have everything you need on your computer to be able to get this to work. If so, let me know. RStudio usually includes all the necessary tools when you install it, but there may be some exceptions. If you could get the document to knit, do the following tasks: Look through the HTML document that was created. Compare it to the R Markdown script that created it, and see if you can understand, at least broadly, what’s going on. Look in the directory where you saved the R Markdown file. You should now also see a new, .html file in that folder. Try opening it with a web browser like Safari. Go back to the R Markdown file. Delete everything after the initial header information (everything after the 6th line). In the header information, make sure the title, author, and date are things you’re happy with. If not, change them. Using Markdown syntax, write up a description of the data (worldcup) we used last week to create the fancier figure. Try to include the following elements: Bold and italic text Hyperlinks A list, either ordered or bulleted Headers 5.7.2 Adding in R code Now incorporate the R code from last week’s exercise into your document. Once you get the document to render with some basic pieces of code in it, try the following: Try some different chunk options. For example, try setting echo = FALSE in some of your code chunks. Similarly, try using the options results = &quot;hide&quot; and include = FALSE. You should have at least one code chunk that generates figures. Try experimenting with the fig.width and fig.height options for the chunk to change the size of the figure. Try using the global commands. See if you can switch the echo default value for this document from TRUE (the usual default) to FALSE. 5.7.3 Working with R Markdown documents Finally, try the following tasks to get some experience working with R Markdown files in RStudio: Go to one of your code chunks. Locate the small gray arrow just to the left of the line where you initiate the code chunk. Click on it and see what happens. Then click on it again. Put your cursor inside one of your code chunks. Try using the “Run” button (or Ctrl-Return) to run code in that chunk at your R console. Did it work? Pick a code chunk in your document. Put your cursor somewhere in the code in that chunk. Click on the “Run” button and choose “Run All Chunks Above”. What did that do? If it did not work, what do you think might be going on? (Hint: Check getwd() and think about which directory you’ve used to save your R Markdown file.) Pick another chunk of code. Put the cursor somewhere in the code for that chunk. Click on the “Run” button and choose “Run Current Chunk”. Then try “Run Next Chunk”. Try to figure out all the options the “Run” button gives you and when each might be useful. Click on the small gray arrow to the right of the “Knit HTML” button. If the option is offered, select “Knit Word” and try it. What does this do? 5.7.4 R style guidelines Go through all the R code in your R Markdown file. Are there are places where your code is not following style conventions for R? Clean up your code to correct any of these issues. 5.7.5 Trying out knitr with your own data Pick a dataset either from your own research or something interesting available online (if you’re struggling to find something, check out Five Thirty Eight’s GitHub data repository). Create an R Markdown document. Add some text to describe the data you’re using. Write some code to read in the data (you can save it to your computer and read it from a file or, if the data’s online, read it in directly). Use dplyr functions (especially summarize) to create a dataframe with some summary statistics for the data. Print this out (just as R output, not as a formatted table, for now). Try using kable to create a formatted version of the summary table you created. Create at least one plot using the data. Try using fig.width and fig.height chunk options to change the size of the figure in the output. Find an image online related to your data. Save it to your computer and use include_graphics from the knitr package to include it in your R Markdown document. "],
["entering-and-cleaning-data-2.html", "Chapter 6 Entering and cleaning data #2 6.1 Joining datasets 6.2 Tidy data 6.3 Gathering 6.4 In-course exercise", " Chapter 6 Entering and cleaning data #2 Download a pdf of the lecture slides covering this topic. 6.1 Joining datasets So far, you have only worked with a single data source at a time. When you work on your own projects, however, you typically will need to merge together two or more datasets to create the a data frame to answer your research question. For example, for air pollution epidemiology, you will often have to join several datasets: Health outcome data (e.g., number of deaths per day) Air pollution concentrations Weather measurements (since weather can be a confounder) Demographic data The dplyr package has a family of different functions to join two dataframes together, the *_join family of functions. All combine two dataframes, which I’ll call x and y here. The functions include: inner_join(x, y): Keep only rows where there are observations in both x and y. left_join(x, y): Keep all rows from x, whether they have a match in y or not. right_join(x, y): Keep all rows from y, whether they have a match in x or not. full_join(x, y): Keep all rows from both x and y, whether they have a match in the other dataset or not. In the examples, I’ll use two datasets, x and y. Both datasets include the column course. The other column in x is grade, while the other column in y is day. Observations exist for courses x and y in both datasets, but for w and z in only one dataset. x &lt;- data.frame(course = c(&quot;x&quot;, &quot;y&quot;, &quot;z&quot;), grade = c(90, 82, 78)) y &lt;- data.frame(course = c(&quot;w&quot;, &quot;x&quot;, &quot;y&quot;), day = c(&quot;Tues&quot;, &quot;Mon / Fri&quot;, &quot;Tue&quot;)) Here is what these two example datasets look like: x ## course grade ## 1 x 90 ## 2 y 82 ## 3 z 78 y ## course day ## 1 w Tues ## 2 x Mon / Fri ## 3 y Tue With inner_join, you’ll only get the observations that show up in both datasets. That means you’ll lose data on z (only in the first dataset) and w (only in the second dataset). inner_join(x, y) ## Joining, by = &quot;course&quot; ## course grade day ## 1 x 90 Mon / Fri ## 2 y 82 Tue With left_join, you’ll keep everything in x (the “left” dataset), but not keep things in y that don’t match something in x. That means that, here, you’ll lose w: left_join(x, y) ## Joining, by = &quot;course&quot; ## course grade day ## 1 x 90 Mon / Fri ## 2 y 82 Tue ## 3 z 78 &lt;NA&gt; right_join is the opposite: right_join(x, y) ## Joining, by = &quot;course&quot; ## course grade day ## 1 w NA Tues ## 2 x 90 Mon / Fri ## 3 y 82 Tue full_join keeps everything from both datasets: full_join(x, y) ## Joining, by = &quot;course&quot; ## course grade day ## 1 x 90 Mon / Fri ## 2 y 82 Tue ## 3 z 78 &lt;NA&gt; ## 4 w NA Tues 6.2 Tidy data All of the material in this section comes directly from Hadley Wickham’s paper on tidy data. You will need to read this paper to prepare for the quiz on this section. Getting your data into a “tidy” format makes it easier to model and plot. By taking the time to tidy your data at the start of an analysis, you will save yourself time, and make it easier to plan out later steps. Characteristics of tidy data are: Each variable forms a column. Each observation forms a row. Each type of observational unit forms a table. Here are five common problems that Hadley Wickham has identified that keep data from being tidy: Column headers are values, not variable names. Multiple variables are stored in one column. Variables are stored in both rows and columns. Multiple types of observational units are stored in the same table. A single observational unit is stored in multiple tables. Here are examples (again, from Hadley Wickham’s paper on tidy data, which is required reading for this week of the course) of each of these problems. Column headers are values, not variable names. Solution: Multiple variables are stored in one column. Solution: Variables are stored in both rows and columns. Solution: Multiple types of observational units are stored in the same table. Solution: A single observational unit is stored in multiple tables. Example: exposure and outcome data stored in different files: File 1: Daily mortality counts File 2: Daily air pollution measurements 6.3 Gathering There are two functions from the tidyr package (another member of the tidyverse) that you can use to change between wide and long data: gather and spread. Here is a description of these two functions: gather: Take several columns and gather them into two columns, one with the former column names, and one with the former cell values. spread: Take two columns and spread them into multiple columns. Column names for the new columns will come from one of the two original columns, while cell values will come from the other of the original columns. The following examples are from tidyr help files and show the effects of gathering and spreading a dataset. Here is some simulated wide data: wide_stocks[1:3, ] ## time X Y Z ## 1 2009-01-01 -0.1257781 -0.09127212 -3.2463716 ## 2 2009-01-02 1.1159311 -0.35177323 -4.0367217 ## 3 2009-01-03 2.0656407 -2.47318344 0.2121228 In the wide_stocks dataset, there are separate columns for three different stocks (X, Y, and Z). Each cell gives the value for a certain stock on a certain day. This data isn’t “tidy”, because the identify of the stock (X, Y, or Z) is a variable, and you’ll probably want to include it as a variable in modeling. wide_stocks[1:3, ] ## time X Y Z ## 1 2009-01-01 -0.1257781 -0.09127212 -3.2463716 ## 2 2009-01-02 1.1159311 -0.35177323 -4.0367217 ## 3 2009-01-03 2.0656407 -2.47318344 0.2121228 If you want to convert the dataframe to have all stock values in a single column, you can use gather to convert wide data to long data: long_stocks &lt;- gather(wide_stocks, key = stock, value = price, -time) long_stocks[1:5, ] ## time stock price ## 1 2009-01-01 X -0.1257781 ## 2 2009-01-02 X 1.1159311 ## 3 2009-01-03 X 2.0656407 ## 4 2009-01-04 X 0.3503618 ## 5 2009-01-05 X 0.2159070 In this “long” dataframe, there is now one column that gives the identify of the stock (stock) and another column that gives the price of that stock that day (price): long_stocks[1:5, ] ## time stock price ## 1 2009-01-01 X -0.1257781 ## 2 2009-01-02 X 1.1159311 ## 3 2009-01-03 X 2.0656407 ## 4 2009-01-04 X 0.3503618 ## 5 2009-01-05 X 0.2159070 The format for a gather call is: ## Generic code new_df &lt;- gather(old_df, key = [name of column with old column names], value = [name of column with cell values], - [name of column(s) you want to exclude from gather]) Three important notes: Everything is gathered into one of two columns – one column with the old column names, and one column with the old cell values With the key and value arguments, you are just providing column names for the two columns that everything’s gathered into. If there is a column you don’t want to gather (date in the example), use - to exclude it in the gather call. Notice how easy it is, now that the data is gathered, to use stock for aesthetics of faceting in a ggplot2 call: ggplot(long_stocks, aes(x = time, y = price)) + geom_line() + facet_grid(. ~ stock) If you have data in a “long” format and would like to spread it out, you can use spread to do that: stocks &lt;- spread(long_stocks, key = stock, value = price) stocks[1:5, ] ## time X Y Z ## 1 2009-01-01 -0.1257781 -0.09127212 -3.2463716 ## 2 2009-01-02 1.1159311 -0.35177323 -4.0367217 ## 3 2009-01-03 2.0656407 -2.47318344 0.2121228 ## 4 2009-01-04 0.3503618 -1.14999383 -3.7303754 ## 5 2009-01-05 0.2159070 -0.46444486 -3.5374791 Notice that this reverses the action of gather. “Spread” data is typically not tidy, so you often won’t want to use spread when you are preparing data for analysis. However, spread can be very helpful in creating clean tables for final reports and presentations. For example, if you wanted to create a table with means and standard deviations for each of the three stocks, you could use spread to rearrange the final summary to create an attractive table. stock_summary &lt;- long_stocks %&gt;% group_by(stock) %&gt;% summarize(N = n(), mean = mean(price), sd = sd(price)) stock_summary ## # A tibble: 3 × 4 ## stock N mean sd ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 X 10 0.01512573 1.081888 ## 2 Y 10 -0.32605485 1.851385 ## 3 Z 10 -2.80244951 4.062946 stock_summary %&gt;% mutate(&quot;Mean (Std.dev.)&quot; = paste0(round(mean, 2), &quot; (&quot;, round(sd, 2), &quot;)&quot;)) %&gt;% select(- mean, - sd) %&gt;% gather(key = &quot;Statistic&quot;, value = &quot;Value&quot;, -stock) %&gt;% spread(key = stock, value = Value) %&gt;% knitr::kable() Statistic X Y Z Mean (Std.dev.) 0.02 (1.08) -0.33 (1.85) -2.8 (4.06) N 10 10 10 6.4 In-course exercise For today’s exercise, we’ll be using the following three datasets (click on the file name to access the correct file for today’s class for each dataset): File name Description country_timeseries.csv Ebola cases by country for the 2014 outbreak mexico_exposure.csv and mexico_deaths.csv Daily death counts and environmental measurements for Mexico City, Mexico, for 2008 measles_data/ Number of cases of measles in CA since end of Dec. 2014 Note that you likely have already downloaded all the files in the measles_data folder, since we used them in an earlier in-course exercise. If so, there is no need to re-download those files. Here are the sources for this data: country_timeseries.csv : Caitlin Rivers’ Ebola repository (Caitlin originally collected this data from the WHO and WHO Situation reports) mexico_exposure.csv and mexico_deaths.csv : one of Hadley Wickham’s GitHub repos (Hadley got the data originally from the Secretaria de Salud of Mexico’s website, although it appears the link is now broken. I separated the data into two dataframes so students could practice merging.) measles_data/: one of scarpino’s GitHub repos (Data originally from pdfs from the California Department of Public Health) If you want to use these data further, you should go back and pull them from their original sources. They are here only for use in R code examples for this course. Here are some of the packages you will need for this exercise: library(tidyverse) library(gridExtra) library(ggthemes) 6.4.1 Designing tidy data Check out the country_timeseries.csv file on Ebola for this week’s example data. Talk with your partner and decide what changes you would need to make to this dataset to turn it into a “tidy” dataset, in particular which of the five common “untidy” problems the data currently has and why. Do the same for the data on daily mortality and daily weather in Mexico. Do the same for the set of files with measles data. 6.4.2 Easier data wrangling Use read_csv to read the mexico data (exposure and mortality) directly from GitHub into your R session. Call the dataframes mex_deaths and mex_exp. Merge the two datasets together to create the dataframe mexico. Exclude all columns except the outcome (deaths), date, and mean temperature. Convert the date to a date class. Try combining all the steps in the previous task into one “chained” command using the pipe operator, %&gt;%. Use this new dataframe to plot deaths by date using ggplot. 6.4.2.1 Example R code Use read_csv to read the mexico data (exposure and mortality) directly from GitHub into your R session. Call the dataframes mex_deaths and mex_exp: deaths_url &lt;- paste0(&quot;https://github.com/geanders/RProgrammingForResearch/&quot;, &quot;raw/master/data/mexico_deaths.csv&quot;) mex_deaths &lt;- read_csv(deaths_url) head(mex_deaths) ## # A tibble: 6 × 2 ## day deaths ## &lt;chr&gt; &lt;int&gt; ## 1 1/1/08 296 ## 2 1/2/08 274 ## 3 1/3/08 339 ## 4 1/4/08 300 ## 5 1/5/08 327 ## 6 1/6/08 332 exposure_url &lt;- paste0(&quot;https://github.com/geanders/RProgrammingForResearch/&quot;, &quot;raw/master/data/mexico_exposure.csv&quot;) mex_exp &lt;- read_csv(exposure_url) head(mex_exp) ## # A tibble: 6 × 14 ## day temp_min temp_max temp_mean humidity wind NO ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1/1/08 7.8 17.8 11.770833 53.45833 2.6625000 0.009250000 ## 2 1/2/08 2.6 9.8 6.637500 61.70833 3.3458333 0.005416667 ## 3 1/3/08 1.1 15.6 7.041667 59.91667 1.8857143 0.015958333 ## 4 1/4/08 3.1 20.6 10.862500 57.54167 1.1958333 0.040833333 ## 5 1/5/08 6.0 21.3 13.404167 45.70833 0.9875000 0.046916667 ## 6 1/6/08 7.2 22.1 14.341667 40.75000 0.8541667 0.028583333 ## # ... with 7 more variables: NO2 &lt;dbl&gt;, NOX &lt;dbl&gt;, O3 &lt;dbl&gt;, CO &lt;dbl&gt;, ## # SO2 &lt;dbl&gt;, PM10 &lt;dbl&gt;, PM25 &lt;dbl&gt; Merge the two datasets together to create the dataframe mexico. Exclude all columns except the outcome (deaths), date, and mean temperature. Convert the date to a date class. mexico &lt;- full_join(mex_deaths, mex_exp, by = &quot;day&quot;) mexico &lt;- select(mexico, day, deaths, temp_mean) library(lubridate) ## For parsing dates mexico &lt;- mutate(mexico, day = mdy(day)) Try combining all the steps in the previous task into one “chained” command: mexico &lt;- full_join(mex_deaths, mex_exp, by = &quot;day&quot;) %&gt;% select(day, deaths, temp_mean) %&gt;% mutate(day = mdy(day)) head(mexico) ## # A tibble: 6 × 3 ## day deaths temp_mean ## &lt;date&gt; &lt;int&gt; &lt;dbl&gt; ## 1 2008-01-01 296 11.770833 ## 2 2008-01-02 274 6.637500 ## 3 2008-01-03 339 7.041667 ## 4 2008-01-04 300 10.862500 ## 5 2008-01-05 327 13.404167 ## 6 2008-01-06 332 14.341667 Use this new dataframe to plot deaths by date using ggplot: ggplot(mexico, aes(x = day, y = deaths)) + geom_point(size = 1.5, alpha = 0.5) + xlab(&quot;Date in 2008&quot;) + ylab(&quot;# of deaths&quot;) + ggtitle(&quot;Deaths by date&quot;) + theme_few() 6.4.3 More extensive data wrangling Read the ebola data directly from GitHub into your R session. Call the dataframe ebola. Use dplyr functions to create a tidy dataset. First, change it from “wide” data to “long” data. Name the new column with the key variable and the new column with the values count. Run the following code to create new columns named type and country that split up the variable column into type (“Cases” or “Deaths”) and country (“Guinea”, “Liberia”, etc.). (This type of code is moving towards using regular expressions to clean up really messy data, which we’ll talk about some in the third section.) foo &lt;- strsplit(as.character(ebola$variable), split = &quot;_&quot;) bar &lt;- matrix(unlist(foo), ncol = 2, byrow = TRUE) ebola$type &lt;- factor(bar[ , 1]) ebola$country &lt;- factor(bar[ , 2]) Use dplyr functions and piping to remove Day and variable (now that you’ve split it into type and country) and to convert Date to a date class. Use the dplyr function spread() to convert the data so you have separate columns for the two variables of numbers of Cases and Deaths. Remove any observations where counts of cases or deaths are missing for that country. Challenge question (you can do the next step without doing this, but your graphs won’t be in order): Create a dataframe called case_sum that gives the total number of cases recorded for each country. (Hint: Use the dplyr functions group_by() and summarize().) Use arrange() to re-order this dataset by the order of the number of cases, and then use this arrangement to re-order the levels in country in your main ebola dataset, so that your graphs in the next step will be ordered from the country with the most ebola cases to the one with the least. Now that your data is tidy, create one plot showing ebola cases by date, faceted by country, and one showing ebola deaths by date, also faceted by country. Try using the option scales = &quot;free_y&quot; in the facet_wrap() function (in the gridExtra package) and see how that changes these graphs. Based on these plots, what would your next questions be about this data before you used it for an analysis? Super-challenge question: Can you put all of the steps of this cleaning process into just a few “chaining” calls? 6.4.3.1 Example R code Read the data in using read_csv. (Review question: Why can’t you read it directly using read.csv()?) ebola_url &lt;- paste0(&quot;https://github.com/geanders/RProgrammingForResearch/&quot;, &quot;raw/master/data/country_timeseries.csv&quot;) ebola &lt;- read_csv(ebola_url) head(ebola) ## # A tibble: 6 × 18 ## Date Day Cases_Guinea Cases_Liberia Cases_SierraLeone ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1/5/2015 289 2776 NA 10030 ## 2 1/4/2015 288 2775 NA 9780 ## 3 1/3/2015 287 2769 8166 9722 ## 4 1/2/2015 286 NA 8157 NA ## 5 12/31/2014 284 2730 8115 9633 ## 6 12/28/2014 281 2706 8018 9446 ## # ... with 13 more variables: Cases_Nigeria &lt;int&gt;, Cases_Senegal &lt;int&gt;, ## # Cases_UnitedStates &lt;int&gt;, Cases_Spain &lt;int&gt;, Cases_Mali &lt;int&gt;, ## # Deaths_Guinea &lt;int&gt;, Deaths_Liberia &lt;int&gt;, Deaths_SierraLeone &lt;int&gt;, ## # Deaths_Nigeria &lt;int&gt;, Deaths_Senegal &lt;int&gt;, Deaths_UnitedStates &lt;int&gt;, ## # Deaths_Spain &lt;int&gt;, Deaths_Mali &lt;int&gt; Change the data to long data using the gather() function from dplyr: ebola &lt;- gather(ebola, variable, count, -Date, -Day) head(ebola) ## # A tibble: 6 × 4 ## Date Day variable count ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; ## 1 1/5/2015 289 Cases_Guinea 2776 ## 2 1/4/2015 288 Cases_Guinea 2775 ## 3 1/3/2015 287 Cases_Guinea 2769 ## 4 1/2/2015 286 Cases_Guinea NA ## 5 12/31/2014 284 Cases_Guinea 2730 ## 6 12/28/2014 281 Cases_Guinea 2706 Split variable into type and country: foo &lt;- strsplit(as.character(ebola$variable), split = &quot;_&quot;) ebola[ , c(&quot;type&quot;, &quot;country&quot;)] &lt;- matrix(unlist(foo), ncol = 2, byrow = TRUE) head(ebola) ## # A tibble: 6 × 6 ## Date Day variable count type country ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1/5/2015 289 Cases_Guinea 2776 Cases Guinea ## 2 1/4/2015 288 Cases_Guinea 2775 Cases Guinea ## 3 1/3/2015 287 Cases_Guinea 2769 Cases Guinea ## 4 1/2/2015 286 Cases_Guinea NA Cases Guinea ## 5 12/31/2014 284 Cases_Guinea 2730 Cases Guinea ## 6 12/28/2014 281 Cases_Guinea 2706 Cases Guinea Use dplyr functions and piping to remove Day and variable and to convert Date to a date class: ebola &lt;- select(ebola, -Day, -variable) %&gt;% mutate(Date = mdy(Date)) head(ebola) ## # A tibble: 6 × 4 ## Date count type country ## &lt;date&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 2015-01-05 2776 Cases Guinea ## 2 2015-01-04 2775 Cases Guinea ## 3 2015-01-03 2769 Cases Guinea ## 4 2015-01-02 NA Cases Guinea ## 5 2014-12-31 2730 Cases Guinea ## 6 2014-12-28 2706 Cases Guinea Convert the data so you have separate columns for the two variables of numbers of Cases and Deaths: ebola &lt;- spread(ebola, type, count) head(ebola) ## # A tibble: 6 × 4 ## Date country Cases Deaths ## &lt;date&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 2014-03-22 Guinea 49 29 ## 2 2014-03-22 Liberia NA NA ## 3 2014-03-22 Mali NA NA ## 4 2014-03-22 Nigeria NA NA ## 5 2014-03-22 Senegal NA NA ## 6 2014-03-22 SierraLeone NA NA Remove any observations where counts of cases or deaths are missing for that country: ebola &lt;- filter(ebola, !is.na(Cases) &amp; !is.na(Deaths)) head(ebola) ## # A tibble: 6 × 4 ## Date country Cases Deaths ## &lt;date&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 2014-03-22 Guinea 49 29 ## 2 2014-03-24 Guinea 86 59 ## 3 2014-03-25 Guinea 86 60 ## 4 2014-03-26 Guinea 86 62 ## 5 2014-03-27 Guinea 103 66 ## 6 2014-03-27 Liberia 8 6 Create a dataframe called case_sum that gives the total number of cases recorded for each country. Use arrange() to re-order this dataset by the order of the number of cases: case_sum &lt;- group_by(ebola, country) %&gt;% summarize(Cases = sum(Cases, na.rm = TRUE)) case_sum ## # A tibble: 8 × 2 ## country Cases ## &lt;chr&gt; &lt;int&gt; ## 1 Guinea 82099 ## 2 Liberia 191351 ## 3 Mali 42 ## 4 Nigeria 636 ## 5 Senegal 24 ## 6 SierraLeone 211169 ## 7 Spain 16 ## 8 UnitedStates 59 Use this arrangement to re-order the levels in country in your main ebola dataset, so that your graphs in the next step will be ordered from the country with the most ebola cases to the one with the least: case_sum &lt;- arrange(case_sum, desc(Cases)) case_sum ## # A tibble: 8 × 2 ## country Cases ## &lt;chr&gt; &lt;int&gt; ## 1 SierraLeone 211169 ## 2 Liberia 191351 ## 3 Guinea 82099 ## 4 Nigeria 636 ## 5 UnitedStates 59 ## 6 Mali 42 ## 7 Senegal 24 ## 8 Spain 16 ebola &lt;- mutate(ebola, country = factor(country, levels = case_sum$country)) levels(ebola$country) ## [1] &quot;SierraLeone&quot; &quot;Liberia&quot; &quot;Guinea&quot; &quot;Nigeria&quot; ## [5] &quot;UnitedStates&quot; &quot;Mali&quot; &quot;Senegal&quot; &quot;Spain&quot; Now that your data is tidy, create one plot showing ebola cases by date, faceted by country, and one showing ebola deaths by date, also faceted by country: ggplot(ebola, aes(x = Date, y = Cases)) + geom_line() + facet_wrap(~ country, ncol = 4) + theme_few() ggplot(ebola, aes(x = Date, y = Deaths)) + geom_line() + facet_wrap(~ country, ncol = 4) + theme_few() Try using the option scales = &quot;free_y&quot; in the facet_wrap() function (in the gridExtra package) and see how that changes these graphs: ggplot(ebola, aes(x = Date, y = Cases)) + geom_line() + facet_wrap(~ country, ncol = 4, scales = &quot;free_y&quot;) + theme_few() ggplot(ebola, aes(x = Date, y = Deaths)) + geom_line() + facet_wrap(~ country, ncol = 4, scales = &quot;free_y&quot;) + theme_few() Put all of the steps of this cleaning process into just a few “chaining” calls. (Note: I’m using sub here instead of strsplit for the variable-splitting step, just to keep the code a bit cleaner. Again, this is using regular expressions, which we’ll cover more later in the course.) ebola &lt;- read_csv(ebola_url) %&gt;% gather(variable, count, -Date, -Day) %&gt;% mutate(type = sub(&quot;_.*&quot;, &quot;&quot;, variable), country = sub(&quot;.*_&quot;, &quot;&quot;, variable)) %&gt;% select(-Day, -variable) %&gt;% mutate(Date = mdy(Date)) %&gt;% spread(type, count) %&gt;% filter(!is.na(Cases) &amp; !is.na(Deaths)) case_sum &lt;- group_by(ebola, country) %&gt;% summarize(Cases = sum(Cases, na.rm = TRUE)) %&gt;% arrange(desc(Cases)) ebola &lt;- mutate(ebola, country = factor(country, levels = case_sum$country)) case_sum ## # A tibble: 8 × 2 ## country Cases ## &lt;chr&gt; &lt;int&gt; ## 1 SierraLeone 211169 ## 2 Liberia 191351 ## 3 Guinea 82099 ## 4 Nigeria 636 ## 5 UnitedStates 59 ## 6 Mali 42 ## 7 Senegal 24 ## 8 Spain 16 head(ebola) ## # A tibble: 6 × 4 ## Date country Cases Deaths ## &lt;date&gt; &lt;fctr&gt; &lt;int&gt; &lt;int&gt; ## 1 2014-03-22 Guinea 49 29 ## 2 2014-03-24 Guinea 86 59 ## 3 2014-03-25 Guinea 86 60 ## 4 2014-03-26 Guinea 86 62 ## 5 2014-03-27 Guinea 103 66 ## 6 2014-03-27 Liberia 8 6 6.4.4 Tidying VADeaths data R comes with a dataset called VADeaths that gives death rates per 1,000 people in Virginia in 1940 by age, sex, and rural / urban. Use data(&quot;VADeaths&quot;) to load this data. Make sure you understand what each column and row is showing – use the helpfile (?VADeaths) if you need. Go through the three characteristics of tidy data and the five common problems in untidy data that we talked about in class. Sketch out (you’re welcome to use the whiteboards) what a tidy version of this data would look like. Open a new R script file. Write R code to transform this dataset into a tidy dataset. Try using a pipe chain, with %&gt;% and tidyverse functions, to clean the data. Use the tidy data to create the following graph: There is no example R code for this – try to figure out the code yourselves. We will go over a solution in class. You may find the RStudio Data Wrangling cheatsheet helpful for remembering which tidyverse functions do what. 6.4.5 Exploring Fatality Analysis Reporting System (FARS) data Explore the interactive visualization at http://metrocosm.com/10-years-of-traffic-accidents-mapped.html. This was created by Max Galka using this dataset. Go to FARS web page. We want to get the raw data on fatal accidents. Navigate this page to figure out how you can get this raw data for the whole county for 2015. Save 2015 data to your computer. What is the structure of how this data is saved (e.g., directory structure, file structure)? On the FARS web page, find the documentation describing this raw data. Look through both this documentation and the raw files you downloaded to figure out what information is included in the data. Read the accident.csv file for 2015 into R (this is one of the files you’ll get if you download the raw data for 2015). Use the documentation to figure out what each column represents. Discuss what steps you would need to take to create the following plot. To start, don’t write any code, just develop a plan. Talk about what the dataset should look like right before you create the plot and what functions you could use to get the data from its current format to that format. (Hint: Functions from the lubridate package will be very helpful, including yday and wday). Discuss which of the variables in this dataset could be used to merge the dataset with other appropriate data, either other datasets in the FARS raw data, or outside datasets. Try to write the code to create this plot. This will include some code for cleaning the data and some code for plotting. I will add one example answer after class, but I’d like you to try to figure it out yourselves first. "],
["exploring-data-2.html", "Chapter 7 Exploring data #2 7.1 Parentheses 7.2 Loops 7.3 Other control structures 7.4 Functions 7.5 Regular expressions 7.6 In-course exercise", " Chapter 7 Exploring data #2 Download a pdf of the lecture slides covering this topic. 7.1 Parentheses If you put parentheses around an entire code statement, it will both run the code and print out the answer. study_months &lt;- c(&quot;Jan&quot;, &quot;Feb&quot;, &quot;Mar&quot;) study_months ## [1] &quot;Jan&quot; &quot;Feb&quot; &quot;Mar&quot; (study_months &lt;- c(&quot;Jan&quot;, &quot;Feb&quot;, &quot;Mar&quot;)) ## [1] &quot;Jan&quot; &quot;Feb&quot; &quot;Mar&quot; 7.2 Loops Loops allow you to “walk through” and repeat the same code for different values of an index. For each run of the loop, R is told that, for some index in some vector, do some code. For example, the following loop specifies: For i in 1:3, print(i). for(i in c(1, 2, 3)){ print(i) } ## [1] 1 ## [1] 2 ## [1] 3 Note that this code is equivalent to: i &lt;- 1 print(i) ## [1] 1 i &lt;- 2 print(i) ## [1] 2 i &lt;- 3 print(i) ## [1] 3 Often, the index will be set to a number for each cycle of the loop, and then the index will be used within the code to index vectors or dataframes: study_months &lt;- c(&quot;Jan&quot;, &quot;Feb&quot;, &quot;Mar&quot;) for(i in c(1, 3)){ print(study_months[i]) } ## [1] &quot;Jan&quot; ## [1] &quot;Mar&quot; Often, you want to set the index to sequential numbers (e.g., 1, 2, 3, 4). In this case, you can save time by using the : notation to create a vector of a sequence of numbers: for(i in 1:3){ print(i) } ## [1] 1 ## [1] 2 ## [1] 3 With this notation, sometimes it may be helpful to use the length function to set the largest index value for the loop as the length of a vector (or nrow for indexing a dataframe). For example: study_months &lt;- c(&quot;Jan&quot;, &quot;Feb&quot;, &quot;Mar&quot;) for(i in 1:length(study_months)){ print(study_months[i]) } ## [1] &quot;Jan&quot; ## [1] &quot;Feb&quot; ## [1] &quot;Mar&quot; Sometimes, you want to set the index for each cycle of the loop to something that is not a number. You can set the index to any class of vector. Remember that a loop works by saying for some index in some vector, do some code. For example, you may want to run: for study_month in study_months, print(study_month): study_months &lt;- c(&quot;Jan&quot;, &quot;Feb&quot;, &quot;Mar&quot;) for(study_month in study_months){ print(study_month) } ## [1] &quot;Jan&quot; ## [1] &quot;Feb&quot; ## [1] &quot;Mar&quot; Note that this is equivalent to: study_month &lt;- &quot;Jan&quot; print(study_month) ## [1] &quot;Jan&quot; study_month &lt;- &quot;Feb&quot; print(study_month) ## [1] &quot;Feb&quot; study_month &lt;- &quot;Mar&quot; print(study_month) ## [1] &quot;Mar&quot; What would this loop do? vars &lt;- c(&quot;Time&quot;, &quot;Shots&quot;, &quot;Passes&quot;, &quot;Tackles&quot;, &quot;Saves&quot;) for(i in 1:length(vars)){ var_mean &lt;- mean(worldcup[ , vars[i]]) print(var_mean) } vars &lt;- c(&quot;Time&quot;, &quot;Shots&quot;, &quot;Passes&quot;, &quot;Tackles&quot;, &quot;Saves&quot;) for(i in 1:length(vars)){ var_mean &lt;- mean(worldcup[ , vars[i]]) print(var_mean) } ## [1] 208.8639 ## [1] 2.304202 ## [1] 84.52101 ## [1] 4.191597 ## [1] 0.6672269 What would this loop do? vars &lt;- c(&quot;Time&quot;, &quot;Shots&quot;, &quot;Passes&quot;, &quot;Tackles&quot;, &quot;Saves&quot;) for(i in 1:length(vars)){ var_mean &lt;- mean(worldcup[ , vars[i]]) var_mean &lt;- round(var_mean, 1) out &lt;- paste0(&quot;mean of &quot;, vars[i], &quot;: &quot;, var_mean) print(out) } To figure out, you can set i &lt;- 1 and then walk through the loop: i &lt;- 1 (var_mean &lt;- mean(worldcup[ , vars[i]])) ## [1] 208.8639 (var_mean &lt;- round(var_mean, 1)) ## [1] 208.9 (out &lt;- paste0(&quot;mean of &quot;, vars[i], &quot;: &quot;, var_mean)) ## [1] &quot;mean of Time: 208.9&quot; vars &lt;- c(&quot;Time&quot;, &quot;Shots&quot;, &quot;Passes&quot;, &quot;Tackles&quot;, &quot;Saves&quot;) for(i in 1:length(vars)){ var_mean &lt;- mean(worldcup[ , vars[i]]) var_mean &lt;- round(var_mean, 1) out &lt;- paste0(&quot;mean of &quot;, vars[i], &quot;: &quot;, var_mean) print(out) } ## [1] &quot;mean of Time: 208.9&quot; ## [1] &quot;mean of Shots: 2.3&quot; ## [1] &quot;mean of Passes: 84.5&quot; ## [1] &quot;mean of Tackles: 4.2&quot; ## [1] &quot;mean of Saves: 0.7&quot; Often, it’s convenient to create a dataset to fill up as you loop through: vars &lt;- c(&quot;Time&quot;, &quot;Shots&quot;, &quot;Passes&quot;, &quot;Tackles&quot;, &quot;Saves&quot;) my_df &lt;- data.frame(variable = vars, mean = NA) for(i in 1:nrow(my_df)){ var_mean &lt;- mean(worldcup[ , vars[i]]) my_df[i , &quot;mean&quot;] &lt;- round(var_mean, 1) } vars &lt;- c(&quot;Time&quot;, &quot;Shots&quot;, &quot;Passes&quot;, &quot;Tackles&quot;, &quot;Saves&quot;) (my_df &lt;- data.frame(variable = vars, mean = NA)) ## variable mean ## 1 Time NA ## 2 Shots NA ## 3 Passes NA ## 4 Tackles NA ## 5 Saves NA i &lt;- 1 (var_mean &lt;- mean(worldcup[ , vars[i]])) ## [1] 208.8639 my_df[i , &quot;mean&quot;] &lt;- round(var_mean, 1) my_df ## variable mean ## 1 Time 208.9 ## 2 Shots NA ## 3 Passes NA ## 4 Tackles NA ## 5 Saves NA for(i in 1:nrow(my_df)){ var_mean &lt;- mean(worldcup[ , vars[i]]) my_df[i , &quot;mean&quot;] &lt;- round(var_mean, 1) } my_df ## variable mean ## 1 Time 208.9 ## 2 Shots 2.3 ## 3 Passes 84.5 ## 4 Tackles 4.2 ## 5 Saves 0.7 Note: This is a pretty simplistic example. There are some easier ways to have done this: worldcup %&gt;% summarize(Time = mean(Time), Passes = mean(Passes), Shots = mean(Shots), Tackles = mean(Tackles), Saves = mean(Saves)) %&gt;% gather(key = var, value = mean) %&gt;% mutate(mean = round(mean, 1)) ## var mean ## 1 Time 208.9 ## 2 Passes 84.5 ## 3 Shots 2.3 ## 4 Tackles 4.2 ## 5 Saves 0.7 Another way to have done this is with apply: means &lt;- apply(worldcup[ , vars], 2, mean) (means &lt;- round(means, 1)) ## Time Shots Passes Tackles Saves ## 208.9 2.3 84.5 4.2 0.7 However, you can use this same looping process for much more complex tasks that you can’t do as easily with apply or dplyr tools. Loops can be very useful for more complex repeated tasks. For example: Creating this graph requires that you: Create a subset limited to each of the four positions Fit a Poisson regression of Passes on Time within each subset Pull the regression coefficient and standard error from each model Use those values to calculate 95% confidence intervals Convert everything from log relative rate to relative rate Plot everything Create a vector with the names of all positions. Create an empty dataframe to store regression results: (positions &lt;- unique(worldcup$Position)) ## [1] Midfielder Defender Forward Goalkeeper ## Levels: Defender Forward Goalkeeper Midfielder (pos_est &lt;- data.frame(position = positions, est = NA, se = NA)) ## position est se ## 1 Midfielder NA NA ## 2 Defender NA NA ## 3 Forward NA NA ## 4 Goalkeeper NA NA Loop through and fit a Poisson regression model for each subset of data. Save regression coefficients in the empty dataframe: for(i in 1:nrow(pos_est)){ pos_df &lt;- worldcup %&gt;% filter(Position == positions[i]) pos_mod &lt;- glm(Passes ~ Time, data = pos_df, family = poisson(link = &quot;log&quot;)) pos_coefs &lt;- summary(pos_mod)$coefficients[2, 1:2] pos_est[i, c(&quot;est&quot;, &quot;se&quot;)] &lt;- pos_coefs } pos_est[1:2, ] ## position est se ## 1 Midfielder 0.004716096 4.185925e-05 ## 2 Defender 0.004616260 5.192736e-05 Calculate 95% confidence intervals for log relative risk values: pos_est &lt;- pos_est %&gt;% mutate(lower_ci = est - 1.96 * se, upper_ci = est + 1.96 * se) pos_est %&gt;% select(position, est, lower_ci, upper_ci) ## position est lower_ci upper_ci ## 1 Midfielder 0.004716096 0.004634052 0.004798140 ## 2 Defender 0.004616260 0.004514483 0.004718038 ## 3 Forward 0.005299009 0.005158945 0.005439074 ## 4 Goalkeeper 0.003101124 0.002770562 0.003431687 Calculate relative risk per 90 minute increase in minutes played: pos_est &lt;- pos_est %&gt;% mutate(rr_est = exp(90 * est), rr_low = exp(90 * lower_ci), rr_high = exp(90 * upper_ci)) pos_est %&gt;% select(position, rr_est, rr_low, rr_high) ## position rr_est rr_low rr_high ## 1 Midfielder 1.528747 1.517501 1.540077 ## 2 Defender 1.515073 1.501258 1.529015 ## 3 Forward 1.611090 1.590908 1.631527 ## 4 Goalkeeper 1.321941 1.283192 1.361861 Re-level the position factor so the plot will be ordered from highest to lowest estimates: pos_est &lt;- arrange(pos_est, rr_est) %&gt;% mutate(position = factor(position, levels = position)) pos_est %&gt;% select(position, est) ## position est ## 1 Goalkeeper 0.003101124 ## 2 Defender 0.004616260 ## 3 Midfielder 0.004716096 ## 4 Forward 0.005299009 Create the plot: ggplot(pos_est, aes(x = rr_low, y = position)) + geom_segment(aes(xend = rr_high, yend = position)) + geom_point(aes(x = rr_est, y = position)) + theme_few() + ylab(&quot;&quot;) + scale_x_continuous(paste(&quot;Relative rate of passes\\nper&quot;, &quot;90 minute increase in minutes played&quot;), limits = c(1.0, max(pos_est$rr_high))) + geom_vline(aes(xintercept = 1), color = &quot;lightgray&quot;) 7.3 Other control structures 7.3.1 if / else loops There are other control structures you can use in your R code. Two that you will commonly use within R functions are if and ifelse statements. An if statement tells R that, if a certain condition is true, do run some code. For example, if you wanted to print out only odd numbers between 1 and 5, one way to do that is with an if statement: (Note: the %% operator in R returns the remainder of the first value (i) divided by the second value (2).) for(i in 1:5){ if(i %% 2 == 1){ print(i) } } ## [1] 1 ## [1] 3 ## [1] 5 The if statement runs some code if a condition is true, but does nothing if it is false. If you’d like different code to run depending on whether the condition is true or false, you can us an if / else or an if / else if / else statement. for(i in 1:5){ if(i %% 2 == 1){ print(i) } else { print(paste(i, &quot;is even&quot;)) } } ## [1] 1 ## [1] &quot;2 is even&quot; ## [1] 3 ## [1] &quot;4 is even&quot; ## [1] 5 What would this code do? for(i in 1:100){ if(i %% 3 == 0 &amp; i %% 5 == 0){ print(&quot;FizzBuzz&quot;) } else if(i %% 3 == 0){ print(&quot;Fizz&quot;) } else if(i %% 5 == 0){ print(&quot;Buzz&quot;) } else { print(i) } } If / else statements are extremely useful in functions. In R, the if statement evaluates everything in the parentheses and, if that evaluates to TRUE, runs everything in the braces. This means that you can trigger code in an if statement with a single-value logical vector: weekend &lt;- TRUE if(weekend){ print(&quot;It&#39;s the weekend!&quot;) } ## [1] &quot;It&#39;s the weekend!&quot; This functionality can be useful with parameters you choose to include when writing your own functions (e.g., print = TRUE). 7.3.2 Some other control structures The control structures you are most likely to use in data analysis with R are “for” loops and “if / else” statements. However, there are a few other control structures you may occasionally find useful: next break while You can use the next structure to skip to the next round of a loop when a certain condition is met. For example, we could have used this code to print out odd numbers between 1 and 5: for(i in 1:5){ if(i %% 2 == 0){ next } print(i) } ## [1] 1 ## [1] 3 ## [1] 5 You can use break to break out of a loop if a certain condition is met. For example, the final code will break out of the loop once i is over 3, so it will only print the numbers 1 through 3: for(i in 1:5){ if(i &gt; 3){ break } print(i) } ## [1] 1 ## [1] 2 ## [1] 3 my_sum &lt;- 1 while(my_sum &lt; 10){ my_sum &lt;- my_sum * 2 print(my_sum) } ## [1] 2 ## [1] 4 ## [1] 8 ## [1] 16 7.4 Functions As you move to larger projects, you will find yourself using the same code a lot. Examples include: Reading in data from a specific type of equipment (air pollution monitor, accelerometer) Running a specific type of analysis Creating a specific type of plot or map If you find yourself cutting and pasting a lot, convert the code to a function. Advantages of writing functions include: Coding is more efficient Easier to change your code (if you’ve cut and paste code and you want to change something, you have to change it everywhere - this is an easy way to accidentally create bugs in your code) Easier to share code with others You can name a function anything you want (although try to avoid names of preexisting-existing functions). You then define any inputs (arguments; separate multiple arguments with commas) and put the code to run in braces: ## Note: this code will not run [function name] &lt;- function([any arguments]){ [code to run] } Here is an example of a very basic function. This function takes a number as input and adds 1 to that number. add_one &lt;- function(number){ out &lt;- number + 1 return(out) } add_one(number = 3) ## [1] 4 add_one(number = -1) ## [1] 0 Functions can input any type of R object (for example, vectors, data frames, even other functions and ggplot objects) Similarly, functions can output any type of R object When defining a function, you can set default values for some of the parameters You can explicitly specify the value to return from the function There are ways to check for errors in the arguments a user inputs to the function For example, the following function inputs a data frame (datafr) and a one-element vector (child_id) and returns only rows in the data frame where it’s id column matches child_id. It includes a default value for datafr, but not for child_id. subset_nepali &lt;- function(datafr = nepali, child_id){ datafr &lt;- datafr %&gt;% filter(id == child_id) return(datafr) } If an argument is not given for a parameter with a default, the function will run using the default value for that parameter. For example: subset_nepali(child_id = &quot;120011&quot;) ## id sex wt ht mage lit died alive age ## 1 120011 Male 12.8 91.2 35 0 2 5 41 ## 2 120011 Male 12.8 93.9 35 0 2 5 45 ## 3 120011 Male 13.1 95.2 35 0 2 5 49 ## 4 120011 Male 13.8 96.9 35 0 2 5 53 ## 5 120011 Male NA NA 35 0 2 5 57 If an argument is not given for a parameter without a default, the function call will result in an error. For example: subset_nepali(datafr = nepali) ## Error in eval(expr, envir, enclos): argument 2 is empty By default, the function will return the last defined object, although the choice of using return can affect printing behavior when you run the function. For example, I could have written the subset_nepali function like this: subset_nepali &lt;- function(datafr = nepali, child_id){ datafr &lt;- datafr %&gt;% filter(id == child_id) } In this case, the output will not automatically print out when you call the function without assigning it to an R object: subset_nepali(child_id = &quot;120011&quot;) However, the output can be assigned to an R object in the same way as when the function was defined without return: first_childs_data &lt;- subset_nepali(child_id = &quot;120011&quot;) first_childs_data ## id sex wt ht mage lit died alive age ## 1 120011 Male 12.8 91.2 35 0 2 5 41 ## 2 120011 Male 12.8 93.9 35 0 2 5 45 ## 3 120011 Male 13.1 95.2 35 0 2 5 49 ## 4 120011 Male 13.8 96.9 35 0 2 5 53 ## 5 120011 Male NA NA 35 0 2 5 57 The return function can also be used to return an object other than the last defined object (although doesn’t tend to be something you need to do very often). For example, if you did not use return in the following code, it will output “Test output”: subset_nepali &lt;- function(datafr = nepali, child_id){ datafr &lt;- datafr %&gt;% filter(id == child_id) a &lt;- &quot;Test output&quot; } (subset_nepali(child_id = &quot;120011&quot;)) ## [1] &quot;Test output&quot; Conversely, you can use return to output datafr, even though it’s not the last object defined: subset_nepali &lt;- function(datafr = nepali, child_id){ datafr &lt;- datafr %&gt;% filter(id == child_id) a &lt;- &quot;Test output&quot; return(datafr) } subset_nepali(child_id = &quot;120011&quot;) ## id sex wt ht mage lit died alive age ## 1 120011 Male 12.8 91.2 35 0 2 5 41 ## 2 120011 Male 12.8 93.9 35 0 2 5 45 ## 3 120011 Male 13.1 95.2 35 0 2 5 49 ## 4 120011 Male 13.8 96.9 35 0 2 5 53 ## 5 120011 Male NA NA 35 0 2 5 57 You can use stop to stop execution of the function and give the user an error message. For example, the subset_nepali function will fail if the user inputs a data frame that does not have a column named “id”: subset_nepali(datafr = data.frame(wt = rnorm(10)), child_id = &quot;12011&quot;) Error: comparison (1) is possible only for atomic and list types You can rewrite the function to stop if the input datafr does not have a column named “id”: subset_nepali &lt;- function(datafr = nepali, child_id){ if(!(&quot;id&quot; %in% colnames(datafr))){ stop(&quot;`datafr` must include a column named `id`&quot;) } datafr &lt;- datafr %&gt;% filter(id == child_id) return(datafr) } subset_nepali(datafr = data.frame(wt = rnorm(10)), child_id = &quot;12011&quot;) Error in subset_nepali(datafr = data.frame(wt = rnorm(10)), child_id = &quot;12011&quot;) : `datafr` must include a column named `id` The stop function is particularly important if the function would keep running with the wrong input, but would result in the wrong output. You can also output warnings and messages using the functions warning and message. 7.5 Regular expressions For these examples, we’ll use some data on passengers of the Titanic. You can load this data using: # install.packages(&quot;titanic&quot;) library(titanic) data(&quot;titanic_train&quot;) We will be using the stringr package: library(stringr) This data includes a column called “Name” with passenger names. This column is somewhat messy and includes several elements that we might want to separate (last name, first name, title). Here are the first few values of “Name”: titanic_train %&gt;% select(Name) %&gt;% slice(1:3) ## Name ## 1 Braund, Mr. Owen Harris ## 2 Cumings, Mrs. John Bradley (Florence Briggs Thayer) ## 3 Heikkinen, Miss. Laina We’ve already done some things to manipulate strings. For example, if we wanted to separate “Name” into last name and first name (including title), we could actually do that with the separate function: titanic_train %&gt;% select(Name) %&gt;% slice(1:3) %&gt;% separate(Name, c(&quot;last_name&quot;, &quot;first_name&quot;), sep = &quot;, &quot;) ## last_name first_name ## 1 Braund Mr. Owen Harris ## 2 Cumings Mrs. John Bradley (Florence Briggs Thayer) ## 3 Heikkinen Miss. Laina Notice that separate is looking for a regular pattern (“,”) and then doing something based on the location of that pattern in each string (splitting the string). There are a variety of functions in R that can perform manipulations based on finding regular patterns in character strings. The str_detect function will look through each element of a character vector for a designated pattern. If the pattern is there, it will return TRUE, and otherwise FALSE. The convention is: ## Generic code str_detect(string = [vector you want to check], pattern = [pattern you want to check for]) For example, to create a logical vector specifying which of the Titanic passenger names include “Mrs.”, you can call: mrs &lt;- str_detect(titanic_train$Name, &quot;Mrs.&quot;) head(mrs) ## [1] FALSE TRUE FALSE TRUE FALSE FALSE The result is a logical vector, so str_detect can be used in filter to subset data to only rows where the passenger’s name includes “Mrs.”: titanic_train %&gt;% filter(str_detect(Name, &quot;Mrs.&quot;)) %&gt;% select(Name) %&gt;% slice(1:3) ## Name ## 1 Cumings, Mrs. John Bradley (Florence Briggs Thayer) ## 2 Futrelle, Mrs. Jacques Heath (Lily May Peel) ## 3 Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg) There is an older, base R function called grepl that does something very similar (although note that the order of the arguments is reversed). titanic_train %&gt;% filter(grepl(&quot;Mrs.&quot;, Name)) %&gt;% select(Name) %&gt;% slice(1:3) ## Name ## 1 Cumings, Mrs. John Bradley (Florence Briggs Thayer) ## 2 Futrelle, Mrs. Jacques Heath (Lily May Peel) ## 3 Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg) The str_extract function can be used to extract a string (if it exists) from each value in a character vector. It follows similar conventions to str_detect: ## Generic code str_extract(string = [vector you want to check], pattern = [pattern you want to check for]) For example, you might want to extract “Mrs.” if it exists in a passenger’s name: titanic_train %&gt;% mutate(mrs = str_extract(Name, &quot;Mrs.&quot;)) %&gt;% select(Name, mrs) %&gt;% slice(1:3) ## Name mrs ## 1 Braund, Mr. Owen Harris &lt;NA&gt; ## 2 Cumings, Mrs. John Bradley (Florence Briggs Thayer) Mrs. ## 3 Heikkinen, Miss. Laina &lt;NA&gt; Notice that now we’re creating a new column (mrs) that either has “Mrs.” (if there’s a match) or is missing (NA) if there’s not a match. For this first example, we were looking for an exact string (“Mrs”). However, you can use patterns that match a particular pattern, but not an exact string. For example, we could expand the regular expression to find “Mr.” or “Mrs.”: titanic_train %&gt;% mutate(title = str_extract(Name, &quot;Mr\\\\.|Mrs\\\\.&quot;)) %&gt;% select(Name, title) %&gt;% slice(1:3) ## Name title ## 1 Braund, Mr. Owen Harris Mr. ## 2 Cumings, Mrs. John Bradley (Florence Briggs Thayer) Mrs. ## 3 Heikkinen, Miss. Laina &lt;NA&gt; Note that this pattern uses a special operator (|) to find one pattern or another. Double backslashes (\\\\) escape the special character “.”. As a note, in regular expressions, all of the following characters are special characters that need to be escaped with backslashes if you want to use them literally: . * + ^ ? $ \\ | ( ) [ ] { } Notice that “Mr.” and “Mrs.” both start with “Mr”, end with “.”, and may or may not have an “s” in between. titanic_train %&gt;% mutate(title = str_extract(Name, &quot;Mr(s)*\\\\.&quot;)) %&gt;% select(Name, title) %&gt;% slice(1:3) ## Name title ## 1 Braund, Mr. Owen Harris Mr. ## 2 Cumings, Mrs. John Bradley (Florence Briggs Thayer) Mrs. ## 3 Heikkinen, Miss. Laina &lt;NA&gt; This pattern uses (s)* to match zero or more “s”s at this spot in the pattern. In the previous code, we found “Mr.” and “Mrs.”, but missed “Miss.”. We could tweak the pattern again to try to capture that, as well. For all three, we have the pattern that it starts with “M”, has some lowercase letters, and then ends with “.”. titanic_train %&gt;% mutate(title = str_extract(Name, &quot;M[a-z]+\\\\.&quot;)) %&gt;% select(Name, title) %&gt;% slice(1:3) ## Name title ## 1 Braund, Mr. Owen Harris Mr. ## 2 Cumings, Mrs. John Bradley (Florence Briggs Thayer) Mrs. ## 3 Heikkinen, Miss. Laina Miss. The last pattern used [a-z]+ to match one or more lowercase letters. The [a-z]is a character class. You can also match digits ([0-9]), uppercase letters ([A-Z]), just some letters ([aeiou]), etc. You can negate a character class by starting it with ^. For example, [^0-9] will match anything that isn’t a digit. Sometimes, you want to match a pattern, but then only subset a part of it. For example, each passenger seems to have a title (“Mr.”, “Mrs.”, etc.) that comes after “,” and before “.”. We can use this pattern to find the title, but then we get some extra stuff with the match: titanic_train %&gt;% mutate(title = str_extract(Name, &quot;,\\\\s[A-Za-z]*\\\\.\\\\s&quot;)) %&gt;% select(title) %&gt;% slice(1:3) ## title ## 1 , Mr. ## 2 , Mrs. ## 3 , Miss. As a note, in this pattern, \\\\s is used to match a space. We are getting things like “, Mr.”, when we really want “Mr”. We can use the str_match function to do this. We group what we want to extract from the pattern in parentheses, and then the function returns a matrix. The first column is the full pattern match, and each following column gives just what matches within the groups. head(str_match(titanic_train$Name, pattern = &quot;,\\\\s([A-Za-z]*)\\\\.\\\\s&quot;)) ## [,1] [,2] ## [1,] &quot;, Mr. &quot; &quot;Mr&quot; ## [2,] &quot;, Mrs. &quot; &quot;Mrs&quot; ## [3,] &quot;, Miss. &quot; &quot;Miss&quot; ## [4,] &quot;, Mrs. &quot; &quot;Mrs&quot; ## [5,] &quot;, Mr. &quot; &quot;Mr&quot; ## [6,] &quot;, Mr. &quot; &quot;Mr&quot; To get just the title, then, we can run: titanic_train %&gt;% mutate(title = str_match(Name, &quot;,\\\\s([A-Za-z]*)\\\\.\\\\s&quot;)[ , 2]) %&gt;% select(Name, title) %&gt;% slice(1:3) ## Name title ## 1 Braund, Mr. Owen Harris Mr ## 2 Cumings, Mrs. John Bradley (Florence Briggs Thayer) Mrs ## 3 Heikkinen, Miss. Laina Miss The [ , 2] pulls out just the second column from the matrix returned by str_match. Here are some of the most common titles: titanic_train %&gt;% mutate(title = str_match(Name, &quot;,\\\\s([A-Za-z]*)\\\\.\\\\s&quot;)[ , 2]) %&gt;% group_by(title) %&gt;% summarize(n = n()) %&gt;% arrange(desc(n)) %&gt;% slice(1:5) ## # A tibble: 5 × 2 ## title n ## &lt;chr&gt; &lt;int&gt; ## 1 Mr 517 ## 2 Miss 182 ## 3 Mrs 125 ## 4 Master 40 ## 5 Dr 7 Here are a few other examples of regular expressions in action with this dataset. Get just names that start with (“^”) the letter “A”: titanic_train %&gt;% filter(str_detect(Name, &quot;^A&quot;)) %&gt;% select(Name) %&gt;% slice(1:3) ## Name ## 1 Allen, Mr. William Henry ## 2 Andersson, Mr. Anders Johan ## 3 Asplund, Mrs. Carl Oscar (Selma Augusta Emilia Johansson) Get names with “II” or “III” ({2,} says to match at least two times): titanic_train %&gt;% filter(str_detect(Name, &quot;I{2,}&quot;)) %&gt;% select(Name) %&gt;% slice(1:3) ## Name ## 1 Carter, Master. William Thornton II ## 2 Roebling, Mr. Washington Augustus II Get names with “Andersen” or “Anderson” (alternatives in square brackets): titanic_train %&gt;% filter(str_detect(Name, &quot;Anders[eo]n&quot;)) %&gt;% select(Name) ## Name ## 1 Andersen-Jensen, Miss. Carla Christine Nielsine ## 2 Anderson, Mr. Harry ## 3 Walker, Mr. William Anderson ## 4 Olsvigen, Mr. Thor Anderson ## 5 Soholt, Mr. Peter Andreas Lauritz Andersen Get names that start with (“^” outside of brackets) the letters “A” and “B”: titanic_train %&gt;% filter(str_detect(Name, &quot;^[AB]&quot;)) %&gt;% select(Name) %&gt;% slice(1:3) ## Name ## 1 Braund, Mr. Owen Harris ## 2 Allen, Mr. William Henry ## 3 Bonnell, Miss. Elizabeth Get names that end with (“$”) the letter “b” (either lowercase or uppercase): titanic_train %&gt;% filter(str_detect(Name, &quot;[bB]$&quot;)) %&gt;% select(Name) ## Name ## 1 Emir, Mr. Farred Chehab ## 2 Goldschmidt, Mr. George B ## 3 Cook, Mr. Jacob ## 4 Pasic, Mr. Jakob Some useful regular expression operators include: Operator Meaning . Any character * Match 0 or more times (greedy) *? Match 0 or more times (non-greedy) + Match 1 or more times (greedy) +? Match 1 or more times (non-greedy) ^ Starts with (in brackets, negates) $ Ends with […] Character classes For more on these patterns, see: Help file for the stringi-search-regex function in the stringi package (which should install when you install stringr) Introduction to stringr by Hadley Wickham Handling and Processing Strings in R by Gaston Sanchez (seven chapter ebook) http://gskinner.com/RegExr and http://www.txt2re.com: Interactive tools for helping you build regular expression pattern strings 7.6 In-course exercise 7.6.1 Exploring Fatality Analysis Reporting System (FARS) data Visit http://metrocosm.com/10-years-of-traffic-accidents-mapped.html and explore the interactive visualization created by Max Galka using this public dataset on US fatal motor vehicle accidents. Go to FARS web page to find and download the data for this week’s exercise. We want to get the raw data on fatal accidents. Navigate this page to figure out how you can get this raw data for the whole county for 2015. Download the 2015 data to your computer. What is the structure of how this data is saved (e.g., directory structure, file structure)? On the FARS web page, find the documentation describing this raw data. Look through both this documentation and the raw files you downloaded to figure out what information is included in the data. Read the accident.csv file for 2015 into R (this is one of the files you’ll get if you download the raw data for 2015). Use the documentation to figure out what each column represents. Discuss what steps you would need to take to create the following plot. To start, don’t write any code, just develop a plan. Talk about what the dataset should look like right before you create the plot and what functions you could use to get the data from its current format to that format. (Hint: Functions from the lubridate package will be very helpful, including yday and wday). Discuss which of the variables in this dataset could be used to merge the dataset with other appropriate data, either other datasets in the FARS raw data, or outside datasets. Try to write the code to create the plot below. This will include some code for cleaning the data and some code for plotting. There is an example answer below, but I’d like you to try to figure it out yourselves first. 7.6.1.1 Example R code Here is example code for the section above: library(tidyverse) library(lubridate) library(ggthemes) accident &lt;- read_csv(&quot;data/accident.csv&quot;) %&gt;% select(DAY:MINUTE) %&gt;% select(-DAY_WEEK) %&gt;% unite(date, DAY:MINUTE, sep = &quot;-&quot;, remove = FALSE) %&gt;% mutate(date = dmy_hm(date), yday = yday(date), weekday = wday(date, label = TRUE, abbr = FALSE), weekend = weekday %in% c(&quot;Saturday&quot;, &quot;Sunday&quot;)) accident %&gt;% filter(!is.na(yday)) %&gt;% group_by(yday) %&gt;% summarize(accidents = n(), weekend = first(weekend)) %&gt;% ggplot(aes(x = yday, y = accidents, color = weekend)) + geom_point(alpha = 0.5) + xlab(&quot;Day of the year in 2015&quot;) + ylab(&quot;# of fatal accidents&quot;) + theme_few() + geom_smooth(se = FALSE) 7.6.2 Using a loop to create state-specific plots Next, you will write a loop to write a four-page pdf file with state-specific plots for the states of Colorado, Texas, California, and New York. The data has a column called STATE, but it gives state as a one- or two-digit code, rather than by name. These codes are the state Federal Information Processing Standard (FIPS) codes. A dataset with state names and FIPS codes is available at http://www2.census.gov/geo/docs/reference/state.txt. Read that data into an R object called state_fips and clean it so the first few lines look like this: ## # A tibble: 5 × 2 ## state state_name ## &lt;int&gt; &lt;chr&gt; ## 1 1 Alabama ## 2 2 Alaska ## 3 4 Arizona ## 4 5 Arkansas ## 5 6 California Read the 2015 FARS data into an R object named accident. Use all the date and time information to create a column named date with the date and time of the accident. Include information on whether the accident was related to drunk driving (FALSE if there were 0 drunk drivers, TRUE if there were one or more), and create columns that gives whether the accident was during the day (7 AM to 7 PM) or not as well as the month of the accident (for this last column, you can either retain it from the original data or recalculate it based on the new date variable). Filter out any values where the date-time does not render (i.e., date is a missing value). The first few rows of the cleaned dataframe should look like this: ## # A tibble: 5 × 5 ## state date drunk_dr daytime month ## &lt;int&gt; &lt;dttm&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;dbl&gt; ## 1 1 2015-01-01 02:40:00 TRUE FALSE 1 ## 2 1 2015-01-01 22:13:00 FALSE FALSE 1 ## 3 1 2015-01-01 01:25:00 TRUE FALSE 1 ## 4 1 2015-01-04 00:57:00 TRUE FALSE 1 ## 5 1 2015-01-07 07:09:00 FALSE TRUE 1 Join the information from state_fips into the accident dataframe. There may be a few locations in the state_fips dataframe that are not included in the accident dataframe (e.g., Virgin Islands), so when you join keep all observations in accident but only the observations in state_fips that match at least one row of accident. The first few rows of the joined dataset should look like this: ## # A tibble: 5 × 6 ## state date drunk_dr daytime month state_name ## &lt;int&gt; &lt;dttm&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1 2015-01-01 02:40:00 TRUE FALSE 1 Alabama ## 2 1 2015-01-01 22:13:00 FALSE FALSE 1 Alabama ## 3 1 2015-01-01 01:25:00 TRUE FALSE 1 Alabama ## 4 1 2015-01-04 00:57:00 TRUE FALSE 1 Alabama ## 5 1 2015-01-07 07:09:00 FALSE TRUE 1 Alabama Write the code to create boxplots for Colorado of the distribution of total accidents within each month. Create separate boxplots for daytime and nighttime accidents, and facet by whether the accident was related to drunk driving. The plot should look like the plot below. Write a loop to create a plot like the one you just made for Colorado for four different states – Colorado, Texas, California, and New York. The code should write the plots out to a pdf, with one plot per page of the pdf. (Hint: To get a loop to print out a plot created with ggplot, you must explicitly print the plot object. For example, you could assign the plot to a, and then you would run print(a) within your loop as the last step.) 7.6.2.1 Example R code Here is example R code for creating state-specific plots using a loop: state_fips &lt;- read_delim(&quot;http://www2.census.gov/geo/docs/reference/state.txt&quot;, delim = &quot;|&quot;) %&gt;% rename(state = STATE, state_name = STATE_NAME) %&gt;% select(state, state_name) %&gt;% mutate(state = as.integer(state)) accident &lt;- read_csv(&quot;data/accident.csv&quot;) %&gt;% select(STATE, DAY:MINUTE, DRUNK_DR) %&gt;% rename(state = STATE, drunk_dr = DRUNK_DR) %&gt;% select(-DAY_WEEK) %&gt;% unite(date, DAY:MINUTE, sep = &quot;-&quot;) %&gt;% mutate(date = dmy_hm(date), drunk_dr = drunk_dr &gt;= 1, daytime = hour(date) %in% c(7:19), month = month(date)) %&gt;% filter(!is.na(date)) %&gt;% left_join(state_fips, by = &quot;state&quot;) pdf(&quot;StateFARSPlots.pdf&quot;, height = 3.5, width = 7) state_list &lt;- c(&quot;Colorado&quot;, &quot;Texas&quot;, &quot;California&quot;, &quot;New York&quot;) for(which_state in state_list){ a &lt;-accident %&gt;% filter(state_name == which_state) %&gt;% mutate(drunk_dr = factor(drunk_dr, labels = c(&quot;Unrelated to\\ndrunk driving&quot;, &quot;Related to\\ndrunk driving&quot;)), daytime = factor(daytime, labels = c(&quot;Nighttime&quot;, &quot;Daytime&quot;))) %&gt;% group_by(daytime, month, drunk_dr) %&gt;% summarize(accidents = n()) %&gt;% ggplot(aes(x = daytime, y = accidents, group = daytime)) + geom_boxplot() + facet_wrap(~ drunk_dr, ncol = 2) + xlab(&quot;Time of day&quot;) + ylab(&quot;# of monthly accidents&quot;) + ggtitle(paste(&quot;Fatal accidents in&quot;, which_state, &quot;in 2015&quot;)) print(a) } dev.off() "],
["reporting-data-results-2.html", "Chapter 8 Reporting data results #2 8.1 Matrices and lists 8.2 apply functions 8.3 Point maps 8.4 Choropleth maps 8.5 Google Maps API 8.6 String operations 8.7 In-course exercise", " Chapter 8 Reporting data results #2 Download a pdf of the lecture slides covering this topic. 8.1 Matrices and lists In this section, we’ll talk about the apply family of functions, which allow you to apply a function to all values in a vector, matrix, or list. First, you need to know about two more object types in R: matrix list 8.1.1 Matrices A matrix is like a data frame, but all the values in all columns must be of the same class (e.g., numeric, character). R uses matrices a lot for its underlying math (e.g., for the linear algebra operations required for fitting regression models). R can do matrix operations quite quickly. You can create a matrix with the matrix function. Input a vector with the values to fill the matrix and ncol to set the number of columns: foo &lt;- matrix(1:10, ncol = 5) foo ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 3 5 7 9 ## [2,] 2 4 6 8 10 By default, the matrix will fill up by column. You can fill it by row with the byrow function: foo &lt;- matrix(1:10, ncol = 5, byrow = TRUE) foo ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 2 3 4 5 ## [2,] 6 7 8 9 10 In certain situations, you might want to work with a matrix instead of a data frame (for example, in cases where you were concerned about speed – a matrix is more memory efficient than the corresponding data frame). If you want to convert a data frame to a matrix, you can use the as.matrix function: foo &lt;- data.frame(col_1 = 1:2, col_2 = 3:4, col_3 = 5:6, col_4 = 7:8, col_5 = 9:10) (foo &lt;- as.matrix(foo)) ## col_1 col_2 col_3 col_4 col_5 ## [1,] 1 3 5 7 9 ## [2,] 2 4 6 8 10 You can index matrices with square brackets, just like data frames: foo[1, 1:2] ## col_1 col_2 ## 1 3 You cannot, however, use dplyr functions with matrices: foo %&gt;% filter(col_1 == 1) Error in UseMethod(&quot;filter_&quot;) : no applicable method for &#39;filter_&#39; applied to an object of class &quot;c(&#39;matrix&#39;, &#39;integer&#39;, &#39;numeric&#39;)&quot; All elements in a matrix must have the same class. The matrix will default to make all values the most general class of any of the values, in any column. For example, if we replaced one numeric value with the character “a”, everything would turn into a character: foo[1, 1] &lt;- &quot;a&quot; foo ## col_1 col_2 col_3 col_4 col_5 ## [1,] &quot;a&quot; &quot;3&quot; &quot;5&quot; &quot;7&quot; &quot;9&quot; ## [2,] &quot;2&quot; &quot;4&quot; &quot;6&quot; &quot;8&quot; &quot;10&quot; 8.1.2 Lists A list has different elements, just like a data frame has different columns. However, the different elements of a list can have different lengths (unlike the columns of a data frame). The different elements can also have different classes. bar &lt;- list(some_letters = letters[1:3], some_numbers = 1:5, some_logical_values = c(TRUE, FALSE)) bar ## $some_letters ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; ## ## $some_numbers ## [1] 1 2 3 4 5 ## ## $some_logical_values ## [1] TRUE FALSE To index an element from a list, use double square brackets. You can use bracket indexing either with numbers (which element in the list?) or with names. You can also index lists with the $ operator. bar[[1]] ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; bar[[&quot;some_numbers&quot;]] ## [1] 1 2 3 4 5 bar$some_logical_values ## [1] TRUE FALSE Lists can be used to contain data with an unusual structure and / or lots of different components. For example, the information from fitting a regression is often stored as a list: my_mod &lt;- glm(rnorm(10) ~ c(1:10)) is.list(my_mod) ## [1] TRUE head(names(my_mod), 3) ## [1] &quot;coefficients&quot; &quot;residuals&quot; &quot;fitted.values&quot; my_mod[[&quot;coefficients&quot;]] ## (Intercept) c(1:10) ## -0.61559621 0.07839547 8.2 apply functions There is a whole family of apply functions, as part of base R. These include: apply: Apply a function over all the rows (MARGIN = 1) or columns (MARGIN = 2) of a matrix lapply: Apply a function over elements of a list. sapply: Like lapply, but returns a vector instead of a list. Here is the syntax for apply: ## Generic code apply([matrix], MARGIN = [margin (1: rows, 2: columns)], FUN = [function]) I’ll use the worldcup data as an example: ex &lt;- worldcup[ , c(&quot;Shots&quot;, &quot;Passes&quot;, &quot;Tackles&quot;, &quot;Saves&quot;)] head(ex) ## Shots Passes Tackles Saves ## Abdoun 0 6 0 0 ## Abe 0 101 14 0 ## Abidal 0 91 6 0 ## Abou Diaby 1 111 5 0 ## Aboubakar 2 16 0 0 ## Abreu 0 15 0 0 Take the mean of all columns: apply(ex, MARGIN = 2, mean) ## Shots Passes Tackles Saves ## 2.3042017 84.5210084 4.1915966 0.6672269 Take the sum of all rows: head(apply(ex, MARGIN = 1, sum), 4) ## Abdoun Abe Abidal Abou Diaby ## 6 115 97 117 You can use your own function with any of the apply functions. For example, if you wanted to calculate a value for each player that is a weighted mean of some of their statistics, you could run: weighted_mean &lt;- function(soccer_stats, weights = c(0.40, 0.01, 0.25, 1.5)){ out &lt;- sum(weights * soccer_stats) return(out) } head(apply(ex, MARGIN = 1, weighted_mean), 4) ## Abdoun Abe Abidal Abou Diaby ## 0.06 4.51 2.41 2.76 The lapply() function will apply a function across a list. The different elements of the list do not have to be the same length (unlike a data frame, where the columns all have to have the same length). (ex &lt;- list(a = c(1:5), b = rnorm(3), c = letters[1:4])) ## $a ## [1] 1 2 3 4 5 ## ## $b ## [1] 1.7502493280 0.0005955934 2.0270888720 ## ## $c ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;d&quot; This call will calculate the mean of each function: lapply(ex, FUN = mean) ## $a ## [1] 3 ## ## $b ## [1] 1.259311 ## ## $c ## [1] NA You can include arguments for the function that you specify with FUN, and they’ll be passed to that function. For example, to get the first value of each element, you can run: lapply(ex, FUN = head, n = 1) ## $a ## [1] 1 ## ## $b ## [1] 1.750249 ## ## $c ## [1] &quot;a&quot; The sapply() function also applies a function over a list, but it returns a vector rather than a list: sapply(ex, FUN = head, n = 1) ## a b c ## &quot;1&quot; &quot;1.75024932802119&quot; &quot;a&quot; In practice, I do use apply() some, but I can often find a way to do similar things to other apply family functions using the tools in dplyr. You should know that apply family functions take advantage of the matrix structure in R. This can be one of the fastest ways to run code in R. It is usually a lot faster than doing the same things with loops. However, unless you are working with large data sets, you may not notice a difference, and “tidyverse” functions are usually comparable in speed. I would recommend using whichever method makes the most sense to you until you run into an analysis that takes a noticeable amount of time to run, and then you might want to work a bit more to optimize your code. 8.3 Point maps It is very easy now to create point maps in R based on longitude and latitude values of specific locations. You can use the map_data function from the ggplot2 package to pull data for maps at different levels (“usa”, “state”, “world”, “county”). The maps you pull using map_data are just data to use to plot polygon shapes for areas like states and counties. library(ggplot2) us_map &lt;- map_data(&quot;state&quot;) head(us_map, 3) ## long lat group order region subregion ## 1 -87.46201 30.38968 1 1 alabama &lt;NA&gt; ## 2 -87.48493 30.37249 1 2 alabama &lt;NA&gt; ## 3 -87.52503 30.37249 1 3 alabama &lt;NA&gt; You can add points to these based on latitude and longitude. Mapping uses the long and lat columns from this data for location: north_carolina &lt;- us_map %&gt;% filter(region == &quot;north carolina&quot;) ggplot(north_carolina, aes(x = long, y = lat)) + geom_point() If you try to plot lines, however, you’ll have a problem: carolinas &lt;- us_map %&gt;% filter(str_detect(region, &quot;carolina&quot;)) ggplot(carolinas, aes(x = long, y = lat)) + geom_path() The group column fixes this problem. It will plot a separate path or polygon for each separate group. For mapping, this gives separate groupings for mainland versus islands and for different states: carolinas %&gt;% group_by(group) %&gt;% slice(1) ## Source: local data frame [4 x 6] ## Groups: group [4] ## ## long lat group order region subregion ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 -75.89399 36.55471 38 9549 north carolina knotts ## 2 -78.55824 33.86753 39 9587 north carolina main ## 3 -76.00285 36.55471 40 10321 north carolina spit ## 4 -83.10753 34.99053 47 11441 south carolina &lt;NA&gt; Using group = group avoids the extra lines from the earlier map: ggplot(carolinas, aes(x = long, y = lat, group = group)) + geom_path() To plot filled regions, use geom_polygon with fill = region. Also, the “void” theme is often useful when mapping: ggplot(carolinas, aes(x = long, y = lat, group = group, fill = region)) + geom_polygon(color = &quot;black&quot;) + theme_void() Here is an example of plotting all of the US by state: map_1 &lt;- ggplot(us_map, aes(x = long, y = lat, group = group)) + geom_polygon(fill = &quot;dodgerblue&quot;, color = &quot;white&quot;) + theme_void() map_1 To add points to these maps, you can use geom_point, again using longitude and latitude to define position. Here I’ll use an example of data points related to the story told in last year’s “Serial” podcast. serial &lt;- read.csv(&quot;data/serial_map_data.csv&quot;) head(serial, 3) ## x y Type Name Description ## 1 356 437 cell-site L688 ## 2 740 360 cell-site L698 ## 3 910 340 cell-site L654 David Robinson figured out a way to convert the x and y coordinates in this data to latitude and longitude coordinates. I’m also adding a column for whether of not the point is a cell tower. serial &lt;- serial %&gt;% mutate(long = -76.8854 + 0.00017022 * x, lat = 39.23822 + 1.371014e-04 * y, tower = Type == &quot;cell-site&quot;) serial[c(1:2, (nrow(serial) - 1):nrow(serial)), c(&quot;Type&quot;, &quot;Name&quot;, &quot;long&quot;, &quot;lat&quot;, &quot;tower&quot;)] ## Type Name long lat tower ## 1 cell-site L688 -76.82480 39.29813 TRUE ## 2 cell-site L698 -76.75944 39.28758 TRUE ## 24 base-location Adnan&#39;s house -76.76284 39.30622 FALSE ## 25 base-location Jenn&#39;s house -76.72301 39.29443 FALSE Now I can map just Baltimore City and Baltimore County in Maryland and add these points. I used map_data to pull the “county” map and specified “region” as “maryland”, to limit the map just to Maryland counties. baltimore &lt;- map_data(&#39;county&#39;, region = &#39;maryland&#39;) head(baltimore, 3) ## long lat group order region subregion ## 1 -78.64992 39.53982 1 1 maryland allegany ## 2 -78.70148 39.55128 1 2 maryland allegany ## 3 -78.74159 39.57420 1 3 maryland allegany From that, I subset out rows where the subregion column was “baltimore city” or “baltimore”. baltimore &lt;- subset(baltimore, subregion %in% c(&quot;baltimore city&quot;, &quot;baltimore&quot;)) head(baltimore, 3) ## long lat group order region subregion ## 114 -76.88521 39.35074 3 114 maryland baltimore ## 115 -76.89094 39.37939 3 115 maryland baltimore ## 116 -76.88521 39.40804 3 116 maryland baltimore I used geom_point to plot the points. ggplot uses the group column to group together counties, but we don’t need that in the points, so I needed to set group = NA in the geom_point statement. I put color = tower inside the aes statement so that the points would be one color for cell towers and another color for everything else. balt_plot &lt;- ggplot(baltimore, aes(x = long, y = lat, group = group)) + geom_polygon(fill = &quot;lightblue&quot;, color = &quot;black&quot;) + geom_point(data = serial, aes(x = long, y = lat, group = NA, color = tower)) + theme_bw() balt_plot 8.4 Choropleth maps There’s a fantastic new(-ish) package in R to plot choropleth maps. You could also plot choropleths using ggplot and other mapping functions, but I would strongly recommend this new package if you’re mapping the US. You will need to install and load the choroplethr package in R to use the functions below. # install.packages(&quot;choroplethr&quot;) library(choroplethr) # install.packages(&quot;choroplethrMaps&quot;) library(choroplethrMaps) At the most basic level, you can use this package to plot some data that comes automatically with the package (you’ll just need to load the data using the data function). For example, if you wanted to plot state-by-state populations as of 2012, you could use: data(df_pop_state) map_3 &lt;- state_choropleth(df_pop_state) map_3 You can find out more about the df_pop_state data if you type ?df_pop_state. Notice that, for the data frame, the location is given in a column called region and the population size to plot is in a column called value. head(df_pop_state, 3) ## region value ## 1 alabama 4777326 ## 2 alaska 711139 ## 3 arizona 6410979 You could use this function to create any state-level choropleth you wanted, as long as you could create a data frame with a column for states called region and a column with the value you want to show called value. You can run similar functions at different spatial resolutions (for example, county or zip code): data(df_pop_county) head(df_pop_county, 3) ## region value ## 1 1001 54590 ## 2 1003 183226 ## 3 1005 27469 You can plot choropleths at this level, as well: map_4 &lt;- county_choropleth(df_pop_county) map_4 You can even do this for countries of the world: data(df_pop_country) country_choropleth(df_pop_country) You can zoom into states or counties. For example, to plot population by county in Colorado, you could run: county_choropleth(df_pop_county, state_zoom = &quot;colorado&quot;) You can also use this package to map different tables from the US Census’ American Community Survey. The package includes the choroplethr_acs() function to do this, with an option for which level of map you want (map =, choices are “state”, “country”, and “zip”). If you want to map at the state level, for example, use state_choroplethr_acs() (other options are county level and zip code level). These functions pull recent Census data directly from the US Census using its API, so they require you to get an API key, which you can get here. Once you put in your request, they’ll email you your key. Once they give you your API key, you’ll need to install it on R: library(acs) api.key.install(&#39;[your census api key]&#39;); You can pick from a large number of American Community Survey tables– see here for the list plus ID numbers. If the table has multiple columns, you will be prompted to select which one you want to plot. For example, table B19301 gives per-capita income, so if you wanted to plot that, you could run: county_choropleth_acs(tableId = &quot;B19301&quot;, state_zoom = c(&quot;wyoming&quot;, &quot;colorado&quot;)) 8.5 Google Maps API The ggmap package allows you to use tools from Google Maps directly from R. ## install.packages(&quot;ggmap&quot;) library(ggmap) This package uses the Google Maps API, so you should read their terms of service and make sure you follow them. In particular, you are limited to just a certain number of queries per time. You can use the get_map function to get maps for different locations. You can either use the longitude and latitude of the center point of the map, along with the zoom option to say how much to zoom in (3: continent to 20: building) or you can use a character string to specify a location. If you do the second, get_map will actually use the Google Maps API to geocode the string to a latitude and longitude and then get the map (you can imagine that this is like searching in Google Maps in the search box for a location). beijing &lt;- get_map(&quot;Beijing&quot;, zoom = 12) ggmap(beijing) With this package, you can get maps from the following different sources: Google Maps OpenStreetMap Stamen Maps CloudMade Maps (You may need a separate API key for this) Here are different examples of Beijing using different map sources. (Also, note that I’m using the option extent = &quot;device&quot; to fill up the whole plot are with the map, instead of including axis labels and titles.) beijing_a &lt;- get_map(&quot;Beijing&quot;, zoom = 12, source = &quot;stamen&quot;, maptype = &quot;toner&quot;) a &lt;- ggmap(beijing_a, extent = &quot;device&quot;) beijing_b &lt;- get_map(&quot;Beijing&quot;, zoom = 12, source = &quot;stamen&quot;, maptype = &quot;watercolor&quot;) b &lt;- ggmap(beijing_b, extent = &quot;device&quot;) beijing_c &lt;- get_map(&quot;Beijing&quot;, zoom = 12, source = &quot;google&quot;, maptype = &quot;hybrid&quot;) c &lt;- ggmap(beijing_c, extent = &quot;device&quot;) grid.arrange(a, b, c, nrow = 1) As with the maps from ggplot2, you can add points to these maps: serial_phone &lt;- read.csv(&quot;data/serial_phone_data.csv&quot;) %&gt;% mutate(Cell_Site = substring(Cell_Site, 1, 4), Call_Time = as.POSIXct(Call_Time, format = &quot;%d/%m/%y %H:%M&quot;, tz = &quot;EST&quot;)) %&gt;% left_join(serial, by = c(&quot;Cell_Site&quot; = &quot;Name&quot;)) %&gt;% select(Person_Called, Call_Time, Duration, long, lat) %&gt;% filter(!(Person_Called %in% c(&quot;incoming&quot;, &quot;# + Adnan cell&quot;))) %&gt;% arrange(Call_Time) serial_map &lt;- get_map(c(-76.7, 39.3), zoom = 12, source = &quot;stamen&quot;, maptype = &quot;toner&quot;) serial_map &lt;- ggmap(serial_map, extent = &quot;device&quot;) + geom_point(data = serial_phone, aes(x = long, y = lat), color = &quot;red&quot;, size = 3, alpha = 0.4) + geom_point(data = subset(serial, Type != &quot;cell-site&quot;), aes(x = long, y = lat), color = &quot;darkgoldenrod1&quot;, size = 2) You can also use the Google Maps API, through the geocode function, to get the latitude and longitude of specific locations. Basically, if the string would give you the right location if you typed it in Google Maps, geocode should be able to geocode it. For example, you can get the location of CSU: geocode(&quot;Colorado State University&quot;) ## lon lat ## 1 -105.0807 40.57478 You can also get a location by address: geocode(&quot;1 First St NE, Washington, DC&quot;) ## lon lat ## 1 -77.00465 38.89051 You can get distances, too, using the mapdist function with two locations. This will give you distance and also time. mapdist(&quot;Fort Collins CO&quot;, &quot;1 First St NE, Washington, DC&quot;) %&gt;% select(from, miles, hours) ## from miles hours ## 1 Fort Collins CO 1670.348 24.565 8.6 String operations The str_trim function from the stringr package allows you to trim leading and trailing white space: with_spaces &lt;- c(&quot; a &quot;, &quot; bob&quot;, &quot; gamma&quot;) with_spaces ## [1] &quot; a &quot; &quot; bob&quot; &quot; gamma&quot; str_trim(with_spaces) ## [1] &quot;a&quot; &quot;bob&quot; &quot;gamma&quot; This is rarer, but if you ever want to, you can add leading and/or trailing white space to elements of a character vector with str_pad from the stringr package. There are also functions to change a full character string to uppercase, lowercase, or title case: titanic_train$Name[1] ## [1] &quot;Braund, Mr. Owen Harris&quot; str_to_upper(titanic_train$Name[1]) ## [1] &quot;BRAUND, MR. OWEN HARRIS&quot; str_to_lower(titanic_train$Name[1]) ## [1] &quot;braund, mr. owen harris&quot; str_to_title(str_to_lower(titanic_train$Name[1])) ## [1] &quot;Braund, Mr. Owen Harris&quot; 8.7 In-course exercise This exercise will continue using the Fatality Analysis Reporting System (FARS) data we started using last week. 8.7.1 Writing a function to create state-specific plots Adapt the code that you wrote for to create a loop in last week’s in-course exercise and write a function that will create a state-specific plot when you call it. This function should have inputs of datafr and which_state, where datafr is the data frame with the full data set (accident in this case) and which_state is the name of the state for which you’d like to create the plot. Name your functions make_state_plot. Here are a few examples of running this function: make_state_plot(accident, which_state = &quot;California&quot;) make_state_plot(accident, which_state = &quot;North Carolina&quot;) make_state_plot(accident, which_state = &quot;Illinois&quot;) In general, there is a pattern of more fatal accidents during the daytime being unrelated to drunk driving versus related. The pattern is reversed for nighttime accidents. Use your function to see if you can find any states that don’t follow this pattern. So far, we have not covered error checking in functions. In the function that you just wrote, we are assuming that the user will never give inputs that would cause errors. Look through the code for your function and see what assumptions we’re making about what values a user would input for datafr and which_state. What could go wrong if a user does not comply with these assumptions? The best practice when writing functions, especially for others to use, is to use the package::function syntax to identify which package each function comes from. For example, if you’re using mutate in a function, you should write the function code as dplyr::mutate. Go back through the code for the function you wrote. Every time it uses a function that is not part of base R, change the code to use this package::function syntax. If you have time: Can you combine your function, a loop (or lapply), and regular expressions to create plots for all states that start with “C”? That have “North” in their names? (Hint: One way to get a list of states is from the accident data frame. Once you’ve joined in state names as a column called state_name, you can use: unique(accident$state_name).) 8.7.1.1 Example R code Here is some example R code for writing the function to create state-specific plots: make_state_plot &lt;- function(datafr, which_state){ a &lt;- datafr %&gt;% dplyr::filter(state_name == which_state) %&gt;% dplyr::mutate(drunk_dr = factor(drunk_dr, labels = c(&quot;Unrelated to\\ndrunk driving&quot;, &quot;Related to\\ndrunk driving&quot;)), daytime = factor(daytime, labels = c(&quot;Nighttime&quot;, &quot;Daytime&quot;))) %&gt;% dplyr::group_by(daytime, month, drunk_dr) %&gt;% dplyr::summarize_(accidents = ~ n()) %&gt;% ggplot2::ggplot(ggplot2::aes(x = daytime, y = accidents, group = daytime)) + ggplot2::geom_boxplot() + ggplot2::facet_wrap(~ drunk_dr, ncol = 2) + ggplot2::xlab(&quot;Time of day&quot;) + ggplot2::ylab(&quot;# of monthly accidents&quot;) + ggplot2::ggtitle(paste(&quot;Fatal accidents in&quot;, which_state, &quot;in 2015&quot;)) return(a) } # With package::function notation make_state_plot &lt;- function(datafr, which_state){ a &lt;- datafr %&gt;% dplyr::filter(state_name == which_state) %&gt;% dplyr::mutate(drunk_dr = factor(drunk_dr, labels = c(&quot;Unrelated to\\ndrunk driving&quot;, &quot;Related to\\ndrunk driving&quot;)), daytime = factor(daytime, labels = c(&quot;Nighttime&quot;, &quot;Daytime&quot;))) %&gt;% dplyr::group_by(daytime, month, drunk_dr) %&gt;% dplyr::summarize_(accidents = ~ n()) %&gt;% ggplot2::ggplot(ggplot2::aes(x = daytime, y = accidents, group = daytime)) + ggplot2::geom_boxplot() + ggplot2::facet_wrap(~ drunk_dr, ncol = 2) + ggplot2::xlab(&quot;Time of day&quot;) + ggplot2::ylab(&quot;# of monthly accidents&quot;) + ggplot2::ggtitle(paste(&quot;Fatal accidents in&quot;, which_state, &quot;in 2015&quot;)) return(a) } Example of using this function to create plots for all states that start with “C”: states &lt;- unique(accident$state_name) c_states &lt;- states[str_detect(states, &quot;^C&quot;)] for(state in c_states){ state_plot &lt;- make_state_plot(datafr = accident, which_state = state) print(state_plot) } To use lapply, the state name needs to be the first argument in the function. We could re-write the function to comply and then use lapply: make_state_plot &lt;- function(which_state, datafr){ a &lt;- datafr %&gt;% dplyr::filter(state_name == which_state) %&gt;% dplyr::mutate(drunk_dr = factor(drunk_dr, labels = c(&quot;Unrelated to\\ndrunk driving&quot;, &quot;Related to\\ndrunk driving&quot;)), daytime = factor(daytime, labels = c(&quot;Nighttime&quot;, &quot;Daytime&quot;))) %&gt;% dplyr::group_by(daytime, month, drunk_dr) %&gt;% dplyr::summarize_(accidents = ~ n()) %&gt;% ggplot2::ggplot(ggplot2::aes(x = daytime, y = accidents, group = daytime)) + ggplot2::geom_boxplot() + ggplot2::facet_wrap(~ drunk_dr, ncol = 2) + ggplot2::xlab(&quot;Time of day&quot;) + ggplot2::ylab(&quot;# of monthly accidents&quot;) + ggplot2::ggtitle(paste(&quot;Fatal accidents in&quot;, which_state, &quot;in 2015&quot;)) return(a) } north_states &lt;- states[str_detect(states, &quot;North&quot;)] lapply(north_states, make_state_plot, datafr = accident) 8.7.2 First steps in mapping If you have time, you can try some exercises with mapping. Read in and clean up the FARS data and save it to an R object called accident. The accident data frame should include location (longitude and latitude), state, and whether the accident involved drunk driving. Once you’re done cleaning, the dataset should look like this: state_fips &lt;- read_delim(&quot;http://www2.census.gov/geo/docs/reference/state.txt&quot;, delim = &quot;|&quot;) %&gt;% dplyr::rename(state = STATE, state_name = STATE_NAME) %&gt;% dplyr::select(state, state_name) %&gt;% dplyr::mutate(state = as.integer(state)) accident &lt;- read_csv(&quot;data/accident.csv&quot;) %&gt;% dplyr::select(STATE, LATITUDE, LONGITUD, DRUNK_DR) %&gt;% dplyr::rename(state = STATE, lat = LATITUDE, long = LONGITUD, drunk_dr = DRUNK_DR) %&gt;% dplyr::mutate(drunk_dr = drunk_dr &gt; 0) %&gt;% dplyr::left_join(state_fips, by = &quot;state&quot;) %&gt;% dplyr::select(-state) accident %&gt;% slice(1:5) ## # A tibble: 5 × 4 ## lat long drunk_dr state_name ## &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;chr&gt; ## 1 33.87865 -87.32533 TRUE Alabama ## 2 34.91044 -86.90871 FALSE Alabama ## 3 32.14201 -85.75846 TRUE Alabama ## 4 31.43981 -85.51030 TRUE Alabama ## 5 31.31933 -85.51510 FALSE Alabama Next, create another R object with state-specific summaries. This object should be a data frame called state_summaries that gives the total number of fatal accidents in each state and the percent of all fatal accidents related to drunk driving in each state. The state_summaries data frame should look like this when you are done cleaning: state_summaries &lt;- accident %&gt;% dplyr::group_by(state_name) %&gt;% dplyr::summarize_(n = ~ n(), perc_drunk_dr = ~ 100 * mean(drunk_dr)) # Because `drunk_dr` is # 0 / 1, the mean is the percent of # values where `drunk_dr` == 1. state_summaries %&gt;% slice(1:5) ## # A tibble: 5 × 3 ## state_name n perc_drunk_dr ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Alabama 783 27.45849 ## 2 Alaska 60 40.00000 ## 3 Arizona 810 28.76543 ## 4 Arkansas 472 25.84746 ## 5 California 2925 24.54701 Create the following state-level choropleths of number of fatal accidents in 2015 in each state and of the percent of fatal accidents linked to drunk driving in each state. The quickest way to do this is with the choroplethr package. # With `choroplethr` package library(choroplethr) state_summaries %&gt;% dplyr::mutate(state_name = str_to_lower(state_name)) %&gt;% dplyr::rename(region = state_name, value = n) %&gt;% state_choropleth(title = &quot;Fatal accidents by state, 2015&quot;, legend = &quot;# of fatal accidents&quot;) ## Warning: `panel.margin` is deprecated. Please use `panel.spacing` property ## instead ## Warning: `panel.margin` is deprecated. Please use `panel.spacing` property ## instead ## Warning: `panel.margin` is deprecated. Please use `panel.spacing` property ## instead state_summaries %&gt;% dplyr::mutate(state_name = str_to_lower(state_name)) %&gt;% dplyr::rename(region = state_name, value = perc_drunk_dr) %&gt;% state_choropleth(title = &quot;Percent of fatal accidents linked\\nto drunk driving, 2015&quot;, legend = &quot;% of fatal accidents\\nlinked to drunk driving&quot;) ## Warning: `panel.margin` is deprecated. Please use `panel.spacing` property ## instead ## Warning: `panel.margin` is deprecated. Please use `panel.spacing` property ## instead ## Warning: `panel.margin` is deprecated. Please use `panel.spacing` property ## instead Now, you will be using the latitude and longitude of each accident to plot accident locations within a state. Check if the columns for latitude and longitude need more cleaning (spoiler alert— they do). Filter out any values that could not reasonably be within the 50 U.S. states and D.C. # It turns out that values like 77.7777, 88.8888, and 99.9999 are used as # missing values of latitude (similar for 777.777, etc., for longitude) accident &lt;- accident %&gt;% dplyr::filter(lat &lt; 75 &amp; long &lt; 777) Map the locations of fatal accidents in Texas in 2015 on top of a Google Map base map that shows major roads. An example map is shown below. (Hint: Use the get_map and ggmap functions for this part.) texas &lt;- get_map(location = &quot;texas&quot;, zoom = 6, maptype = &quot;roadmap&quot;) ggmap(texas, extent = &quot;device&quot;) + geom_point(data = dplyr::filter(accident, state_name == &quot;Texas&quot;), aes(x = long, y = lat), alpha = 0.2, size = 0.5, color = &quot;red&quot;) Create a function that will take three inputs: datafr, a data frame like accident with fatal accident data; which_state, a state name; and drunk_dr, a logical value (TRUE / FALSE) specifying whether to create separate maps for accidents that were and were not linked to drunk driving. This function should output a map (or two maps if drunk_dr is set to TRUE) with points for all fatal accidents in the state. Use map_data and create the map yourself, rather than using choroplethr for this question. Examples of calling this function are shown below the function definition. map_fatal_accidents &lt;- function(datafr = accident, which_state, drunk_dr = FALSE){ state_accidents &lt;- datafr %&gt;% dplyr::filter(state_name == which_state) state_map_data &lt;- ggplot2::map_data(&quot;county&quot;, region = which_state) accident_map &lt;- ggplot2::ggplot(state_map_data, ggplot2::aes(x = long, y = lat, group = subregion)) + ggplot2::geom_polygon(color = &quot;gray&quot;, fill = NA) + ggplot2::theme_void() + ggplot2::geom_point(data = state_accidents, ggplot2::aes(x = long, y = lat, group = NULL), alpha = 0.5, size = 0.7 ) if(drunk_dr){ accident_map &lt;- accident_map + ggplot2::facet_wrap(~ drunk_dr, ncol = 2) } return(accident_map) } map_fatal_accidents(accident, which_state = &quot;Colorado&quot;) map_fatal_accidents(accident, which_state = &quot;Texas&quot;) map_fatal_accidents(accident, which_state = &quot;Colorado&quot;, drunk_dr = TRUE) map_fatal_accidents(accident, which_state = &quot;Texas&quot;, drunk_dr = TRUE) "],
["reproducible-research-2.html", "Chapter 9 Reproducible research #2 9.1 R Notebooks 9.2 Templates 9.3 R Projects 9.4 git 9.5 In-course exercise", " Chapter 9 Reproducible research #2 Download a pdf of the lecture slides covering this topic. 9.1 R Notebooks From RStudio’s article on R Notebooks: “An R Notebook is an R Markdown document with chunks that can be executed independently and interactively, with output visible immediately beneath the input.” R Notebooks are a new feature. Right now, if you want to use them, you need to update to RStudio’s Preview version. You can get that here. You can render an R Notebook document to a final, static version (e.g., pdf, Word, HTML) just like an R Markdown file. Therefore, you can use R Notebooks as an alternative to R Markdown, with the ability to try out and change chunks interactively as you write the document. You can open a new R Notebook file by going in RStudio to “File” -&gt; “New File”. In the Preview version of RStudio, there’s an option there for “R Notebook”. As with R Markdown files, when you choose to create a new R Notebook file, RStudio opens a skeleton file with some example text and formatting already in the file. The syntax is very similar to an R Markdown file, but the YAML now specifies: output: html_notebook 9.2 Templates R Markdown templates can be used to change multiple elements of the style of a rendered document. You can think of these as being the document-level analog to the themes we’ve used with ggplot objects. To do this, some kind of style file is applied when rendering document. For HTML documents, Cascading Style Sheets (CSS) (.css) can be used to change the style of different elements. For pdf files, LaTeX package (style) files (.sty) are used. To open a new R Markdown file that uses a template, in RStudio, go to “File” -&gt; “New File” -&gt; “R Markdown” -&gt; “From Template”. Different templates come with different R packages. A couple of templates come with the rmarkdown package, which you likely already have. Many of these templates will only render to pdf. To render a pdf from R Markdown, you need to have a version of TeX installed on your computer. Like R, TeX is open source software. RStudio recommends the following installations by system: For Macs: MacTeX For PCs: MiKTeX Links for installing both can be found at http://www.latex-project.org/ftp.html Current version of TeX: 3.14159265. The tufte package has templates for creating handouts typeset like Edward Tufte’s books. This package includes templates for creating both pdf and HTML documents in this style. The package includes special functions like newthought, special chunk options like fig.fullwidth, and special knitr engines like marginfigure. Special features available in the tufte template include: Margin references Margin figures Side notes Full width figures The rticles package has templates for several journals: Journal of Statistical Software The R Journal Association for Computing Machinery ACS publications (Journal of the American Chemical Society, Environmental Science &amp; Technology) Elsevier publications Some of these templates create a whole directory, with several files besides the .Rmd file. For example, the template for The R Journal includes: The R Markdown file in which you write your article “RJournal.sty”: A LaTeX package (style) file specific to The R Journal. This file tells LaTeX how to render all the elements in your article in the style desired by this journal. “RJreferences.bib”: A BibTeX file, where you can save citation information for all references in your article. “Rlogo.png”: An example figure (the R logo). Once you render the R Markdown document from this template, you’ll end up with some new files in the directory: “[your file name].tex”: A TeX file with the content from your R Markdown file. This will be “wrapped” with some additional formatting input to create “RJwrapper.tex”. “RJwrapper.tex”: A TeX file that includes both the content from your R Markdown file and additional formatting input. Typically, you will submit this file (along with the BibTeX, any figure and image files, and possibly the style file) to the journal. “RJwrapper.pdf”: The rendered pdf file (what the published article would look like) This template files will often require some syntax that looks more like LaTeX than Markdown. For example, for the template for The R Journal, you need to use \\citep{} and \\citet{} to include citations. These details will depend on the style file of the template. As a note, you can always use raw LaTeX in R Markdown documents, not just in documents you’re creating with a template. You just need to be careful not to mix the two. For example, if you use a LaTeX environment to begin an itemized list (e.g., with begin{itemize}), you must start each item with item, not -. You can create your own template. You create it as part of a custom R package, and then will have access to the template once you’ve installed the package. This can be useful if you often write documents in a certain style, or if you ever work somewhere with certain formatting requirements for reports. RStudio has full instructions for creating your own template: http://rmarkdown.rstudio.com/developer_document_templates.html 9.3 R Projects 9.3.1 Organization So far, you have run much of your analysis within a single R script or R Markdown file. Often, any associated data are within the same working directory as your script or R Markdown file, but the files for one project are not separated from files for other projects. As you move to larger projects, this kind of set-up won’t work as well. Instead, you’ll want to start keeping all materials for a project in a single and exclusive directory. Often, it helps to organize the files in a project directory into subdirectories. Common subdirectories include: data-raw: Raw data and R scripts to clean the raw data. data: Cleaned data, often saved as .RData after being generated by a script in data-raw. R: Code for any functions used in analysis. reports: Any final products rendered from R Markdown and their original R Markdown files (e.g., paper drafts, reports, presentations). 9.3.2 Creating R Projects RStudio allows you to create “Projects” to organize code, data, and results within a directory. When you create a project, RStudio adds a file with the extension “.Rproj” to the directory. There are some advantages to setting a directory to be an R Project. The project: Automatically uses the directory as your current working directory when you open the project. Coordinates well with git version control and GitHub repository system. Opens a “Files” window for navigating project files in an RStudio pane when you open the project. You can create a new project from scratch or from an existing directory. To create an R project from a working directory, in RStudio go to “File” -&gt; “New Project” -&gt; “New Directory”. You can then choose where you want to save the new project directory. 9.4 git Git is a version control system. It saves information about all changes you make on all files in a repository. This allows you to revert back to previous versions and search through the history for all files in the repository. Git is open source. You can download it for different operating systems here: https://git-scm.com/downloads You will need git on your computer to use git with RStudio and create local git repositories you can sync with GitHub repositories. Before you use git, you should configure it. For example, you should make sure it has your name and email address. You can configure git with commands at the shell. For example, I would run the following code at a shell to configure git to have my proper user name and email: git config --global user.name &quot;Brooke Anderson&quot; git config --global user.email &quot;brooke.anderson@colostate.edu&quot; Sometimes, RStudio will automatically find git (once you’ve installed git) when you start RStudio. However, in some cases, you may need to take some more steps to activate git in RStudio. To do this, go to “RStudio” -&gt; “Preferences” -&gt; “Git/SVN”. Choose “Enable version control”. If RStudio doesn’t find your version of git in the “Git executable” box, browse for it. 9.4.1 Initializing a git repository You can initialize a git repository using commands from the shell. To do that, take the following steps (first check that it is not already a git repository): Use a shell (“Terminal” on Macs) to navigate to to that directory. You can use cd to do that (similar to setwd in R). Once you are in the directory, run git status. If you get the message fatal: Not a git repository (or any of the parent directories): .git, it is not yet a git repository. If you do not get an error from git status, the directory is already a repository. If you do get an error, run git init to initialize it as a repository. For example, if I wanted to make the “fars_analysis” directory, which is a direct subdirectory of my home directory, a git repository, I could open a shell and run: cd ~/fars_analysis git init You can also initialize a git repository for a directory that is an R Project directory through R Studio. Open the Project. Go to “Tools” -&gt; “Version Control” -&gt; “Project Setup”. In the box for “Version control system”, choose “Git”. Note: If you have just installed git, and have not restarted RStudio, you’ll need to do that before RStudio will recognize git. If you do not see “Git” in the box for “Version control system”, it means either that you do not have git installed on your computer or that RStudio was unable to find it. Once you initialize the project as a git repository, you should have a “Git” window in one of your RStudio panes (top right pane by default). As you make and save changes to files, they will show up in this window for you to commit. For example, this is what the Git window for our coursebook looks like when I have changes to the slides for week 9 that I need to commit: 9.4.2 Committing When you want git to record changes, you commit the files with the changes. Each time you commit, you have to include a short commit message with some information about the changes. You can make commits from a shell. However, in this course we’ll just make commits from the RStudio environment. To make a commit from RStudio, click on the “Commit” button in the Git window. That will open a separate commit window that looks like this: In this window, to commit changes: Click on the files you want to commit to select them. If you’d like, you can use the bottom part of the window to look through the changes you are committing in each file. Write a message in the “Commit message” box. Keep the message to one line in this box if you can. If you need to explain more, write a short one-line message, skip a line, and then write a longer explanation. Click on the “Commit” button on the right. Once you commit changes to files, they will disappear from the Git window until you make and save more changes in them. 9.4.3 Browsing history On the top left of the Commit window, you can toggle to “History”. This window allows you to explore the history of commits for the repository. GitHub allows you to host git repositories online. This allows you to: Work collaboratively on a shared repository Fork someone else’s repository to create your own copy that you can use and change as you want Suggest changes to other people’s repositories through pull requests To push local repositories to GitHub and fork other people’s repositories, you will need a GitHub account. You can sign up at https://github.com. A free account is fine. The basic unit for working in GitHub is the repository. You can think of a repository as very similar to an R Project— it’s a directory of files with some supplemental files saving some additional information about the directory. While R Projects have this additional information saved as an “.RProj” file, git repositories have this information in a directory called “.git”. Because this pathname starts with a dot, it won’t show up in many of the ways you list files in a directory. From a shell, you can see files that start with . by running ls -a from within that directory. 9.4.4 Linking local repo to GitHub repo If you have a local directory that you would like to push to GitHub, these are the steps to do it. First, you need to make sure that the directory is under git version control. See the notes on initializing a repository. Next, you need to create an empty repository on GitHub to sync with your local repository. Do that by: In GitHub, click on the “+” in the upper right corner (“Create new”). Choose “Create new repository”. Give your repository the same name as the local directory you’d like to connect it to. For example, if you want to connect it to a directory called “fars_analysis” on your computer, name the repository “fars_analysis”. Leave everything else as-is (unless you’d like to add a short description in the “Description” box). Click on “Create repository” at the bottom of the page. Now you are ready to connect the two repositories. First, you’ll want to change some settings in RStudio so GitHub will recognize that your local repository belongs to you, rather than asking for you password every time. In RStudio, go to “RStudio” -&gt; “Preferences” -&gt; “Git / svn”. Choose to “Create RSA key”. Click on “View public key”. Copy what shows up. Go to your GitHub account and navigate to “Settings”. Click on “SSH and GPG keys”. Click on “New SSH key”. Name the key something like “RStudio” (you might want to include the device name if you’ll have SSH keys from RStudio on several computers). Past in your public key in the “Key box”. 9.4.5 Syncing RStudio and GitHub Now you’re ready to push your local repository to the empty GitHub repository you created. Open a shell and navigate to the directory you want to push. (You can open a shell from RStudio using the gear button in the Git window.) Add the GitHub repository as a remote branch with the following command (this gives an example for adding a GitHub repository named “ex_repo” in my GitHub account, “geanders”): git remote add origin git@github.com:geanders/ex_repo.git Push the contents of the local repository to the GitHub repository. git push -u origin master To pull a repository that already exists on GitHub and to which you have access (or that you’ve forked), first use cd to change a shell into the directory where you want to put the repository then run git clone to clone the repository locally. For example, if I wanted to clone a GitHub repository called “ex_repo” in my GitHub account, I would run: git clone git@github.com:geanders/ex_repo.git Once you have linked a local R project with a GitHub repository, you can push and pull commits using the blue down arrow (pull from GitHub) and green up arrow (push to GitHub) in the Git window in RStudio. GitHub helps you work with others on code. There are two main ways you can do this: Collaborating: Different people have the ability to push and pull directly to and from the same repository. When one person pushes a change to the repository, other collaborators can immediately get the changes by pulling the latest GitHub commits to their local repository. Forking: Different people have their own GitHub repositories, with each linked to their own local repository. When a person pushes changes to GitHub, it only makes changes to his own repository. The person must issue a pull request to another person’s fork of the repository to share the changes. 9.4.6 Issues Each original GitHub repository (i.e., not a fork of another repository) has a tab for “Issues”. This page works like a Discussion Forum. You can create new “Issue” threads to describe and discuss things that you want to change about the repository. Issues can be closed once the problem has been resolved. You can close issues on the “Issue” page with the “Close issue” button. If a commit you make in RStudio closes an issue, you can automatically close the issue on GitHub by including “Close #[issue number]” in your commit message and then pushing to GitHub. For example, if issue #5 is “Fix typo in section 3”, and you make a change to fix that typo, you could make and save the change locally, commit that change with the commit message “Close #5”, and then push to GitHub, and issue #5 in “Issues” for that GitHub repository will automatically be closed, with a link to the commit that fixed the issue. 9.4.7 Pull request You can use a pull request to suggest changes to a repository that you do not own or otherwise have the permission to directly change. You can also use pull requests within your own repositories. Some people will create a pull request every time they have a big issue they want to fix in one of their repositories. In GitHub, each repository has a “Pull requests” tab where you can manage pull requests (submit a pull request to another fork or merge in someone else’s pull request for your fork). Take the following steps to suggest changes to someone else’s repository: Fork the repository Make changes (locally or on GitHub) Save your changes and commit them Submit a pull request to the original repository If there are not any conflicts and the owner of the original repository likes your changes, he or she can merge them directly into the original repository. If there are conflicts, these need to be resolved before the pull request can be merged. 9.4.8 Merge conflicts At some point, you will get merge conflicts. These happen when two people have changed the same piece of code in two different ways at the same time. For example, say Rachel and are both working on local versions of the same repository, and I change a line to mtcars[1, ] while Rachel changes the same line to head(mtcars, 1). Rachel pushes to the GitHub version of the repository before I do. When I pull the latest commits to the GitHub repository, I will have a merge conflict for this line. To be able to commit a final version, I’ll need to decide which version of the code to use and commit a version of the file with that code. Merge conflicts can come up in a few situations: You pull in commits from the GitHub branch of a repository you’ve been working on locally. Someone sends a pull request for one of your repositories. If there are merge conflicts, they’ll show up like this in the file: &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD mtcars[1, ] ======= head(mtcars, 1) &gt;&gt;&gt;&gt;&gt;&gt;&gt; remote-branch To fix them, search for all these spots in files with conflicts, pick the code you want to use, and delete everything else. For the example conflict, I might change the file from this: &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD mtcars[1, ] ======= head(mtcars, 1) &gt;&gt;&gt;&gt;&gt;&gt;&gt; remote-branch To this: head(mtcars, 1) Then you can save and commit the file. 9.4.9 Find out more If you’d like to find out more, Hadley Wickham has a great chapter on using git and GitHub with RStudio in his R Packages book: http://r-pkgs.had.co.nz/git.html 9.5 In-course exercise 9.5.1 Trying out templates Open a “Tufte” template document. To open a new document from a template, in RStudio go to “File” -&gt; “New File” -&gt; “R Markdown” -&gt; “From Template”. If “Tufte Handout” does not show up as a choice, install and load the tufte package, restart R, and try again. Render the document to an HTML and open it in a browser. Explore the rendered document. What elements does it include that are not typical for R Markdown documents? Now go and look at the .Rmd file. Find the spots where these new elements are specified in the R Markdown file. How are elements like margin plots and all caps at the starts of some paragraphs included in the document? Download and install the rticles package. Create a new document using the template for The R Journal. When you do that, a whole directory will be copied to your current working directory. Check out the files in that directory. If you have a version of TeX installed on your computer, try compiling the example file. Select three or four key journals in your field. Go to the journal’s website and see if you can figure out: Do they allow authors to submit “.tex” files? If so, do they provide a LaTeX style file for their journal? 9.5.2 Organizing a project In this part of the group exercise, you will set up an R Project to use for the next homework assignment. You will want to set up a similar project for your group project. First, you need to create a new project. In RStudio, go to “File” -&gt; “New Project” -&gt; “New Directory”. Choose where you want to save this directory and what you want to name it. If the R Project does not automatically open once you make it, navigate to the “.Rproj” file created for the project and double-click on that. Alternatively, you can go to “File” -&gt; “Open Project”. Use getwd once you’ve opened your project to determine which working directory is automatically used for the project. Once you open the project, one of the RStudio panes should have a tab called “Files”. This shows the files in this project directory and allows you to navigate through them. Currently, you won’t have any files other than the R project file (“.Rproj”). As a next step, create several subdirectories. We’ll use these to structure the code, data, and R Markdown files for your homework. Create the following subdirectories (you can use the “New Folder” button in the RStudio “Files” pane): data-raw data R writing The data-raw directory will ultimately have your raw data as well as some R scripts with code for cleaning up the raw data. The homework requires you to pull FARS data from several years. Create a subdirectory in data-raw that will just have that data. In the “Files” pane in RStudio, navigate into the data-raw subdirectory. Use the “New Folder” button to create a new subdirectory within the data-raw subdirectory. Name it yearly_person_data. Download FARS data from the years 1999 to 2010. From each year, pull out the “person” file. Save these yearly “person” files in the yearly_person_data subdirectory you created. As a file name, use “person_” and then the year. For example, if you are saving this file for 1999 in the form of a csv, you would name the file “person_1999.csv”. The writing subdirectory will have your R Markdown file and its output. Create a new R Markdown file (“File” -&gt; “New File” -&gt; “R Markdown”) and save it to this subdirectory. You can change the name and date for the file if you’d like. Delete all the text that comes as a default. Write a piece of code that lists the files you saved in data-raw/yearly_person_data. Remember that the working directory for an R Markdown file is the directory in which it’s saved, so you’ll need to use a relative pathname that goes up one directory (..) and then goes into data-raw and yearly_person_data. If you have time, go to the FARS documentation and find out more about which variables are included in this data set and which values they can have. 9.5.3 Linking your project with a GitHub repository If you do not already have one, sign up for a GitHub account. The free option is fine. If you do not already have git installed on your computer, install it: https://git-scm.com/downloads Open a shell and configure git. For example, I would open a shell and run: git config --global user.name &quot;Brooke Anderson&quot; git config --global user.email &quot;brooke.anderson@colostate.edu&quot; Restart RStudio. go to “RStudio” -&gt; “Preferences” -&gt;“Git/SVN”. Choose “Enable version control”. If RStudio doesn’t find yourversion of git in the “Git executable” box, browse for it. Open your FARS analysis project in RStudio. Change your Project settings to initialize git for this project (see the course notes for tips on how to do that). Try changing a few things in files you have in that project directory (or add a new file). Commit those changes using the “Commit” window. After you commit the changes, look at the “History” window to see the history of your commits. Create an empty GitHub repository for the project. Give it the same name as the name of your R project directory. Create an RSA key in RStudio and add it as an SSH key in your GitHub settings. Add this empty GitHub repository as the remote branch of your local git repository for the project. Push your local repository to this GitHub repository. Go to your GitHub account and make sure the repository was pushed. Try making some more changes to your local repository. Commit the changes, then use the green up arrow in the Git window to push the changes to your GitHub repository. "],
["entering-and-cleaning-data-3.html", "Chapter 10 Entering and cleaning data #3 10.1 Cleaning very messy data 10.2 Pulling online data 10.3 Example R API wrappers 10.4 Parsing webpages", " Chapter 10 Entering and cleaning data #3 Download a pdf of the lecture slides covering this topic. 10.1 Cleaning very messy data 10.1.1 Example of messy data One version of Atlantic basin hurricane tracks is available here: http://www.nhc.noaa.gov/data/hurdat/hurdat2-1851-2015-070616.txt. The data is not in a classic delimited format: This data is formatted in the following way: Data for many storms are included in one file. Data for a storm starts with a shorter line, with values for the storm ID, name, and number of observations for the storm. These values are comma separated. Observations for each storm are longer lines. There are multiple observations for each storm, where each observation gives values like the location and maximum winds for the storm at that time. 10.1.2 Strategy for messy data Because the hurricane tracking data is not nicely formatted, you can’t use read_csv or similar functions to read it in. Here is a strategy for reading in very messy data: Read in all lines individually. Use regular expressions to split each line into the elements you’d like to use to fill columns. Write functions, loops, or apply calls to process lines and use the contents to fill a data frame. Once you have the data in a data frame, do any remaining cleaning to create a data frame that is easy to use to answer research questions. 10.1.3 readLines function The readLines function allows you to read a text file in one line at a time. You can then write code and functions to parse the file one line at a time, to turn it into a dataframe you can use. The readLines function will read in lines from a text file directly, without trying to separate into columns. You can use the n argument to specify the number of lines to read it. For example, to read in three lines from the hurricane tracking data, you can run: tracks_url &lt;- paste0(&quot;http://www.nhc.noaa.gov/data/hurdat/&quot;, &quot;hurdat2-1851-2015-070616.txt&quot;) hurr_tracks &lt;- readLines(tracks_url, n = 3) hurr_tracks ## [1] &quot;AL011851, UNNAMED, 14,&quot; ## [2] &quot;18510625, 0000, , HU, 28.0N, 94.8W, 80, -999, -999, -999, -999, -999, -999, -999, -999, -999, -999, -999, -999, -999,&quot; ## [3] &quot;18510625, 0600, , HU, 28.0N, 95.4W, 80, -999, -999, -999, -999, -999, -999, -999, -999, -999, -999, -999, -999, -999,&quot; The data has been read in as a vector, rather than a dataframe: class(hurr_tracks) ## [1] &quot;character&quot; length(hurr_tracks) ## [1] 3 hurr_tracks[1] ## [1] &quot;AL011851, UNNAMED, 14,&quot; 10.1.4 Regular expressions for cleaning You can use regular expressions to break each line up. For example, you can use str_split from the stringr package to break the first line of the hurricane track data into its three separate components: library(stringr) str_split(hurr_tracks[1], pattern = &quot;,&quot;) ## [[1]] ## [1] &quot;AL011851&quot; &quot; UNNAMED&quot; &quot; 14&quot; ## [4] &quot;&quot; You can use this to create a list where each element of the list has the split-up version of a line of the original data. First, read in all of the data: tracks_url &lt;- paste0(&quot;http://www.nhc.noaa.gov/data/hurdat/&quot;, &quot;hurdat2-1851-2015-070616.txt&quot;) hurr_tracks &lt;- readLines(tracks_url) length(hurr_tracks) ## [1] 50919 10.1.5 Working with lists Next, use lapply with str_split to split each line of the data at the commas: hurr_tracks &lt;- lapply(hurr_tracks, str_split, pattern = &quot;,&quot;, simplify = TRUE) hurr_tracks[[1]] ## [,1] [,2] [,3] [,4] ## [1,] &quot;AL011851&quot; &quot; UNNAMED&quot; &quot; 14&quot; &quot;&quot; hurr_tracks[[2]][1:2] ## [1] &quot;18510625&quot; &quot; 0000&quot; Next, you want to split this list into two lists, one with the shorter “meta-data” lines and one with the longer “observation” lines. You can use sapply to create a vector with the length of each line. You will later use this to identify which lines are short or long. hurr_lengths &lt;- sapply(hurr_tracks, length) hurr_lengths[1:17] ## [1] 4 21 21 21 21 21 21 21 21 21 21 21 21 21 21 4 21 unique(hurr_lengths) ## [1] 4 21 You can use bracket indexing to split the hurr_tracks into two lists: one with the shorter lines that start each observation (hurr_meta) and one with the storm observations (hurr_obs). Use bracket indexing with the hurr_lengths vector you just created to make that split. hurr_meta &lt;- hurr_tracks[hurr_lengths == 4] hurr_obs &lt;- hurr_tracks[hurr_lengths == 21] hurr_meta[1:3] ## [[1]] ## [,1] [,2] [,3] [,4] ## [1,] &quot;AL011851&quot; &quot; UNNAMED&quot; &quot; 14&quot; &quot;&quot; ## ## [[2]] ## [,1] [,2] [,3] [,4] ## [1,] &quot;AL021851&quot; &quot; UNNAMED&quot; &quot; 1&quot; &quot;&quot; ## ## [[3]] ## [,1] [,2] [,3] [,4] ## [1,] &quot;AL031851&quot; &quot; UNNAMED&quot; &quot; 1&quot; &quot;&quot; hurr_obs[1:2] ## [[1]] ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] ## [1,] &quot;18510625&quot; &quot; 0000&quot; &quot; &quot; &quot; HU&quot; &quot; 28.0N&quot; &quot; 94.8W&quot; &quot; 80&quot; &quot; -999&quot; ## [,9] [,10] [,11] [,12] [,13] [,14] [,15] [,16] ## [1,] &quot; -999&quot; &quot; -999&quot; &quot; -999&quot; &quot; -999&quot; &quot; -999&quot; &quot; -999&quot; &quot; -999&quot; &quot; -999&quot; ## [,17] [,18] [,19] [,20] [,21] ## [1,] &quot; -999&quot; &quot; -999&quot; &quot; -999&quot; &quot; -999&quot; &quot;&quot; ## ## [[2]] ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] ## [1,] &quot;18510625&quot; &quot; 0600&quot; &quot; &quot; &quot; HU&quot; &quot; 28.0N&quot; &quot; 95.4W&quot; &quot; 80&quot; &quot; -999&quot; ## [,9] [,10] [,11] [,12] [,13] [,14] [,15] [,16] ## [1,] &quot; -999&quot; &quot; -999&quot; &quot; -999&quot; &quot; -999&quot; &quot; -999&quot; &quot; -999&quot; &quot; -999&quot; &quot; -999&quot; ## [,17] [,18] [,19] [,20] [,21] ## [1,] &quot; -999&quot; &quot; -999&quot; &quot; -999&quot; &quot; -999&quot; &quot;&quot; 10.1.6 Converting to dataframes Now, you can use bind_rows from dplyr to change the list of metadata into a dataframe. (You first need to use as_tibble with lapply to convert all elements of the list from matrices to dataframes.) library(dplyr) hurr_meta &lt;- lapply(hurr_meta, tibble::as_tibble) hurr_meta &lt;- bind_rows(hurr_meta) hurr_meta %&gt;% slice(1:3) ## # A tibble: 3 × 4 ## V1 V2 V3 V4 ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 AL011851 UNNAMED 14 ## 2 AL021851 UNNAMED 1 ## 3 AL031851 UNNAMED 1 You can clean up the data a bit more. First, the fourth column doesn’t have any non-missing values, so you can get rid of it: unique(hurr_meta$V4) ## [1] &quot;&quot; Second, the second and third columns include a lot of leading whitespace: hurr_meta$V2[1:2] ## [1] &quot; UNNAMED&quot; &quot; UNNAMED&quot; Last, we want to name the columns. hurr_meta &lt;- hurr_meta %&gt;% dplyr::select(-V4) %&gt;% dplyr::rename(storm_id = V1, storm_name = V2, n_obs = V3) %&gt;% dplyr::mutate(storm_name = str_trim(storm_name), n_obs = as.numeric(n_obs)) hurr_meta %&gt;% slice(1:3) ## # A tibble: 3 × 3 ## storm_id storm_name n_obs ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 AL011851 UNNAMED 14 ## 2 AL021851 UNNAMED 1 ## 3 AL031851 UNNAMED 1 Now you can apply the same idea to the hurricane observations. First, we’ll want to add storm identifiers to that data. The “meta” data includes storm ids and the number of observations per storm. We can take advantage of that to make a storm_id vector that will line up with the storm observations. storm_id &lt;- rep(hurr_meta$storm_id, times = hurr_meta$n_obs) head(storm_id, 3) ## [1] &quot;AL011851&quot; &quot;AL011851&quot; &quot;AL011851&quot; length(storm_id) ## [1] 49105 length(hurr_obs) ## [1] 49105 hurr_obs &lt;- lapply(hurr_obs, tibble::as_tibble) hurr_obs &lt;- dplyr::bind_rows(hurr_obs) %&gt;% dplyr::mutate(storm_id = storm_id) hurr_obs %&gt;% dplyr::select(V1, V2, V5, V6, storm_id) %&gt;% slice(1:3) ## # A tibble: 3 × 5 ## V1 V2 V5 V6 storm_id ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 18510625 0000 28.0N 94.8W AL011851 ## 2 18510625 0600 28.0N 95.4W AL011851 ## 3 18510625 1200 28.0N 96.0W AL011851 To finish, you just need to clean up the data. Now that the data is in a dataframe, this process is inline with what you’ve been doing with dplyr and related packages. The “README” file for the hurricane tracking data is useful at this point: http://www.nhc.noaa.gov/data/hurdat/hurdat2-format-atlantic.pdf First, say you only want some of the columns for a study you are doing. You can use select to clean up the dataframe by limiting it to columns you need. If you only need date, time, storm status, location (latitude and longitude), maximum sustained winds, and minimum pressure, then you can run: hurr_obs &lt;- hurr_obs %&gt;% dplyr::select(V1, V2, V4:V8, storm_id) %&gt;% dplyr::rename(date = V1, time = V2, status = V4, latitude = V5, longitude = V6, wind = V7, pressure = V8) hurr_obs %&gt;% slice(1:3) %&gt;% dplyr::select(date, time, status, latitude, longitude) ## # A tibble: 3 × 5 ## date time status latitude longitude ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 18510625 0000 HU 28.0N 94.8W ## 2 18510625 0600 HU 28.0N 95.4W ## 3 18510625 1200 HU 28.0N 96.0W Next, the first two columns give the date and time. You can unite these and then convert them to a Date-time class. library(tidyr) library(lubridate) hurr_obs &lt;- hurr_obs %&gt;% tidyr::unite(date_time, date, time) %&gt;% dplyr::mutate(date_time = ymd_hm(date_time)) hurr_obs %&gt;% slice(1:3) %&gt;% dplyr::select(date_time, status, latitude, longitude) ## # A tibble: 3 × 4 ## date_time status latitude longitude ## &lt;dttm&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1851-06-25 00:00:00 HU 28.0N 94.8W ## 2 1851-06-25 06:00:00 HU 28.0N 95.4W ## 3 1851-06-25 12:00:00 HU 28.0N 96.0W Next, you can change status to a factor and give the levels more meaningful names: unique(hurr_obs$status) ## [1] &quot; HU&quot; &quot; TS&quot; &quot; EX&quot; &quot; TD&quot; &quot; LO&quot; &quot; DB&quot; &quot; SD&quot; &quot; SS&quot; &quot; WV&quot; storm_levels &lt;- c(&quot;TD&quot;, &quot;TS&quot;, &quot;HU&quot;, &quot;EX&quot;, &quot;SD&quot;, &quot;SS&quot;, &quot;LO&quot;, &quot;WV&quot;, &quot;DB&quot;) storm_labels &lt;- c(&quot;Tropical depression&quot;, &quot;Tropical storm&quot;, &quot;Hurricane&quot;, &quot;Extratropical cyclone&quot;, &quot;Subtropical depression&quot;, &quot;Subtropical storm&quot;, &quot;Other low&quot;, &quot;Tropical wave&quot;, &quot;Disturbance&quot;) hurr_obs &lt;- hurr_obs %&gt;% dplyr::mutate(status = factor(str_trim(status), levels = storm_levels, labels = storm_labels)) Now, you can clean up the latitude and longitude. Ultimately, we’ll want numeric values for those so we can use them for mapping. You can use regular expressions to separate the numeric and non-numeric parts of these columns. For example: head(str_extract(hurr_obs$latitude, &quot;[A-Z]&quot;)) ## [1] &quot;N&quot; &quot;N&quot; &quot;N&quot; &quot;N&quot; &quot;N&quot; &quot;N&quot; head(str_extract(hurr_obs$latitude, &quot;[^A-Z]+&quot;)) ## [1] &quot; 28.0&quot; &quot; 28.0&quot; &quot; 28.0&quot; &quot; 28.1&quot; &quot; 28.2&quot; &quot; 28.2&quot; Use this idea to split the numeric latitude from the direction of that latitude: hurr_obs &lt;- hurr_obs %&gt;% dplyr::mutate(lat_dir = str_extract(latitude, &quot;[A-Z]&quot;), latitude = as.numeric(str_extract(latitude, &quot;[^A-Z]+&quot;)), lon_dir = str_extract(longitude, &quot;[A-Z]&quot;), longitude = as.numeric(str_extract(longitude, &quot;[^A-Z]+&quot;))) Now these elements are in separate columns: hurr_obs %&gt;% dplyr::select(latitude, lat_dir, longitude, lon_dir) %&gt;% dplyr::slice(1:2) ## # A tibble: 2 × 4 ## latitude lat_dir longitude lon_dir ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 28 N 94.8 W ## 2 28 N 95.4 W unique(hurr_obs$lat_dir) ## [1] &quot;N&quot; unique(hurr_obs$lon_dir) ## [1] &quot;W&quot; &quot;E&quot; If we’re looking at US impacts, we probably only need observations from the western hemisphere, so let’s filter out other values: hurr_obs &lt;- hurr_obs %&gt;% dplyr::filter(lon_dir == &quot;W&quot;) Next, clean up the wind column: unique(hurr_obs$wind)[1:5] ## [1] &quot; 80&quot; &quot; 70&quot; &quot; 60&quot; &quot; 50&quot; &quot; 40&quot; hurr_obs &lt;- hurr_obs %&gt;% dplyr::mutate(wind = ifelse(wind == &quot; -99&quot;, NA, as.numeric(wind))) Check the cleaned measurements: library(ggplot2) ggplot(hurr_obs, aes(x = wind)) + geom_histogram(binwidth = 10) Clean and check air pressure measurements in the same way: head(unique(hurr_obs$pressure)) ## [1] &quot; -999&quot; &quot; 961&quot; &quot; 924&quot; &quot; 938&quot; &quot; 950&quot; &quot; 997&quot; hurr_obs &lt;- hurr_obs %&gt;% dplyr::mutate(pressure = ifelse(pressure == &quot; -999&quot;, NA, as.numeric(pressure))) ggplot(hurr_obs, aes(x = pressure)) + geom_histogram(binwidth = 5) Check some of the very low pressure measurements: hurr_obs %&gt;% dplyr::arrange(pressure) %&gt;% dplyr::select(date_time, wind, pressure) %&gt;% slice(1:5) ## # A tibble: 5 × 3 ## date_time wind pressure ## &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2005-10-19 12:00:00 160 882 ## 2 1988-09-14 00:00:00 160 888 ## 3 1988-09-14 06:00:00 155 889 ## 4 1935-09-03 00:00:00 160 892 ## 5 1935-09-03 02:00:00 160 892 10.1.7 Exploring the hurricane tracking data Explore pressure versus wind speed, by storm status: ggplot(hurr_obs, aes(x = pressure, y = wind, color = status)) + geom_point(size = 0.2, alpha = 0.4) Next, we want to map storms by decade. Add hurricane decade: hurr_obs &lt;- hurr_obs %&gt;% dplyr::mutate(decade = substring(year(date_time), 1, 3), decade = paste0(decade, &quot;0s&quot;)) unique(hurr_obs$decade) ## [1] &quot;1850s&quot; &quot;1860s&quot; &quot;1870s&quot; &quot;1880s&quot; &quot;1890s&quot; &quot;1900s&quot; &quot;1910s&quot; &quot;1920s&quot; ## [9] &quot;1930s&quot; &quot;1940s&quot; &quot;1950s&quot; &quot;1960s&quot; &quot;1970s&quot; &quot;1980s&quot; &quot;1990s&quot; &quot;2000s&quot; ## [17] &quot;2010s&quot; Add logical for whether the storm was ever category 5: hurr_obs &lt;- hurr_obs %&gt;% dplyr::group_by(storm_id) %&gt;% dplyr::mutate(cat_5 = max(wind) &gt;= 137) %&gt;% dplyr::ungroup() To map the hurricane tracks, you need a base map to add the tracks to. Pull data to map hurricane-prone states: east_states &lt;- c(&quot;florida&quot;, &quot;georgia&quot;, &quot;south carolina&quot;, &quot;north carolina&quot;, &quot;virginia&quot;, &quot;maryland&quot;, &quot;delaware&quot;, &quot;new jersey&quot;, &quot;new york&quot;, &quot;connecticut&quot;, &quot;massachusetts&quot;, &quot;rhode island&quot;, &quot;vermont&quot;, &quot;new hampshire&quot;, &quot;maine&quot;, &quot;pennsylvania&quot;, &quot;west virginia&quot;, &quot;tennessee&quot;, &quot;kentucky&quot;, &quot;alabama&quot;, &quot;arkansas&quot;, &quot;texas&quot;, &quot;mississippi&quot;, &quot;louisiana&quot;) east_us &lt;- map_data(&quot;state&quot;, region = east_states) Plot tracks over a map of hurricane-prone states. Add thicker lines for storms that were category 5 at least once in their history. ggplot(east_us, aes(x = long, y = lat, group = group)) + geom_polygon(fill = &quot;cornsilk&quot;, color = &quot;cornsilk&quot;) + theme_void() + xlim(c(-108, -65)) + ylim(c(23, 48)) + geom_path(data = hurr_obs, aes(x = -longitude, y = latitude, group = storm_id), color = &quot;red&quot;, alpha = 0.2, size = 0.2) + geom_path(data = filter(hurr_obs, cat_5), aes(x = -longitude, y = latitude, group = storm_id), color = &quot;red&quot;) + facet_wrap(~ decade) Check trends in maximum wind recorded in any observation each year: Maximum wind observed each year: hurr_obs %&gt;% dplyr::mutate(storm_year = year(date_time)) %&gt;% dplyr::group_by(storm_year) %&gt;% dplyr::summarize(highest_wind = max(wind, na.rm = TRUE)) %&gt;% ggplot(aes(x = storm_year, y = highest_wind)) + geom_line() + geom_smooth(se = FALSE, span = 0.5) There is an R package named gender that predicts whether a name is male or female based on historical data: Vignette for gender package This package uses one of several databases of names (here, we’ll use Social Security Administration data), inputs a year or range of years, and outputs whether a name in that year was more likely female or male. We can apply a function from this package across all the named storms to see how male / female proportions changed over time. First, install the package (as well as genderdata, which is required to use the package). Once you do, you can use gender to determine the most common gender associated with a name in a given year or range of years: # install.packages(&quot;gender&quot;) # install.packages(&quot;genderdata&quot;, type = &quot;source&quot;, # repos = &quot;http://packages.ropensci.org&quot;) library(gender) gender(&quot;KATRINA&quot;, years = 2005)[ , c(&quot;name&quot;, &quot;gender&quot;)] ## # A tibble: 1 × 2 ## name gender ## &lt;chr&gt; &lt;chr&gt; ## 1 KATRINA female To apply this function across all our storms, it helps if we write a small function that “wraps” the gender function and outputs exactly (and only) what we want, in the format we want: get_gender &lt;- function(storm_name, storm_year){ storm_gender &lt;- gender(names = storm_name, years = storm_year, method = &quot;ssa&quot;)$gender if(length(storm_gender) == 0) storm_gender &lt;- NA return(storm_gender) } Now we can use mapply with this wrapper function to apply it across all our named storms: hurr_genders &lt;- hurr_meta %&gt;% dplyr::filter(storm_name != &quot;UNNAMED&quot;) %&gt;% dplyr::mutate(storm_year = substring(storm_id, 5, 8), storm_year = as.numeric(storm_year)) %&gt;% dplyr::filter(1880 &lt;= storm_year &amp; storm_year &lt;= 2012) %&gt;% dplyr::select(storm_name, storm_year, storm_id) %&gt;% dplyr::mutate(storm_gender = mapply(get_gender, storm_name = storm_name, storm_year = as.numeric(storm_year))) Now, plot a bar chart with the number of male, female, and unclear storms each year: hurr_genders %&gt;% dplyr::group_by(storm_year, storm_gender) %&gt;% dplyr::summarize(n = n()) %&gt;% ggplot(aes(x = storm_year, y = n, fill = storm_gender)) + geom_bar(stat = &quot;identity&quot;) + coord_flip() + scale_x_reverse() + theme_bw() + xlab(&quot;&quot;) + ylab(&quot;# of storms&quot;) Next, you can write a function to plot the track for a specific storm. You’ll want to be able to call the function by storm name and year, so join in the storm names from the hurr_meta dataset. We’ll exclude any “UNNAMED” storms. hurr_obs &lt;- hurr_obs %&gt;% dplyr::left_join(hurr_meta, by = &quot;storm_id&quot;) %&gt;% dplyr::filter(storm_name != &quot;UNNAMED&quot;) %&gt;% dplyr::mutate(storm_year = year(date_time)) Next, write a function to plot the track for a single storm. Use color to show storm status and size to show wind speed. map_track &lt;- function(storm, year, map_data = east_us, hurr_data = hurr_obs){ to_plot &lt;- hurr_obs %&gt;% dplyr::filter(storm_name == toupper(storm) &amp; storm_year == year) out &lt;- ggplot(east_us, aes(x = long, y = lat, group = group)) + geom_polygon(fill = &quot;cornsilk&quot;) + theme_void() + xlim(c(-108, -65)) + ylim(c(23, 48)) + geom_path(data = to_plot, aes(x = -longitude, y = latitude, group = NULL)) + geom_point(data = to_plot, aes(x = -longitude, y = latitude, group = NULL, color = status, size = wind), alpha = 0.5) return(out) } map_track(storm = &quot;Katrina&quot;, year = &quot;2005&quot;) map_track(storm = &quot;Camille&quot;, year = &quot;1969&quot;) map_track(storm = &quot;Hazel&quot;, year = &quot;1954&quot;) You can also write code with readLines that will read, check, and clean each line, one line at a time. con &lt;- file(&quot;~/my_file.txt&quot;, open = &quot;r&quot;) while (length(single_line &lt;- readLines(con, n = 1, warn = FALSE)) &gt; 0) { ## Code to check and clean each line and ## then add it to &quot;cleaned&quot; data frame. ## Run operations on `single_line`. } close(con) This can be particularly useful if you’re cleaning a very big file, especially if there are many lines you don’t want to keep. 10.2 Pulling online data 10.2.1 APIs API: “Application Program Interface” An API provides the rules for software applications to interact. In the case of open data APIs, they provide the rules you need to know to write R code to request and pull data from the organization’s web server into your R session. Often, an API can help you avoid downloading all available data, and instead only download the subset you need. Strategy for using APIs from R: Figure out the API rules for HTTP requests Write R code to create a request in the proper format Send the request using GET or POST HTTP methods Once you get back data from the request, parse it into an easier-to-use format if necessary Start by reading any documentation available for the API. This will often give information on what data is available and how to put together requests. Source: https://api.nasa.gov/api.html#EONET Many organizations will require you to get an API key and use this key in each of your API requests. This key allows the organization to control API access, including enforcing rate limits per user. API rate limits restrict how often you can request data (e.g., an hourly limit of 1,000 requests per user for NASA APIs). You should keep this key private. In particular, make sure you do not include it in code that is posted to GitHub. 10.2.2 Example– riem package The riem package, developed by Maelle Salmon and an ROpenSci package, is an excellent and straightforward example of how you can use R to pull open data through a web API. This package allows you to pull weather data from airports around the world directly from the Iowa Environmental Mesonet. To get a certain set of weather data from the Iowa Environmental Mesonet, you can send an HTTP request specifying a base URL, “https://mesonet.agron.iastate.edu/cgi-bin/request/asos.py/”, as well as some parameters describing the subset of dataset you want (e.g., date ranges, weather variables, output format). Once you know the rules for the names and possible values of these parameters (more on that below), you can submit an HTTP GET request using the GET function from the httr package. https://mesonet.agron.iastate.edu/cgi-bin/request/asos.py?station=DEN&amp;data=sknt&amp;year1=2016&amp;month1=6&amp;day1=1&amp;year2=2016&amp;month2=6&amp;day2=30&amp;tz=America%2FDenver&amp;format=comma&amp;latlon=no&amp;direct=no&amp;report_type=1&amp;report_type=2 When you are making an HTTP request using the GET or POST functions from the httr package, you can include the key-value pairs for any query parameters as a list object in the query argurment of the function. library(httr) meso_url &lt;- paste0(&quot;https://mesonet.agron.iastate.edu/&quot;, &quot;cgi-bin/request/asos.py/&quot;) denver &lt;- GET(url = meso_url, query = list(station = &quot;DEN&quot;, data = &quot;sped&quot;, year1 = &quot;2016&quot;, month1 = &quot;6&quot;, day1 = &quot;1&quot;, year2 = &quot;2016&quot;, month2 = &quot;6&quot;, day2 = &quot;30&quot;, tz = &quot;America/Denver&quot;, format = &quot;comma&quot;)) You can then use content from httr to retrieve the contents of the HTTP request. For this particular web data, the requested data is a comma-separated file, so you can convert it to a dataframe with read_csv: denver %&gt;% content() %&gt;% readr::read_csv(skip = 5, na = &quot;M&quot;) %&gt;% slice(1:3) ## # A tibble: 3 × 3 ## station valid sped ## &lt;chr&gt; &lt;dttm&gt; &lt;dbl&gt; ## 1 DEN 2016-06-01 01:00:00 6.9 ## 2 DEN 2016-06-01 01:05:00 6.9 ## 3 DEN 2016-06-01 01:10:00 6.9 10.3 Example R API wrappers 10.3.1 rOpenSci rOpenSci (https://ropensci.org): “At rOpenSci we are creating packages that allow access to data repositories through the R statistical programming environment that is already a familiar part of the workflow of many scientists. Our tools not only facilitate drawing data into an environment where it can readily be manipulated, but also one in which those analyses and methods can be easily shared, replicated, and extended by other researchers.” rOpenSci collects a number of packages for tapping into open data for research: https://ropensci.org/packages Some examples (all descriptions from rOpenSci): AntWeb: Access data from the world’s largest ant database chromer: Interact with the chromosome counts database (CCDB) gender: Encodes gender based on names and dates of birth musemeta: R Client for Scraping Museum Metadata, including The Metropolitan Museum of Art, the Canadian Science &amp; Technology Museum Corporation, the National Gallery of Art, and the Getty Museum, and more to come. rusda: Interface to some USDA databases webchem: Retrieve chemical information from many sources. Currently includes: Chemical Identifier Resolver, ChemSpider, PubChem, and Chemical Translation Service. For example, here is the description for the rnoaa package: “Access climate data from NOAA, including temperature and precipitation, as well as sea ice cover data, and extreme weather events” Buoy data from the National Buoy Data Center Historical Observing Metadata Repository (HOMR))— climate station metadata National Climatic Data Center weather station data Sea ice data International Best Track Archive for Climate Stewardship (IBTrACS)— tropical cyclone tracking data Severe Weather Data Inventory (SWDI) 10.3.2 countyweather The countyweather package wraps the rnoaa package to let you pull and aggregate weather at the county level in the U.S. For example, you can pull all data from Miami during Hurricane Andrew: When you pull the data for a county, the package also maps the contributing weather stations: 10.3.3 USGS-R Packages USGS has a very nice collection of R packages that wrap USGS open data APIs: https://owi.usgs.gov/R/ “USGS-R is a community of support for users of the R scientific programming language. USGS-R resources include R training materials, R tools for the retrieval and analysis of USGS data, and support for a growing group of USGS-R developers.” USGS R packages include: dataRetrieval: Obtain water quality sample data, streamflow data, and metadata directly from either the USGS or EPA EGRET: Analysis of long-term changes in water quality and streamflow, including the water-quality method Weighted Regressions on Time, Discharge, and Season (WRTDS) laketemps: Lake temperature data package for Global Lake Temperature Collaboration Project lakeattributes: Common useful lake attribute data soilmoisturetools: Tools for soil moisture data retrieval and visualization 10.3.4 U.S. Census packages A number of R packages help you access and use data from the U.S. Census: tigris: Download and use Census TIGER/Line shapefiles in R acs: Download, manipulate, and present American Community Survey and Decennial data from the US Census USABoundaries: Historical and contemporary boundaries of the United States of America idbr: R interface to the US Census Bureau International Data Base API For example, the tigris package allows you to pull spatial data for: Location boundaries States Counties Blocks Tracks School districts Congressional districts Roads Primary roads Primary and secondary roads Water Area-water Linear-water Coastline Other Landmarks Military Example from: Kyle Walker. 2016. “tigris: An R Package to Access and Work with Geographic Data from the US Census Bureau”. The R Journal. 10.3.5 Other R API wrappers Here are some examples of other R packages that faciliate use of an API for open data: twitteR: Twitter Quandl: Quandl (financial data) RGoogleAnalytics: Google Analytics WDI, wbstats: World Bank GuardianR, rdian: The Guardian Media Group blsAPI: Bureau of Labor Statistics rtimes: New York Times 10.3.6 R and APIs Find out more about writing API packages with this vignette for the httr package: https://cran.r-project.org/web/packages/httr/vignettes/api-packages.html. This document includes advice on error handling within R code that accesses data through an open API. 10.4 Parsing webpages You can also use R to pull and clean web-based data that is not accessible through a web API or as an online flat file. In this case, the strategy is: Pull in the full web page file (often in HTML or XML) Parse or clean the file within R (e.g., with regular expressions) 10.4.1 rvest The rvest package should be the first thing you try if you need to pull and parse data from a webpage that is not a flat file. This package allows you to read an HTML or XML file and pull out a certain element. Here is a very simple example of this parsing (this and later examples are from rvest documentation): library(rvest) read_html(&quot;&lt;html&gt;&lt;title&gt;Hi&lt;title&gt;&lt;/html&gt;&quot;) ## {xml_document} ## &lt;html&gt; ## [1] &lt;head&gt;\\n &lt;title&gt;Hi&lt;title/&gt;&lt;/title&gt;\\n&lt;/head&gt; If you have an HTML or XML page you want to pull data from, you’ll first need to read the page: library(rvest) lego_movie &lt;- read_html(&quot;http://www.imdb.com/title/tt1490017/&quot;) lego_movie ## {xml_document} ## &lt;html xmlns:og=&quot;http://ogp.me/ns#&quot; xmlns:fb=&quot;http://www.facebook.com/2008/fbml&quot;&gt; ## [1] &lt;head&gt;\\n &lt;meta charset=&quot;utf-8&quot;/&gt;\\n &lt;meta http-equiv=&quot;X-UA-Compatib ... ## [2] &lt;body&gt;&lt;noscript&gt;\\n &lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; ... Then you can use html_nodes and html_text to pull and parse just the elements you want: rating_node &lt;- lego_movie %&gt;% html_nodes(&quot;strong span&quot;) rating_node ## {xml_nodeset (1)} ## [1] &lt;span itemprop=&quot;ratingValue&quot;&gt;7.8&lt;/span&gt; rating &lt;- rating_node %&gt;% html_text() %&gt;% as.numeric() rating ## [1] 7.8 You can pull and parse tables: lego_movie %&gt;% html_nodes(&quot;table&quot;) %&gt;% `[[`(1) %&gt;% html_table() %&gt;% select(X2) %&gt;% slice(2:8) ## X2 ## 1 Will Arnett ## 2 Elizabeth Banks ## 3 Craig Berry ## 4 Alison Brie ## 5 David Burrows ## 6 Anthony Daniels ## 7 Charlie Day The only tricky part of this is figuring out which CSS selector you can use to pull a specific element of a webpage. You can use “Selectorgadget” to help with this. Read the vignette for that tool here: ftp://cran.r-project.org/pub/R/web/packages/rvest/vignettes/selectorgadget.html 10.4.2 Scraping multiple pages cities &lt;- c(&quot;denver&quot;, &quot;boulder&quot;, &quot;fort-collins&quot;) kitchen_addresses &lt;- c() for(i in 1:length(cities)){ restaurant_url &lt;- paste0(&quot;http://thekitchen.com/the-kitchen-&quot;, cities[i]) restaurant_page &lt;- read_html(restaurant_url) address &lt;- restaurant_page %&gt;% html_nodes(&quot;p:nth-child(2)&quot;) %&gt;% html_text() kitchen_addresses[i] &lt;- address[1] } kitchen_addresses ## [1] &quot;1530 16th Street (Entrance on Wazee Street)\\nDenver, CO 80202&quot; ## [2] &quot;1039 Pearl St.,\\nBoulder, CO 80302&quot; ## [3] &quot;100 North College Avenue\\nFort Collins, CO 80524&quot; library(ggmap) library(stringr) kitchen_latlons &lt;- geocode(kitchen_addresses) for(i in 1:length(cities)){ city_map &lt;- get_map(paste(gsub(&quot;-&quot;, &quot; &quot;, cities[i]), &quot;colorado&quot;), zoom = 14, maptype = &quot;roadmap&quot;) city_map &lt;- ggmap(city_map) + geom_point(data = kitchen_latlons[i, ], aes(x = lon, y = lat), color = &quot;red&quot;, size = 4, alpha = 0.4) + theme_void() + ggtitle(paste(&quot;The Kitchen in&quot;, str_to_title(gsub(&quot;-&quot;, &quot; &quot;, cities[i])))) print(city_map) } "],
["exploring-data-3.html", "Chapter 11 Exploring data #3 11.1 forcats 11.2 Simulations 11.3 Other computationally-intensive approaches", " Chapter 11 Exploring data #3 Download a pdf of the lecture slides covering this topic. 11.1 forcats Hadley Wickham has developed a package called forcats that helps you work with categorical variables (factors). I’ll show some examples of its functions using the worldcup dataset: library(forcats) library(faraway) data(worldcup) The fct_recode function can be used to change the labels of a function (along the lines of using factor with levels and labels to reset factor labels). One big advantage is that fct_recode lets you change labels for some, but not all, levels. For example, here are the team names: worldcup %&gt;% filter(str_detect(Team, &quot;^US&quot;)) %&gt;% slice(1:3) %&gt;% select(Team, Position, Time) ## Team Position Time ## 1 USA Midfielder 10 ## 2 USA Defender 390 ## 3 USA Defender 200 If you just want to change “USA” to “United States”, you can run: worldcup &lt;- worldcup %&gt;% mutate(Team = fct_recode(Team, `United States` = &quot;USA&quot;)) worldcup %&gt;% filter(str_detect(Team, &quot;^Un&quot;)) %&gt;% slice(1:3) %&gt;% select(Team, Position, Time) ## Team Position Time ## 1 United States Midfielder 10 ## 2 United States Defender 390 ## 3 United States Defender 200 You can use the fct_lump function to lump uncommon factors into an “Other” category. For example, to lump the two least common positions together, you can run (n specifies how many categories to keep outside of “Other”): worldcup %&gt;% dplyr::mutate(Position = forcats::fct_lump(Position, n = 2)) %&gt;% dplyr::count(Position) ## # A tibble: 3 × 2 ## Position n ## &lt;fctr&gt; &lt;int&gt; ## 1 Defender 188 ## 2 Midfielder 228 ## 3 Other 179 You can use the fct_infreq function to reorder the levels of a factor from most common to least common: levels(worldcup$Position) ## [1] &quot;Defender&quot; &quot;Forward&quot; &quot;Goalkeeper&quot; &quot;Midfielder&quot; worldcup &lt;- worldcup %&gt;% mutate(Position = fct_infreq(Position)) levels(worldcup$Position) ## [1] &quot;Midfielder&quot; &quot;Defender&quot; &quot;Forward&quot; &quot;Goalkeeper&quot; If you want to reorder one factor by another variable (ascending order), you can use fct_reorder (e.g., homework 3). For example, to relevel Position by the average shots on goals for each position, you can run: levels(worldcup$Position) ## [1] &quot;Midfielder&quot; &quot;Defender&quot; &quot;Forward&quot; &quot;Goalkeeper&quot; worldcup &lt;- worldcup %&gt;% group_by(Position) %&gt;% mutate(ave_shots = mean(Shots)) %&gt;% ungroup() %&gt;% mutate(Position = fct_reorder(Position, ave_shots)) levels(worldcup$Position) ## [1] &quot;Midfielder&quot; &quot;Defender&quot; &quot;Forward&quot; &quot;Goalkeeper&quot; 11.2 Simulations 11.2.1 The lady tasting tea Source: Flikr commons, https://www.flickr.com/photos/internetarchivebookimages/20150531109/ “Dr. Muriel Bristol, a colleague of Fisher’s, claimed that when drinking tea she could distinguish whether milk or tea was added to the cup first (she preferred milk first). To test her claim, Fisher asked her to taste eight cups of tea, four of which had milk added first and four of which had tea added first.” — Agresti, Categorical Data Analysis, p.91 Question: If she just guesses, what is the probability she will get all cups right? What if more or fewer cups are used in the experiment? One way to figure this out is to run a simulation. In R, sample can be a very helpful function for simulations. It lets you randomly draw values from a vector, with or without replacement. ## Generic code sample(x = [vector to sample from], size = [number of samples to take], replace = [logical-- should values in the vector be replaced?], prob = [vector of probability weights]) Create vectors of the true and guessed values, in order, for the cups of tea: n_cups &lt;- 8 cups &lt;- sample(rep(c(&quot;milk&quot;, &quot;tea&quot;), each = n_cups / 2)) cups ## [1] &quot;milk&quot; &quot;milk&quot; &quot;tea&quot; &quot;tea&quot; &quot;milk&quot; &quot;tea&quot; &quot;milk&quot; &quot;tea&quot; guesses &lt;- sample(rep(c(&quot;milk&quot;, &quot;tea&quot;), each = n_cups / 2)) guesses ## [1] &quot;tea&quot; &quot;tea&quot; &quot;tea&quot; &quot;milk&quot; &quot;tea&quot; &quot;milk&quot; &quot;milk&quot; &quot;milk&quot; For this simulation, determine how many cups she got right (i.e., guess equals the true value): cup_results &lt;- cups == guesses cup_results ## [1] FALSE FALSE TRUE FALSE FALSE FALSE TRUE FALSE n_right &lt;- sum(cup_results) n_right ## [1] 2 Write a function that will run one simulation. It takes the argument n_cups— in real life, they used eight cups (n_cups = 8). Note that this function just wraps the code we just walked through. sim_null_tea &lt;- function(n_cups){ cups &lt;- sample(rep(c(&quot;milk&quot;, &quot;tea&quot;), each = n_cups / 2)) guesses &lt;- sample(rep(c(&quot;milk&quot;, &quot;tea&quot;), each = n_cups / 2)) cup_results &lt;- cups == guesses n_right &lt;- sum(cup_results) return(n_right) } sim_null_tea(n_cups = 8) ## [1] 6 Now, we need to run a lot of simulations, to see what happens on average if she guesses. You can use the replicate function to do that. ## Generic code replicate(n = [number of replications to run], eval = [code to replicate each time]) tea_sims &lt;- replicate(5, sim_null_tea(n_cups = 8)) tea_sims ## [1] 4 4 4 4 6 This call gives a vector with the number of cups she got right for each simulation. You can replicate the simulation many times to get a better idea of what to expect if she just guesses, including what percent of the time she gets all cups right. tea_sims &lt;- replicate(1000, sim_null_tea(n_cups = 8)) mean(tea_sims) ## [1] 3.94 quantile(tea_sims, probs = c(0.025, 0.975)) ## 2.5% 97.5% ## 2 6 mean(tea_sims == 8) ## [1] 0.016 Now we’d like to know, for different numbers of cups of tea, what is the probability that the lady will get all cups right? For this, we can apply the replication code across different values of n_cups: n_cups &lt;- seq(from = 2, to = 14, by = 2) perc_all_right &lt;- sapply(n_cups, FUN = function(n_cups){ cups_right &lt;- replicate(1000, sim_null_tea(n_cups)) out &lt;- mean(cups_right == n_cups) return(out) }) perc_all_right ## [1] 0.500 0.145 0.054 0.019 0.009 0.001 0.000 tea_sims &lt;- data_frame(n_cups, perc_all_right) ggplot(tea_sims, aes(x = n_cups, y = perc_all_right)) + geom_point() + xlab(&quot;# of cups tested&quot;) + ylab(&quot;Probability of getting\\nall cups right if guessing&quot;) You can answer this question analytically using the hypergeometric distribution: \\[ P(n_{11} = t) = \\frac{{n_{1+} \\choose t} {n_{2+} \\choose n_{+1}-t}}{{n \\choose n_{+1}}} \\] Guessed milk Guessed tea Total Really milk \\(n_{11}\\) \\(n_{12}\\) \\(n_{1+} = 4\\) Really tea \\(n_{21}\\) \\(n_{22}\\) \\(n_{2+} = 4\\) Total \\(n_{+1} = 4\\) \\(n_{+2} = 4\\) \\(n = 8\\) In R, you can use dhyper to get the density of the hypergeometric function: dhyper(x = [# of cups she guesses have milk first that do], m = [# of cups with milk first], n = [# of cups with tea first], k = [# of cups she guesses have milk first]) Probability she gets three “milk” cups right if she’s just guessing and there are eight cups, four with milk first and four with tea first: dhyper(x = 3, m = 4, n = 4, k = 4) ## [1] 0.2285714 Probability she gets three or more “milk” cups right if she’s just guessing: dhyper(x = 3, m = 4, n = 4, k = 4) + dhyper(x = 4, m = 4, n = 4, k = 4) ## [1] 0.2428571 Other density functions: dnorm: Normal dpois: Poisson dbinom: Binomial dchisq: Chi-squared dt: Student’s t dunif: Uniform You can get the analytical result for each of the number of cups we simulated and compare those values to our simulations: analytical_results &lt;- data_frame(n_cups = seq(2, 14, 2)) %&gt;% mutate(perc_all_right = dhyper(x = n_cups / 2, m = n_cups / 2, n = n_cups / 2, k = n_cups / 2)) ggplot(analytical_results, aes(x = n_cups, y = perc_all_right)) + geom_line(color = &quot;darkgray&quot;) + geom_point(data = tea_sims) + xlab(&quot;# of cups tested&quot;) + ylab(&quot;Probability of getting\\nall cups right if guessing&quot;) For more on this story (and R.A. Fisher), see: The Lady Tasting Tea: How Statistics Revolutionized Science in the Twentieth Century. David Salsburg. The Design of Experiments. Ronald Fisher. https://priceonomics.com/why-the-father-of-modern-statistics-didnt-believe/ 11.2.2 Playing darts Research question: Is a person skilled at playing darts? Here’s our dart board– the numbers are the number of points you win for a hit in each area. First, what would we expect to see if the person we test has no skill at playing darts? Questions to consider: What would the dart board look like under the null (say the person throws 20 darts for the experiment)? About what do you think the person’s mean score would be if they had no skill at darts? What are some ways to estimate or calculate the expected mean score under the null? Let’s use R to answer the first question: what would the null look like? First, create some random throws (the square goes from -1 to 1 on both sides): n.throws &lt;- 20 throw.x &lt;- runif(n.throws, min = -1, max = 1) throw.y &lt;- runif(n.throws, min = -1, max = 1) head(cbind(throw.x, throw.y)) ## throw.x throw.y ## [1,] 0.4864163 0.5143803 ## [2,] 0.5522363 0.9521110 ## [3,] -0.9532503 0.5069258 ## [4,] 0.4420512 0.1773405 ## [5,] 0.8259857 -0.4362430 ## [6,] 0.3654047 -0.5930003 plot(c(-1, 1), c(-1,1), type = &quot;n&quot;, asp=1, xlab = &quot;&quot;, ylab = &quot;&quot;, axes = FALSE) rect( -1, -1, 1, 1) draw.circle( 0, 0, .75, col = &quot;red&quot;) draw.circle( 0, 0, .5, col = &quot;white&quot;) draw.circle( 0, 0, .25, col = &quot;red&quot;) points(throw.x, throw.y, col = &quot;black&quot;, pch = 19) Next, let’s tally up the score for this simulation of what would happen under the null. To score each throw, we calculate how far the point is from (0, 0), and then use the following rules: 20 points: \\(0.00 \\le \\sqrt{x^2 + y^2} \\le .25\\) 15 points: \\(0.25 &lt; \\sqrt{x^2 + y^2} \\le .50\\) 10 points: \\(0.50 &lt; \\sqrt{x^2 + y^2} \\le .75\\) 0 points: \\(0.75 &lt; \\sqrt{x^2 + y^2} \\le 1.41\\) Use these rules to “score” each random throw: throw.dist &lt;- sqrt(throw.x^2 + throw.y^2) head(throw.dist) ## [1] 0.7079462 1.1006726 1.0796573 0.4762971 0.9341094 0.6965415 throw.score &lt;- cut(throw.dist, breaks = c(0, .25, .5, .75, 1.5), labels = c(&quot;20&quot;, &quot;15&quot;, &quot;10&quot;, &quot;0&quot;), right = FALSE) head(throw.score) ## [1] 10 0 0 15 0 10 ## Levels: 20 15 10 0 Now that we’ve scored each throw, let’s tally up the total: table(throw.score) ## throw.score ## 20 15 10 0 ## 1 3 4 12 mean(as.numeric(as.character(throw.score))) ## [1] 5.25 So, this just showed one example of what might happen under the null. If we had a lot of examples like this (someone with no skill throwing 20 darts), what would we expect the mean scores to be? Questions to consider: How can you figure out the expected value of the mean scores under the null (that the person has no skill)? Do you think that 20 throws will be enough to figure out if a person’s mean score is different from this value, if he or she is pretty good at darts? What steps do you think you could take to figure out the last question? What could you change about the experiment to make it easier to tell if someone’s skilled at darts? How can we figure this out? Theory. Calculate the expected mean value using the expectation formula. Simulation. Simulate a lot of examples using R and calculate the mean of the mean score from these. The expected value of the mean, \\(E[\\bar{X}]\\), is the expected value of \\(X\\), \\(E[X]\\). To calculate the expected value of \\(X\\), use the formula: \\[ E[X] = \\sum_x xp(x) \\] \\[ E[X] = 20 * p(X = 20) + 15 * p(X = 15) + 10 * p(X = 10) + 0 * p(X = 0) \\] So we just need to figure out \\(p(X = x)\\) for \\(x = 20, 15, 10\\). (In all cases, we’re dividing by 4 because that’s the area of the full square, \\(2^2\\).) \\(p(X = 20)\\): Proportional to area of the smallest circle, \\((\\pi * 0.25^2) / 4 = 0.049\\) \\(p(X = 15)\\): Proportional to area of the middle circle minus area of the smallest circle, \\(\\pi(0.50^2 - 0.25^2) / 4 = 0.147\\) \\(p(X = 10)\\): Proportional to area of the largest circle minus area of the middle circle, \\(\\pi(0.75^2 - 0.50^2) / 4 = 0.245\\) \\(p(X = 0)\\): Proportional to area of the square minus area of the largest circle, \\((2^2 - \\pi * 0.75^2) / 4 = 0.558\\) As a double check, if we’ve done this right, the probabilities should sum to 1: \\[0.049 + 0.147 + 0.245 + 0.558 = 0.999\\] \\[ E[X] = \\sum_x xp(x)\\] \\[ E[X] = 20 * 0.049 + 15 * 0.147 + 10 * 0.245 + 0 * 0.558\\] \\[ E[X] = 5.635 \\] Remember, this also gives us \\(E[\\bar{X}]\\). Now it’s pretty easy to also calculate \\(var(X)\\) and \\(var(\\bar{X})\\): \\[ Var(X) = E[(X - \\mu)^2] = E[X^2] - E[X]^2 \\] \\[ E[X^2] = 20^2 * 0.049 + 15^2 * 0.147 + 10^2 * 0.245 + 0^2 * 0.558 = 77.18 \\] \\[ Var(X) = 77.175 - (5.635)^2 = 45.42 \\] \\[ Var(\\bar X) = \\sigma^2 / n = 45.42 / 20 = 2.27 \\] Note that we can use the Central Limit Theorem to calculate a 95% confidence interval for the mean score when someone with no skill (null hypothesis) throws 20 darts: 5.635 + c(-1, 1) * qnorm(.975) * sqrt(2.27) ## [1] 2.682017 8.587983 We can check our math by running simulations– we should get the same values of \\(E[\\bar{X}]\\) and \\(Var(\\bar{X})\\) (which we can calculate directly from the simulations using R). n.throws &lt;- 20 n.sims &lt;- 10000 x.throws &lt;- matrix(runif(n.throws * n.sims, -1, 1), ncol = n.throws, nrow = n.sims) y.throws &lt;- matrix(runif(n.throws * n.sims, -1, 1), ncol = n.throws, nrow = n.sims) dist.throws &lt;- sqrt(x.throws^2 + y.throws^2) score.throws &lt;- apply(dist.throws, 2, cut, breaks = c(0, .25, .5, .75, 1.5), labels = c(&quot;20&quot;, &quot;15&quot;, &quot;10&quot;, &quot;0&quot;), right = FALSE) dist.throws[1:3,1:5] ## [,1] [,2] [,3] [,4] [,5] ## [1,] 0.8775691 1.058769 0.5702137 0.6570697 1.2935053 ## [2,] 0.3530668 0.429293 0.7054812 1.1418023 0.7531201 ## [3,] 1.1500552 1.023406 0.2567319 0.8656221 0.1500548 score.throws[1:3,1:5] ## [,1] [,2] [,3] [,4] [,5] ## [1,] &quot;0&quot; &quot;0&quot; &quot;10&quot; &quot;10&quot; &quot;0&quot; ## [2,] &quot;15&quot; &quot;15&quot; &quot;10&quot; &quot;0&quot; &quot;0&quot; ## [3,] &quot;0&quot; &quot;0&quot; &quot;15&quot; &quot;0&quot; &quot;20&quot; mean.scores &lt;- apply(score.throws, MARGIN = 1, function(x){ out &lt;- mean(as.numeric( as.character(x))) return(out) }) head(mean.scores) ## [1] 4.25 3.25 4.25 6.00 4.25 4.25 Let’s check the simulated mean and variance against the theoretical values: mean(mean.scores) ## Theoretical: 5.635 ## [1] 5.642625 var(mean.scores) ## Theoretical: 2.27 ## [1] 2.282418 11.2.3 Simulations in research Simulations in the wild (just a few examples): The Manhattan Project US Coast Guard search and rescue Infectious disease modeling 11.3 Other computationally-intensive approaches 11.3.1 Bootstrap and friends Bootstraping: Sample the dataset with replacement and reestimate the statistical parameter(s) each time. Jackknifing: Rake out one observation at a time and reestimate the statistical parameter(s) with the rest of the data. Permutation tests: See how unusual the result from the data is compared to if you shuffle your data (and so remove any relationship in observed data between variables). Cross-validation: See how well your model performs if you pick a subset of the data, build the model just on that subset, and then test how well it predicts for the rest of the data, and repeat that many times. 11.3.2 Bayesian analysis Suggested books for learning more about Bayesian analysis in R: Doing Bayesian Data Analysis, Second Edition: A Tutorial with R, JAGS, and Stan. John Kruschke. Statistical Rethinking: A Bayesian Course with Examples in R and Stan. Richard McElreath. Bayesian Data Analysis, Third Edition. Andrew Gelman et al. R can tap into software for Bayesian analysis: BUGS JAGS STAN 11.3.3 Ensemble models and friends Bagging: Sample data with replacement and build a tree model. Repeat many times. To predict, predict from all models and take the majority vote. Random forest: Same as bagging, for picking each node of a tree, only consider a random subset of variables. Boosting: Same as bagging, but “learn” from previous models as you build new models. Stacked models: Build many different models (e.g., generalized linear regression, Naive Bayes, k-nearest neighbors, random forest, …), determine weights for each, and predict using weighted predictions combined from all models. For more on these and other machine learning topics, see: An Introduction to Statistical Learning. Gareth James, Robert Tibshirani, and Trevor Hastie. The caret package: http://topepo.github.io/caret/index.html For many examples of predictive models like this built with R (and Python): https://www.kaggle.com "],
["reporting-data-results-3.html", "Chapter 12 Reporting data results #3 12.1 Shiny web apps 12.2 Viridis color map 12.3 htmlWidgets", " Chapter 12 Reporting data results #3 Download a pdf of the lecture slides covering this topic. 12.1 Shiny web apps 12.1.1 Resources for learning Shiny There is an excellent tutorial to get you started here at RStudio. There are also several great sites that show you both Shiny examples and their code, here and here. Many of the examples and ideas in the course notes this week come directly or are adapted from RStudio’s Shiny tutorial. To start, Shiny has several example apps that you can try out. These are all available through your R session once you install the Shiny package. You can make them available to your R session using the command system.file(): install.packages(&quot;shiny&quot;) library(shiny) system.file(&quot;examples&quot;, package = &quot;shiny&quot;) 12.1.2 Basics of Shiny apps Once you have Shiny installed, you can run the examples using the runExample() command. For example, to run the first example, you would run: runExample(&quot;01_hello&quot;) This is a histogram that lets you adjust the number of bins using a slider bar. Other examples are: 02_text, 03_reactivity, 04_mpg, 05_sliders, 06_tabsets, 07_widgets, 08_html, 09_upload, 10_download, and 11_timer. When you run any of these, a window will come up in your R session that shows the Shiny App, and your R session will pay attention to commands it gets from that application until you close the window. Notice that if you scroll down, you’ll be able to see the code that’s running behind the application: Generally, each application takes two files: (1) a user interface file and (2) a server file. You can kind of think of the two elements of an R shiny app, the user interface and the server, as two parts of a restaurant. The user interface is the dining area. This is the only place the customer every sees. It’s where the customer makes his order, and it’s also where the final product comes out for him to consume. The server is the kitchen. It takes the order, does all the stuff to make it happen, and then sends out the final product back to the dining area. At its heart, an R shiny app is just a directory on your computer or a server with these two files (as well as any necessary data files) in it. For example, here’s a visual of an App I wrote to go with a paper: This has the heart of the application (server.R and ui.R) plus a couple of R helper files and subdirectories with some figures and data that I’m using in the that application. If I open either of the main files in RStudio, I can run the application locally using a button at the top of the file called “Run App”. Once I have the App running, if I have an account for the Shiny server, I can choose to “Publish” the application to the Shiny server, and then anyone can access and use it online (this service is free up to a certain number of visitors per time– unless you make something that is very popular, you should be well within the free limit). 12.1.3 server.R file The server file will be named server.R. This file tells R what code to run with the inputs it gets from a user making certain selections. For example, for the histogram example, this file tells R how to re-draw a histogram of the data with the number of bins that the user specified on the slider on the application. Once you get through all the code for what to do, this file also will have code telling R what to send back to the application for the user to see (in this case, a picture of a histogram made with the specified number of bars). Here is the code in the server.R file for the histogram example: library(shiny) # Define server logic required to draw a histogram shinyServer(function(input, output) { # Expression that generates a histogram. The expression is # wrapped in a call to renderPlot to indicate that: # # 1) It is &quot;reactive&quot; and therefore should be automatically # re-executed when inputs change # 2) Its output type is a plot output$distPlot &lt;- renderPlot({ x &lt;- faithful[, 2] # Old Faithful Geyser data bins &lt;- seq(min(x), max(x), length.out = input$bins + 1) # draw the histogram with the specified number of bins hist(x, breaks = bins, col = &#39;darkgray&#39;, border = &#39;white&#39;) }) }) Notice that some of the “interior” code here looks very familiar and should remind you of code we’ve been learning about this class. For example, this file has within it some code to figure out the breaks for histogram bins, based on how many total bins you want, and draw a histogram with those bin breaks: x &lt;- faithful[, 2] # Old Faithful Geyser data bins &lt;- seq(min(x), max(x), length.out = input$bins + 1) # draw the histogram with the specified number of bins hist(x, breaks = bins, col = &#39;darkgray&#39;, border = &#39;white&#39;) This code is then “wrapped” in two other functions. First, this code is generating a plot that will be posted to the application, so it’s wrapped in a renderPlot function to send that plot as output back to the application: output$distPlot &lt;- renderPlot({ x &lt;- faithful[, 2] # Old Faithful Geyser data bins &lt;- seq(min(x), max(x), length.out = input$bins + 1) # draw the histogram with the specified number of bins hist(x, breaks = bins, col = &#39;darkgray&#39;, border = &#39;white&#39;) }) Notice that this code is putting the results of renderPlot into a slot of the object output named distPlot. We could have used any name we wanted to here, not just distPlot, for the name of the slot where we’re putting this plot, but it is important to put everything into an object called output. Now that we’ve rendered the plot and put it in that slot of the output object, we’ll be able to refer to it by its name in the user interface file, when we want to draw it somewhere there. All of this is wrapped up in another wrapper: # Define server logic required to draw a histogram shinyServer(function(input, output) { # Expression that generates a histogram. The expression is # wrapped in a call to renderPlot to indicate that: # # 1) It is &quot;reactive&quot; and therefore should be automatically # re-executed when inputs change # 2) Its output type is a plot output$distPlot &lt;- renderPlot({ x &lt;- faithful[, 2] # Old Faithful Geyser data bins &lt;- seq(min(x), max(x), length.out = input$bins + 1) # draw the histogram with the specified number of bins hist(x, breaks = bins, col = &#39;darkgray&#39;, border = &#39;white&#39;) }) }) The server.R file also has a line to load the shiny package. You should think of apps as being like Rmd files– if there are any packages or datasets that you need to use in the code in that file, you need to load it within the file, because R won’t check in your current R session to find it when it runs the file. 12.1.4 ui.R file The other file that a Shiny app needs is the user interface file (ui.R). This is the file that describes how the application should look. It will write all the buttons and sliders and all that you want for the application interface. This is also where you specify what you want to go where and put in any text that you want to show up. For example, here is the ui.R file for the histogram example: library(shiny) # Define UI for application that draws a histogram shinyUI(fluidPage( # Application title titlePanel(&quot;Hello Shiny!&quot;), # Sidebar with a slider input for the number of bins sidebarLayout( sidebarPanel( sliderInput(&quot;bins&quot;, &quot;Number of bins:&quot;, min = 1, max = 50, value = 30) ), # Show a plot of the generated distribution mainPanel( plotOutput(&quot;distPlot&quot;) ) ) )) There are a few things to notice with this code. First, there is some code that tells the application to show the results from the server.R code. For example, the following code tells R to show the histogram that we put into the output object in the distPlot slot and to put that graph in the main panel of the application: mainPanel( plotOutput(&quot;distPlot&quot;) ) Other parts of the ui.R code will tell the application what kinds of choice boxes and sliders to have on the application, and what default value to set each to. For example, the following code tells the application that it should have a slider bar that can take a minimum value of 1 and a maximum value of 50. When you first open the application, its default value should be 30. It should be annotated with the text “Number of bins:”. Whatever value is selected should be saved to the bins slot of the input object (just like we’re using the output object to get things out of the server and printed to the application, we’re using the input object to get things that the user chooses from the application interface to the server where we can run R code). sliderInput(&quot;bins&quot;, &quot;Number of bins:&quot;, min = 1, max = 50, value = 30) 12.1.5 Making a Shiny app The first step in making a Shiny app is to make a new directory somewhere and to create R scripts for that directory called server.R and ui.R. You can just make these two files the normal way– within RStudio, do “New File”, “R Script”, and then just save them with the correct names to the directory you created for the App. Once you save a file as ui.R, notice that you’ll have a button in the top right of the file called “Run App”. When you’re ready to run your application, you can either use this button or use the command runApp. 12.1.6 Starting with the ui.R file Next, you’ll need to put code in these files. I would suggest starting with the ui.R files. This file is where you get to set up how the application looks and how people will be able to interact with it. That means that this is a good place to start because it’s both quickly fulfilling (I made something pretty!) and also because you need to have an idea of what inputs and outputs you need before you can effectively make the server file to tell R what to do. In truth, though, you’ll be going back and forth quite a bit between these two files as you edit your application. In the ui.R file, everything needs to be wrapped in a shinyUI() function, and then most things will be wrapped in other functions within that to set up different panels. For example, here’s a very basic ui.R file (adapted directly from the RStudio tutorial) that shows a very basic set up for a user interface: shinyUI(fluidPage( titlePanel(&quot;Tweets during Paris Attack&quot;), sidebarLayout( sidebarPanel(&quot;Select hashtag to display&quot;), mainPanel(&quot;Map of tweets&quot;) ) )) Notice that everything that I want to go in certain panels of the page are wrapped in functions like sidebarPanel and mainPanel and titlePanel. Everything in this file will be divided up by the place you want it to go in the final version. As a note, the sidebar layout (a sidebar on one side and one main panel) is the simplest possible Shiny layout. You can do fancier layouts if you want by using different functions like fluidRow() and navBarPage(). RStudio has a layout help page with very detailed instructions and examples to help you figure out how to do other layouts. If I run this ui.R, even if my server.R file only includes the line shinyServer(function(input, output) { }), I’ll get the following version application: This doesn’t have anything interactive on it, and it isn’t using R at all, but it shows the basics of how the syntax of the ui.R file works. As a note, I don’t have all of the functions for this, like fluidPage and titlePanel memorized. When I’m working on this file, I’ll either look to example code from other Shiny apps of look at RStudio’s help for Shiny applications until I can figure out what syntax to use to do what I want to do. 12.1.7 Adding in widgets Next, I’ll add in some cool things that will let the user interact with the application. In this case, I’d like to have a slider bar so people can chose the range of time for the tweets that are shown. I’d also like to have a selection box so that users can look at maps of specific hashtags or terms. To add these on (they won’t be functional, yet, but they’ll be there!), I can edit the ui.R script to the following: shinyUI(fluidPage( titlePanel(&quot;Tweets during Paris Attack&quot;), sidebarLayout(position = &quot;right&quot;, sidebarPanel(&quot;Choose what to display&quot;, sliderInput(inputId = &quot;time_range&quot;, label = &quot;Select the time range: &quot;, value = c(as.POSIXct(&quot;2015-11-13 00:00:00&quot;, tz = &quot;CET&quot;), as.POSIXct(&quot;2015-11-14 12:00:00&quot;, tz = &quot;CET&quot;)), min = as.POSIXct(&quot;2015-11-13 00:00:00&quot;, tz = &quot;CET&quot;), max = as.POSIXct(&quot;2015-11-14 12:00:00&quot;, tz = &quot;CET&quot;), step = 60, timeFormat = &quot;%dth %H:%M&quot;, timezone = &quot;+0100&quot;)), mainPanel(&quot;Map of tweets&quot;) ) )) The important part of this is the new sliderInput call, which sets up a slider bar that users can use to specify certain time ranges to look at. Here is what the interface of the app looks like now: If I open this application, I can move the slider bar around, but I it isn’t actually sending any information to R yet. The heart of this new addition to the ui.R file is this: sliderInput(inputId = &quot;time_range&quot;, label = &quot;Select the time range: &quot;, value = c(as.POSIXct(&quot;2015-11-13 00:00:00&quot;, tz = &quot;CET&quot;), as.POSIXct(&quot;2015-11-14 12:00:00&quot;, tz = &quot;CET&quot;)), min = as.POSIXct(&quot;2015-11-13 00:00:00&quot;, tz = &quot;CET&quot;), max = as.POSIXct(&quot;2015-11-14 12:00:00&quot;, tz = &quot;CET&quot;), step = 60, timeFormat = &quot;%dth %H:%M&quot;, timezone = &quot;+0100&quot;) This is all within the function sliderInput and is all being used to set up the slider bar. &quot;time_range&quot; is the name I’m giving the input I get from this. Later, when I write my server code, I’ll be able to pull the values that the user suggested from the time_range slot of the input object. The next thing is the label. This is what I want R to print right before it gives the slider bar. The value object says which values I want to be the defaults on the slider. This is where the slider positions will be when someone initially opens the application. min and max give the highest and lowest values that will show up on the slider bar. For this application, I’m calling them as POSIXct objects because I want to do this for times rather than numbers. step says how big of an increment I want the slider to advance by when someone is pulling it. If the values of the slider are times, then the default unit for this is seconds, so I’m saying to have a step size of one minute. The timeFormat says how I want the time to print out at the interface and the timezone says what time zone I want time values to display in. Things like this slider bar are called “Control Widgets”, and there’s a whole list of them in the third lesson of RStudio’s Shiny tutorial. There are also examples online in the Shiny Gallery. 12.1.8 Creating output in server.R Next, I’ll put some R code in the server.R file to create a figure and pass it through to the ui.R file to print out to the application interface. At first, I won’t make this figure “reactive”; that is, it won’t change at all when the user changes the slider bar. However, I will eventually add in that reactivity so that the plot changes everytime a user changes the slider bar. I am going to create a map of all the Tweets that included certain hashtags or phrases and that were Tweeted (and geolocated) from within a five-mile radius of the center of Paris during the attacks last Friday. For this, I’m going to use data on Tweets I pulled using the TwitteR package, which syncs up with Twitter’s API. Here’s an example of what the data looks like (I’ve cleared out some of the extra columns I won’t use): paris_twitter &lt;- read.csv(&quot;data/App-1/data/final_tweets.csv&quot;, as.is = TRUE) %&gt;% mutate(tag = factor(tag), created = ymd_hms(created, tz = &quot;Europe/Paris&quot;), text = iconv(text, to=&#39;ASCII//TRANSLIT&#39;)) paris_twitter[1:2, ] ## text ## 1 RT @forza_will2006: My heart aches for the people of France. #PorteOuverte #PrayForParis #DownWithTerrorism #Pompidou... https://t.co/KwtNPsQ... ## 2 Ensemble contre la haine #jesuisparis #porteouverte #paris @ Place de la Republique https://t.co/TZykpVJHwd ## created longitude latitude tag ## 1 2015-11-15 01:27:56 NA NA #PorteOuverte ## 2 2015-11-14 22:19:47 2.364184 48.86747 #PorteOuverte About an equal number of these have and don’t have location data: table(!is.na(paris_twitter$longitude)) ## ## FALSE TRUE ## 10478 10683 Here is a table of the number of tweets under the five most-tweeted tags: head(paris_twitter) ## text ## 1 RT @forza_will2006: My heart aches for the people of France. #PorteOuverte #PrayForParis #DownWithTerrorism #Pompidou... https://t.co/KwtNPsQ... ## 2 Ensemble contre la haine #jesuisparis #porteouverte #paris @ Place de la Republique https://t.co/TZykpVJHwd ## 3 Liberte, egalite, fraternite #Paris #TodosSomosParis #JeSuisParis #PorteOuverte #peace #paix #paz... https://t.co/AjYVrmCCnL ## 4 Stay With My French #prayforparis #parisattacks #porteouverte #france #riphumanity #liberte #egalite... https://t.co/9DH5diHmyz ## 5 &lt;NA&gt; ## 6 RT @bodoi_music: J&#39;ai peur je cherche un abri #porteouverte https://t.co/1DIr4VLOEa ## created longitude latitude tag ## 1 2015-11-15 01:27:56 NA NA #PorteOuverte ## 2 2015-11-14 22:19:47 2.364184 48.86747 #PorteOuverte ## 3 2015-11-14 21:36:06 2.350800 48.85670 #PorteOuverte ## 4 2015-11-14 21:00:49 2.350800 48.85670 #PorteOuverte ## 5 2015-11-14 20:11:01 2.350800 48.85670 #PorteOuverte ## 6 2015-11-14 19:52:43 NA NA #PorteOuverte tweet_sum &lt;- paris_twitter %&gt;% dplyr::group_by_(~ tag) %&gt;% dplyr::summarize_(n = ~ n(), example = ~ gsub(&quot;[[:punct:]]&quot;, &quot; &quot;, base::sample(text, 1))) %&gt;% dplyr::arrange_(~ dplyr::desc(n)) knitr::kable(tweet_sum[1:5, ], col.names = c(&quot;Tag&quot;, &quot;# of Tweets&quot;, &quot;Example Tweet&quot;)) Tag # of Tweets Example Tweet #Paris 7502 RT Vince66240 Tirs a la Kalash au petit Cambodge dans le 10 eme a Paris plusieurs morts Secours et police sur place tir https t co B #PrayForParis 7250 The only thing that we can do now is prayforparis Paris France https t co c5AgWfdCKj #13novembre 1255 RT taimaz Des dizaines de victimes sont acheminees en bus en mairie du 11e Paris AFP 13novembre https t co bYrPC7e1TV #PorteOuverte 1147 RT Folivao PorteOuverte geo localise ici Place d Italie p roche metro to lbiac 13eme arrondiss Une mention pour avoir adresse exacte F #fusillade 765 RT brunetmanxn Je suis francaise fiere de l etre Mais aneanti par cette anarchie prayingforparis fusillade https t co 8aSmJWNodj For the Tweets that are geolocated, it’s possible to map the tweet locations using the following code: paris_map &lt;- get_map(&quot;paris&quot;, zoom = 12, color = &quot;bw&quot;) paris_locations &lt;- c(&quot;Stade de France&quot;, &quot;18 Rue Alibert&quot;, &quot;50 Boulevard Voltaire&quot;, &quot;92 Rue de Charonne&quot;, &quot;Place de la Republique&quot;) paris_locations &lt;- paste(paris_locations, &quot;paris france&quot;) paris_locations &lt;- cbind(paris_locations, geocode(paris_locations)) # Plot contour map of tweet locations plot_map &lt;- function(tag = &quot;all&quot;, df = paris_twitter){ library(ggmap) library(dplyr) df &lt;- dplyr::select(df, tag, latitude, longitude) %&gt;% filter(!is.na(longitude)) %&gt;% mutate(tag = as.character(tag)) if(tag != &quot;all&quot;){ if(!(tag %in% df$tag)){ stop(paste(&quot;That tag is not in the data. Try one of the following tags instead: &quot;, paste(unique(df$tag), collapse = &quot;, &quot;))) } to_plot &lt;- df[df$tag == tag, ] } else { to_plot &lt;- df } hotel_de_ville &lt;- to_plot$latitude == 48.85670 &amp; to_plot$longitude == 2.350800 n_hotel_de_ville &lt;- sum(hotel_de_ville) if(n_hotel_de_ville == max(table(to_plot$latitude))){ hdv_index &lt;- sample((1:nrow(to_plot))[hotel_de_ville], round(n_hotel_de_ville / 2)) to_plot &lt;- to_plot[-hdv_index, ] } my_map &lt;- ggmap(paris_map, extent = &quot;device&quot;) + geom_point(data = to_plot, aes(x = longitude, y = latitude), color = &quot;darkgreen&quot;, alpha = 0.75) + geom_density2d(data = to_plot, aes(x = longitude, latitude), size = 0.3) + stat_density2d(data = to_plot, aes(x = longitude, y = latitude, fill = ..level.., alpha = ..level..), size = 0.01, bins = round(nrow(to_plot) / 3.3), geom = &quot;polygon&quot;) + scale_fill_gradient(low = &quot;green&quot;, high = &quot;yellow&quot;, guide = FALSE) + scale_alpha(guide = FALSE) + geom_point(data = paris_locations, aes(x = lon, y = lat), color = &quot;red&quot;, size = 5, alpha = 0.75) return(my_map) } ## Warning: `panel.margin` is deprecated. Please use `panel.spacing` property ## instead To print this out in the application, I’ll put all the code for the mapping function in a file called helper.R, source this file in the server.R file, and then I can just call the function within the server file. The application will look as follows after this step: To complete this, I first changed the server.R file to look like this: library(dplyr) library(lubridate) source(&quot;helper.R&quot;) paris_twitter &lt;- read.csv(&quot;data/final_tweets.csv&quot;, as.is = TRUE) %&gt;% mutate(tag = factor(tag), created = ymd_hms(created, tz = &quot;Europe/Paris&quot;)) shinyServer(function(input, output) { output$twitter_map &lt;- renderPlot({ plot_map() }) }) Notice a few things here: I’m loading the packages I’ll need for the code. I’m running all the code in the helper.R file (which includes the function I created to plot this map) using the source() command. I put the code to plot the map (plot_map()) inside the renderPlot({}) function. I’m putting the plot in a twitter_map slot of the output object. All of this is going inside the call shinyServer(function(input, output){ }). One other change is necessary to get the map to print on the app. I need to add code to the ui.R file to tell R where to plot this map on the final interface. The full file now looks like this: shinyUI(fluidPage( titlePanel(&quot;Tweets during Paris Attack&quot;), sidebarLayout(position = &quot;right&quot;, sidebarPanel(&quot;Choose what to display&quot;, sliderInput(inputId = &quot;time_range&quot;, label = &quot;Select the time range: &quot;, value = c(as.POSIXct(&quot;2015-11-13 00:00:00&quot;, tz = &quot;CET&quot;), as.POSIXct(&quot;2015-11-15 06:00:00&quot;, tz = &quot;CET&quot;)), min = as.POSIXct(&quot;2015-11-13 00:00:00&quot;, tz = &quot;CET&quot;), max = as.POSIXct(&quot;2015-11-14 12:00:00&quot;, tz = &quot;CET&quot;), step = 60, timeFormat = &quot;%dth %H:%M&quot;, timezone = &quot;+0100&quot;)), mainPanel(&quot;Map of tweets&quot;, plotOutput(&quot;twitter_map&quot;)) ) )) The new part is where I’ve added the code: mainPanel(&quot;Map of tweets&quot;, plotOutput(&quot;twitter_map&quot;)) This tells R to put the plot output twitter_map from the output object in the main panel of the Shiny app. 12.1.9 Making the output reactive Now almost all of the pieces are in place to make this graphic reactive. First, I added some options to the function in helper.R to let it input time ranges and only plot the tweets within that range. Next, I need to use the values that the user selects from the slider in the call for plotting the map. To do this, I can use the values passed from the slider bar in the input object into the code in the server.R file. Here is the new code for the server.R file: library(ggmap) library(ggplot2) library(dplyr) library(lubridate) source(&quot;helper.R&quot;) paris_twitter &lt;- read.csv(&quot;data/final_tweets.csv&quot;, as.is = TRUE) %&gt;% mutate(tag = factor(tag)) paris_twitter$created &lt;- as.POSIXct(paris_twitter$created, tz = &quot;CET&quot;) shinyServer(function(input, output) { output$twitter_map &lt;- renderPlot({ plot_map(start.time = input$time_range[1], end.time = input$time_range[2]) }) }) The only addition from before is to use the start.time and end.time options in the plot_map function and to set them to the first, [1], and second, [2], values in the time_range slot of the input object. Remember that we chose to label the input from the slider bar time_range when we set up the ui.R file. This app is saved in the directory App-1 in this week’s directory if you’d like to play around with the code. I’ve deployed it on shinyapps here. 12.1.10 Fancier version I’ve also created a (much) fancier version of a Shiny App looking at this Twitter data that you can check out here. All the code for that is here. 12.2 Viridis color map There is a package called viridisLite with some good color maps that are gaining population in visualization. From the package’s GitHub repository: “These four color maps are designed in such a way that they will analytically be perfectly perceptually-uniform, both in regular form and also when converted to black-and-white. They are also designed to be perceived by readers with the most common form of color blindness.” Viridis is now the default color map for Matplotlib, a key Python plotting library. The viridisLite package is a simpler version of the viridis package (and requires fewer dependencies). Several of the packages we’ll look at today use Viridis as the default color map. Here is an example of a hexbin graph of random values that uses the Viridis color map: library(viridisLite) library(hexbin) dat &lt;- data.frame(x = rnorm(10000), y = rnorm(10000)) ggplot(dat, aes(x = x, y = y)) + geom_hex() + coord_fixed() + scale_fill_gradientn(colours = viridis(256, option = &quot;D&quot;)) The option argument allows you to pick between four color maps: Magma, Inferno, Plasma, and Viridis. Here is the code to visualize each of those: library(gridExtra) ex_plot &lt;- ggplot(dat, aes(x = x, y = y)) + geom_hex() + coord_fixed() magma_plot &lt;- ex_plot + scale_fill_gradientn(colours = viridis(256, option = &quot;A&quot;)) + ggtitle(&quot;magma&quot;) inferno_plot &lt;- ex_plot + scale_fill_gradientn(colours = viridis(256, option = &quot;B&quot;)) + ggtitle(&quot;inferno&quot;) plasma_plot &lt;- ex_plot + scale_fill_gradientn(colours = viridis(256, option = &quot;C&quot;)) + ggtitle(&quot;plasma&quot;) viridis_plot &lt;- ex_plot + scale_fill_gradientn(colours = viridis(256, option = &quot;D&quot;)) + ggtitle(&quot;viridis&quot;) grid.arrange(magma_plot, inferno_plot, plasma_plot, viridis_plot, ncol = 2) 12.3 htmlWidgets 12.3.1 Overview of htmlWidgets Very smart people have been working on creating interactive graphics in R for a long time. So far, nothing coded in R has taken off in a big way. JavaScript has developed a number of interactive graphics libraries that can be for documents viewed in a web browser. There is now a series of R packages that allow you to create plots from these JavaScript libraries from within R. There is a website with much more on these htmlWidgets at http://www.htmlwidgets.org. Some of the packages availabe to help you create interactive graphics from R using JavaScript graphics libraries: leaflet: Mapping (we’ll cover this next week) dygraphs: Time series plotly: A variety of plots, including maps rbokeh: A variety of plots, including maps networkD3: Network data d3heatmap: Heatmaps DT: Data tables DiagrammeR: Diagrams and flowcharts These packages can be used to make some pretty cool interactive visualizations for HTML output from R Markdown or Shiny (you can also render any of theme in RStudio). There are, however, a few limitations: Written by different people. The different packages have different styles as well as different interfaces. Learning how to use one package may not help you much with other of these packages. Many are still in development, often in early development. 12.3.2 plotly package From the package documentation: “Easily translate ggplot2 graphs to an interactive web-based version and / or create custom web-based visualizations directly from R.” Like many of the packages today, draws on functionality external to R, but within a package that allows you to work exclusively within R. Allows you to create interactive graphs from R. Some of the functions extend the ggplot2 code you’ve learned. Interactivity will only work within RStudio or on documents rendered to HTML. The plotly package allows an interface to let you work with plotly.js code directly using R code. plotly.js is an open source library for creating interactive graphs in JavaScript. This JavaScript library is built on d3.js (Data-Driven Documents), which is a key driver in interactive web-based data graphics today. There are two main ways of create plots within plotly: Use one of the functions to create a customized interactive graphic: plot_ly: Workhorse of plotly, renders most non-map types of graphs plot_geo, plot_mapbax: Specific functions for creating plotly maps. Create a ggplot object and then convert it to a plotly object using the ggplotly function. library(faraway) data(worldcup) library(plotly) plot_ly(worldcup, type = &quot;scatter&quot;, x = ~ Time, y = ~ Shots) Just like with ggplot2, the mappings you need depend on the type of plot you are creating. For example, scatterplots (type = &quot;scatter&quot;) need x and y defined, while a surface plot (type = &quot;surface&quot;) can be created with a single vector of elevation (we’ll see an example in a few slides). The help file for plot_ly includes a link with more documentation on the types of plots that can be made and the required mappings for each. plot_ly(worldcup, type = &quot;scatter&quot;, x = ~ Time, y = ~ Shots, color = ~ Position) The plotly package is designed so you can pipe data into plot_ly and add elements by piping into add_* functions (this idea is similar to adding elements to a ggplot object with +). worldcup %&gt;% plot_ly(x = ~ Time, y = ~ Shots, color = ~ Position) %&gt;% add_markers() Some of the add_* functions include: add_markers add_lines add_paths add_polygons add_segments add_histogram If you pipe to the rangeslider function, it allows the viewer to zoom in on part of the x range. (This can be particularly nice for time series.) You should have a dataset available through your R session named USAccDeaths. This gives a montly county of accidental deaths in the US for 1973 to 1978. This code will plot it and add a range slider on the lower x-axis. plot_ly(x = time(USAccDeaths), y = USAccDeaths) %&gt;% add_lines() %&gt;% rangeslider() For a 3-D scatterplot, add a mapping to the z variable: worldcup %&gt;% plot_ly(x = ~ Time, y = ~ Shots, z = ~ Passes, color = ~ Position, size = I(3)) %&gt;% add_markers() The volcano data comes with R and is in a matrix format. Each value gives the elevation for a particular pair of x- and y-coordinates. dim(volcano) ## [1] 87 61 volcano[1:4, 1:4] ## [,1] [,2] [,3] [,4] ## [1,] 100 100 101 101 ## [2,] 101 101 102 102 ## [3,] 102 102 103 103 ## [4,] 103 103 104 104 plot_ly(z = ~ volcano, type = &quot;surface&quot;) Mapping with plotly can build on some data that comes with base R or other packages you’ve likely added (or can add easily, as with the map_data function from ggplot2). For example, we can map state capitals and cities with &gt; 40,000 people using data in the us.cities dataframe in the maps package: head(maps::us.cities, 3) ## name country.etc pop lat long capital ## 1 Abilene TX TX 113888 32.45 -99.74 0 ## 2 Akron OH OH 206634 41.08 -81.52 0 ## 3 Alameda CA CA 70069 37.77 -122.26 0 Here is code you can use to map all of these cities on a US map: ggplot2::map_data(&quot;world&quot;, &quot;usa&quot;) %&gt;% group_by(group) %&gt;% filter(-125 &lt; long &amp; long &lt; -60 &amp; 25 &lt; lat &amp; lat &lt; 52) %&gt;% plot_ly(x = ~long, y = ~lat) %&gt;% add_polygons(hoverinfo = &quot;none&quot;) %&gt;% add_markers(text = ~paste(name, &quot;&lt;br /&gt;&quot;, pop), hoverinfo = &quot;text&quot;, alpha = 0.25, data = filter(maps::us.cities, -125 &lt; long &amp; long &lt; -60 &amp; 25 &lt; lat &amp; lat &lt; 52)) %&gt;% layout(showlegend = FALSE) You can also make choropleths interactive. Remember that we earlier created a choropleth of US state populations with the following code: library(choroplethr) data(df_pop_state) state_choropleth(df_pop_state) You can use the following code with plotly to make an interactive choropleth instead: us_map &lt;- list(scope = &#39;usa&#39;, projection = list(type = &#39;albers usa&#39;), lakecolor = toRGB(&#39;white&#39;)) plot_geo() %&gt;% add_trace(z = df_pop_state$value[df_pop_state$region != &quot;district of columbia&quot;], text = state.name, locations = state.abb, locationmode = &#39;USA-states&#39;) %&gt;% add_markers(x = state.center[[&quot;x&quot;]], y = state.center[[&quot;y&quot;]], size = I(2), symbol = I(8), color = I(&quot;white&quot;), hoverinfo = &quot;none&quot;) %&gt;% layout(geo = us_map) The other way to create a plotly graph is to first create a ggplot object and then transform it into an interactive graphic using the ggplotly function. The following code can be used to plot Time versus Shots for the World Cup date in a regular, non-interactive plot: shots_vs_time &lt;- worldcup %&gt;% mutate(Name = rownames(worldcup)) %&gt;% filter(Team %in% c(&quot;Netherlands&quot;, &quot;Germany&quot;, &quot;Spain&quot;, &quot;Uruguay&quot;)) %&gt;% ggplot(aes(x = Time, y = Shots, color = Position, group = Name)) + geom_point() + facet_wrap(~ Team) shots_vs_time To make the plot interactive, just pass the ggplot object to ggplotly: ggplotly(shots_vs_time) With R, not only can you pull things from another website using an API, you can also upload or submit things. There is a function in the plotly library, plotly_POST, that lets you post a plot you create in R to https://plot.ly. You need a plot.ly account to do that, but there are free accounts available. The creator of the R plotly package has written a bookdown book on the package that you can read here. It provides extensive details and examples for using plotly. Getting Started with D3 by Mike Dewar (a short book on D3 in JavaScript) is available for free here. 12.3.3 rbokeh package The rbokeh package provides an R interface to a Python interactive visualization library, Bokeh. There is a website with many more details on using the rbokeh package: https://hafen.github.io/rbokeh/ You can find out more about the original Python library, Bokeh, at http://bokeh.pydata.org/en/latest/. Here is an example of an interactive scatterplot of the World Cup data made with the rbokeh package: library(rbokeh) figure(width = 600, height = 300) %&gt;% ly_points(Time, Shots, data = worldcup, color = Position, hover = list(Time, Shots)) This package can also be used to create interactive maps. For example, the following dataset has data on Oregon climate stations, including locations: orstationc &lt;- read.csv(paste0(&quot;http://geog.uoregon.edu/bartlein/&quot;, &quot;old_courses/geog414s05/data/orstationc.csv&quot;)) head(orstationc, 3) ## station lat lon elev tjan tjul tann pjan pjul pann idnum ## 1 ANT 44.917 -120.717 846 0.0 20.2 9.6 41 9 322 350197 ## 2 ARL 45.717 -120.200 96 0.9 24.6 12.5 40 6 228 350265 ## 3 ASH 42.217 -122.717 543 3.1 20.8 11.1 70 7 480 350304 ## Name ## 1 ANTELOPE 1 N USA-OR ## 2 ARLINGTON USA-OR ## 3 ASHLAND 1 N USA-OR You can use the following code to create an interactive map of these climate stations: gmap(lat = 44.1, lng = -120.767, zoom = 5, width = 500, height = 428) %&gt;% ly_points(lon, lat, data = orstationc, alpha = 0.8, col = &quot;red&quot;, hover = c(station, Name, elev, tann)) You can get very creative with this package. The following code comes directly from the help documentation for the package and shows how to use this to create an interactive version of the periodic table: # prepare data elements &lt;- subset(elements, !is.na(group)) elements$group &lt;- as.character(elements$group) elements$period &lt;- as.character(elements$period) # add colors for groups metals &lt;- c(&quot;alkali metal&quot;, &quot;alkaline earth metal&quot;, &quot;halogen&quot;, &quot;metal&quot;, &quot;metalloid&quot;, &quot;noble gas&quot;, &quot;nonmetal&quot;, &quot;transition metal&quot;) colors &lt;- c(&quot;#a6cee3&quot;, &quot;#1f78b4&quot;, &quot;#fdbf6f&quot;, &quot;#b2df8a&quot;, &quot;#33a02c&quot;, &quot;#bbbb88&quot;, &quot;#baa2a6&quot;, &quot;#e08e79&quot;) elements$color &lt;- colors[match(elements$metal, metals)] elements$type &lt;- elements$metal # make coordinates for labels elements$symx &lt;- paste(elements$group, &quot;:0.1&quot;, sep = &quot;&quot;) elements$numbery &lt;- paste(elements$period, &quot;:0.8&quot;, sep = &quot;&quot;) elements$massy &lt;- paste(elements$period, &quot;:0.15&quot;, sep = &quot;&quot;) elements$namey &lt;- paste(elements$period, &quot;:0.3&quot;, sep = &quot;&quot;) # create figure p &lt;- figure(title = &quot;Periodic Table&quot;, tools = c(&quot;resize&quot;, &quot;hover&quot;), ylim = as.character(c(7:1)), xlim = as.character(1:18), xgrid = FALSE, ygrid = FALSE, xlab = &quot;&quot;, ylab = &quot;&quot;, height = 445, width = 800) %&gt;% # plot rectangles ly_crect(group, period, data = elements, 0.9, 0.9, fill_color = color, line_color = color, fill_alpha = 0.6, hover = list(name, atomic.number, type, atomic.mass, electronic.configuration)) %&gt;% # add symbol text ly_text(symx, period, text = symbol, data = elements, font_style = &quot;bold&quot;, font_size = &quot;10pt&quot;, align = &quot;left&quot;, baseline = &quot;middle&quot;) %&gt;% # add atomic number text ly_text(symx, numbery, text = atomic.number, data = elements, font_size = &quot;6pt&quot;, align = &quot;left&quot;, baseline = &quot;middle&quot;) %&gt;% # add name text ly_text(symx, namey, text = name, data = elements, font_size = &quot;4pt&quot;, align = &quot;left&quot;, baseline = &quot;middle&quot;) %&gt;% # add atomic mass text ly_text(symx, massy, text = atomic.mass, data = elements, font_size = &quot;4pt&quot;, align = &quot;left&quot;, baseline = &quot;middle&quot;) p 12.3.4 dygraphs package The dygraphs package lets you create interactive time series plots from R using the dygraphs JavaScript library. The main function syntax is fairly straightforward. Like many of these packages, it allows piping. There is a website with more information on using dygraphs available at http://rstudio.github.io/dygraphs/index.html. For example, here is the code to plot monthly deaths from lung diseases in the UK in the 1970s. library(dygraphs) lungDeaths &lt;- cbind(mdeaths, fdeaths) dygraph(lungDeaths) %&gt;% dySeries(&quot;mdeaths&quot;, label = &quot;Male&quot;) %&gt;% dySeries(&quot;fdeaths&quot;, label = &quot;Female&quot;) 12.3.5 DT package The DT package provides a way to create interactive tables in R using the JavaScript DataTables library. We’ve already seen some examples of this output in some of the Shiny apps I showed last week. You can also use this package to include interactive tables in R Markdown documents you plan to render to HTML. There is a website with more information on this package at http://rstudio.github.io/DT/. library(DT) datatable(worldcup) 12.3.6 Creating your own widget If you find a JavaScript visualization library and would like to create bindings to R, you can create your own package for a new htmlWidget. There is advice on creating your own widget for R available at http://www.htmlwidgets.org/develop_intro.html. "],
["appendix-a-vocabulary.html", "A Appendix A: Vocabulary A.1 Week 1 (Quiz 1) A.2 Week 2 (Quiz 2) A.3 Week 3 (Quiz 3) A.4 Week 4 (Quiz 4) A.5 Week 5 (Quiz 5) A.6 Week 6 (Quiz 6) A.7 Week 7 (Quiz 7) A.8 Week 8 (Quiz 8)", " A Appendix A: Vocabulary You will be responsible for knowing the following functions and vocabulary for the weekly quizzes. A.1 Week 1 (Quiz 1) c() data.frame() dim() ncol() nrow() head(), option n = read.csv, options head =, skip =, nrow = [...], [..., ...] getwd() setwd(), including setwd(&quot;~&quot;) list.files() install.packages() library() &lt;- = subset() length() open source software “free as in beer” “free as in speech” CRAN GitHub R packages R working directory How to download a csv file from GitHub Nate Silver FiveThirtyEight Grading policies for the course Course requirements / policies for in-class quizzes and weekly journal entries Style rules for naming R objects Difference between R and RStudio Vectors Dataframes Note: Pay attention in the course notes and exercise to where the code uses quotation marks and where it does not– this will help you in the quiz A.2 Week 2 (Quiz 2) source() setwd(), including setwd(&quot;~&quot;), setwd(&quot;..&quot;), setwd(&quot;..\\..&quot;) list.files(), option path = functions in the read.table() family, including read.csv() and read.delim(). What are defaults of the sep = and dec = options for each? For all, the options header =, sep =, as.is =, na.strings =, nrows =, skip =, and col.names =. The tidyverse functions in the read_* family (e.g., read_csv) Advantages of the read_* family of functions compared to their base R analogues (the read.table functions) paste(), option sep = paste0() readxl package and its read_excel() function haven package and its read_sas() function $ class() str() as.Date(), option format = lubridate functions, include ymd, ymd_hm, and mdy range() dplyr package rename() mutate() arrange() %&gt;%, advantages of piping filter() Reading in data from either a local or online flat file save(), option file = load() rm() ls() Main types of vector classes in R: character, numeric, factor, date, logical Which classes of vectors don’t always look like numbers, but R assigns an underlying numeric value to? (Hint: This include the logical class, which R saves with an underlying number, with TRUE = 1 and FALSE = 0.) Common abbreviations for telling R date formats (e.g., “%m”, “%y”) Common logical expressions to use in filter() relative pathnames absolute pathnames delimited files fixed width files R script file (How would you make a new one? What file extension would it have? Why is it important to use? How do you run code from a script file in RStudio?) What kinds of data can be read into R? How to read flat files of data that are online directly into R if they are on: A “http:” site A “https:” site When you might want to save an R object as a .RData file and when (and why) you might not want to A.3 Week 3 (Quiz 3) data() (with and without the name of a dataset as an option) library() (with and without an argument in the parentheses) common addition for all these plotting functions: ggtitle, xlab, ylab, xlim, ylim aes function and common aesthetics, including color, shape, x, y, and fill Some common geoms: geom_histogram, geom_points, geom_lines, geom_boxplot() (both for a single numeric variable and for a numeric vector stratified by a factor) ggpairs() from the GGally package range() min() max() mean() median() table() cor(), both for two variables in a dataframe, and to get the correlation matrix for several variables in a dataframe summary(), as applied to: different classes of vectors (numeric, factor, logical), dataframes, lm objects, and glm objects lm(), data= option glm(), options data=, family= Functions to apply to a lm or glm object: summary(), coef(), residuals(), fitted(), plot(), abline() The following elements that you can pull from the summary of a lm call: summary(mod_1)$call, summary(mod_1)$coef, summary(mod_1)$r.squared, summary(mod_1)$cov.unscaled How to create a logical vector and how to use one to (1) index a data frame and (2) count the number of times a certain condition is true in a vector What the bang operator (!) does to a logical operator What to do if you want to apply a summary statistic function to a vector with missing values (you do not need to know every option name for all the functions, just know that you would need to include an option like na.rm= or use=, and that you can use the help file for a function to figure out the option call for that function). The following about object-oriented programming: In R, it means that some functions, like summary(), will do different things depending on what type of object you call it on. The basic structure of regression formulae in R (for example, y ~ x1 + x2) Difference between using lm() and glm() to fit a linear regression model Difference between the code you would use to fit a linear, Poisson, or logistic model using glm() A.4 Week 4 (Quiz 4) Guidelines for good graphics Data density / data-to-ink ratio Small multiples Edward Tufte Hadley Wickham Where to put the + in ggplot statements to avoid problems (ends of lines instead of starts of new lines) Can you save a ggplot object as an R object that you can reference later? If so, how would you add elements on to that object? How would you print it when you were ready to print the graph to your RStudio graphics window? geom_hline(), geom_vline() geom_text() facet_grid(), facet_wrap() grid.arrange() from the gridExtra package ggthemes package, including theme_few() and theme_tufte() Setting point color for geom_point() both as a constant (all points red) and as a way to show the level of a factor for each observation size, alpha, color Re-naming and re-ordering factors Note: If you read this and find and bring in an example of a “small multiples” graph (from a newspaper, a website, an academic paper), you can get one extra point on this quiz A.5 Week 5 (Quiz 5) Reproducible research, including what it is and advantages to aiming to make your research reproducible R style guidelines on variable names, &lt;- vs. =, line length, spacing, semicolons, commenting, indentation, and code grouping Markup languages (concept and examples) Basic conventions for Markdown (bold, italics, links, headers, lists) Literate programming What working directory R uses for code in an .Rmd document Basic syntax for RMarkdown chunks, including how to name them Options for RMarkdown chunks: echo, eval, messages, warnings, include, fig.width, fig.height, results Difference between global options and chunk options, and which takes precendence What inline code is and how to write it in RMarkdown How to set global options Why style is important in coding RPubs A.6 Week 6 (Quiz 6) Three characteristics of tidy data Five common problems with tidy data and how to resolve them (make sure you understand the examples shown, which you can find out more about in the Hadley Wickham paper I reference) select filter mutate summarize group_by arrange gather spread The *_join family of functions %&gt;% Go through the examples from in-course exercises where we chained together several functions to clean up a dataset and make sure you can follow through these chained examples A.7 Week 7 (Quiz 7) for loops basics of writing a function figuring out the output of a loop based on its code figuring the the output of a function based on its code parentheses around a full assignment statement (e.g., (ex &lt;- 1)) kable() from the knitr package A.8 Week 8 (Quiz 8) apply family of functions matrix objects, including how to subset list objects, including how to subset "],
["appendix-b-homework.html", "B Appendix B: Homework B.1 Homework #1 B.2 Homework #2 B.3 Homework #3 B.4 Homework #4 B.5 Homework #5 B.6 Homework #6", " B Appendix B: Homework The following are six homework assignments for the course. B.1 Homework #1 Due date: Sept. 14 For your first homework assignment, you’ll be working through a few swirl lessons that are relevant to the material we’ve covered so far. Swirl is a platform that helps you learn R in R - you can complete the lessons right in your R console. B.1.1 Getting started First, you’ll need to install the swirl package: install.packages(&quot;swirl&quot;) Next, load the swirl package. We’re going to download a course from swirl’s course repository called R Programming E using the function install_course_github. Then call the swirl() function to enter the interactive platform: library(swirl) uninstall_course(&quot;R_Programming_E&quot;) # Only run if you have an old version of # R_Programming_E installed install_course_github(&quot;swirldev&quot;, &quot;R_Programming_E&quot;) swirl() After calling swirl(), you may be prompted to clear your workspace variables by running rm=(list=ls()). Running this code will clear any variables you already have saved in your global environment. While swirl recommends that you do this, it’s not necessary. B.1.2 Swirl lessons Sign in with your name, and choose R Programming E when swirl asks you to choose a course. For this homework, you will need to work through the following lessons in that course (the lesson number is in parentheses): Basic Building Blocks (1) Vectors (4) Missing Values (5) Subsetting Vectors (6) Logic (8) Looking at Data (12) Dates and Times (14) Each lesson should take about 10-15 minutes, but some are much shorter. You can complete the lessons in any order you want, but you may find it easiest to start with the lowest-numbered lessons and work your way up, in the order we’ve listed the lessons here. You’ll be able to get started on some of these lessons after your first day in class (Basic Building Blocks, for example), but others cover topics that we’ll get to in weeks 2 and 3. Whether or not we’ve covered a swirl topic in class, you should be able to successfully work through the lesson. At the end of each lesson, you’ll be prompted to “inform someone about your successful completion of this lesson via email.” after answering 2 for ‘Yes,’ enter your full name, and enter rachel.severson@colostate.edu as the email address of the person you’d like to notify. You should be sending 7 emails in total. After telling swirl that you would like to send a notification email, an already-populated email should pop up with the lesson you just completed in the subject line - you just need to push send. This might not happen if you access your email through a web browser instead of an app. In this case, just send an email manually with a screenshot of the end of the lesson, and the name of the lesson you just completed. B.1.3 Special swirl commands In the swirl environment, knowing about the following commands will be helpful: Within each lesson, the prompt ... indicates that you should hit Enter to move on to the next section. play(): temporarily exit swirl. It can be useful during a swirl lesson to play around in the R console to try things out. nxt(): regain swirl’s attention after play()ing around in the console. main(): return to swirl’s main menu. bye(): exit swirl. Swirl will save your progress if you exit in the middle of a lesson. You can also hit the Esc. key to exit. (To re-enter swirl, run swirl(). In a new R session you will have to first load the swirl library: library(swirl).) B.1.3.1 For fun While they aren’t required for class, you should consider trying out some other swirl lessons later in the course. The Functions lesson, as well as lapply and sapply and vapply and tapply could be particularly useful. You can also look through the course directory to see what other courses and lessons are available. If you are doing extra swirl courses on your own, you probably want to do them through the “R Programming”, rather than the “R Programming E”, course, since you won’t need to let us know by email. To get this, you can run: library(swirl) install_course(&quot;R_Programming&quot;) swirl() B.2 Homework #2 Due date: Sept. 28 For Homework 2, recreate the R Markdown document that you can download from here. Here are some initial tips: Your goal is to create an R Markdown document that you can compile to create a Word document that looks just like the example document we’ve linked above. You will turn in (by email) both the compiled Word document and the .Rmd original file. Add your name as “Author” and the due date of the assignment as “Date”. You should add these within the R Markdown document, rather than changing them in the final, compiled Word document. If you want to get started before you know how to use R Markdown, you can go ahead and write all of the necessary code to replicate the output and figures in the document in an R script. The code chunks here have been hidden with the option echo = FALSE, but you should include your code in your final document. Set the chunk options warning = FALSE and message = FALSE to prevent warnings and messages from being printed out. You will get some messages and warnings in the code from things like missing values and from loading packages, but you want to hide all of those messages in your final document. For things like templates, colors, level of transparency, and point size, you will receive full credit if you create figures that are visually similar to the ones shown in the example document. In other words, if the example document shows some transparency in points, you will get full credit if you also include some transparency in the points in your plot, but you do not have to include the exact same value of alpha. In R, there are often many different ways to achieve something. As long as your code works, it’s fine if you haven’t coded it exactly like we have in our version. However, your output should look identical to ours (or, in the case of color, transparency, point size, and themes, visually similar). You will not lose points if you cannot recreate the table in the document (although you should try to!). The last section, under the heading “Extra challenge– not graded”, is not graded. However, if you’d like an extra challenge, you’re welcome to try it out and include it in your final submission! If you need them, here are some further tips: Functions from the tidyverse (especially from dplyr, readr, and ggplot packages) will make your life much easier for this exercise. You can now install and load the tidyverse package to load them all at once. To rename column names with “special” characters in them, wrap the whole old column name in backticks. For example, to change a column name that has a dollar sign in it, you would use something like “rename(new_col_name = `old_col_name$`)”. To change the size of a figure in a report, use the “fig.width” and “fig.height” chunk options. You will want to use scale_fill_brewer in several of the figures. Don’t forget that, within functions like scale_x_continuous, you can use the argument breaks to set where the axis has breaks, and then labels to set what will actually be shown at each break. The string “\\n” can be included in legends and labels to include a carriage return. Coordinates can be flipped in a graph with the coord_flip geom. So, if you can figure out a way to make a graph with the coordinates flipped, use that code and just add coord_flip at the end. B.3 Homework #3 Due date: Oct. 12 For Homework 3, recreate the R Markdown document that you can download from here. Here are some initial tips: Your goal is to create an R Markdown document that you can compile to create a Word document that looks just like the target document we’ve linked above. The only difference is that you will use echo = TRUE to show your code within the rendered Word document. All formating within the text should be similar or identical to the target document. You will turn in (by email) both the compiled Word document and the .Rmd original file. Add your name as “Author” and the due date of the assignment as “Date”. You should add these within the R Markdown document, rather than changing them in the final, compiled Word document. Set the chunk options warning = FALSE and message = FALSE to prevent warnings and messages from being printed out. You will get some messages and warnings in the code from things like missing values and from loading packages, but you want to hide all of those messages in your final document. For things like templates, colors, level of transparency, and point size, you will receive full credit if you create figures that are visually similar to the ones shown in the example document. In other words, if the example document shows some transparency in points, you will get full credit if you also include some transparency in the points in your plot, but you do not have to include the exact same value of alpha. In R, there are often many different ways to achieve something. As long as your code works, it’s fine if you haven’t coded it exactly like we have in our version. However, your output should look identical to ours (or, in the case of color, transparency, point size, and themes, visually similar). There is one formated table in the target document. Be sure that you render this as a formated table, not as raw R output. If you need them, here are some further tips: Functions from the tidyverse (especially from dplyr, readr, and ggplot packages) will make your life much easier for this exercise. You can now install and load the tidyverse package to load them all at once. To reference column names with “special” characters in them, like dollar signs or spaces, wrap the whole old column name in backticks. For example, to change a column name that has a dollar sign in it, you would use something like “rename(new_col_name = `old_col_name$`)”. To change the size of a figure in a report, use the “fig.width” and “fig.height” chunk options. Don’t forget that there are functions in the scale family that allow you to use log-scale axes. B.4 Homework #4 Optional due date: Oct. 28 All instructions for this homework can be downloaded here. The example “fars_analysis.pdf” document you will try to recreate is here. You have the option to turn in parts of this homework (up through creating a clean dataset) by Oct. 28. If you do so, I will email you the code I used to clean the data, so you can check your own code and be sure you have a reasonable version of the clean data as you do the final parts of the assignment. B.5 Homework #5 Due date: Nov. 9 All instructions for this homework can be downloaded here. The example “fars_analysis.pdf” document you will try to recreate is here. You will submit this homework by posting a repo with your project directory on GitHub. We will work on setting that up during an in-course exercise. B.6 Homework #6 Due date: Nov. 30 Read the article Good Enough Practices in Scientific Computing by Wilson et al. (available here). In a half page, describe which of these “pretty good practices” your last homework incorporated. Also list one or two practices that you did not follow in your last homework but that would have made sense and how you could have followed them. Read the article Science Isn’t Broken on FiveThirtyEight. This article includes an interactive graphic. In a half page, give your opinion on whether this interactive graphic helps convey the main message of the article. Also, describe in general details how you might be able to create a graphic like this in R. Find an article in The R Journal that describes an R package that you could use in your own research or otherwise find interesting. Describe why the package was created and what you think it’s most interesting features are. In an R Markdown document, run one or two of the R examples included in the article. "],
["references-1.html", "References", " References "]
]
